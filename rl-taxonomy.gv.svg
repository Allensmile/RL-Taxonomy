<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.38.0 (20140413.2041)
 -->
<!-- Title: %3 Pages: 1 -->
<svg width="1581pt" height="1578pt"
 viewBox="0.00 0.00 1581.00 1578.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 1574)">
<title>%3</title>
<polygon fill="white" stroke="none" points="-4,4 -4,-1574 1577,-1574 1577,4 -4,4"/>
<g id="clust1" class="cluster"><title>clusterTimeline</title>
<polygon fill="#707070" stroke="#707070" stroke-width="2" points="44,-8 44,-24 1544,-24 1544,-8 44,-8"/>
</g>
<g id="clust2" class="cluster"><title>clusterModel Free</title>
<path fill="#f7fdff" stroke="black" d="M188.5,-692C188.5,-692 1553,-692 1553,-692 1559,-692 1565,-698 1565,-704 1565,-704 1565,-1550 1565,-1550 1565,-1556 1559,-1562 1553,-1562 1553,-1562 188.5,-1562 188.5,-1562 182.5,-1562 176.5,-1556 176.5,-1550 176.5,-1550 176.5,-704 176.5,-704 176.5,-698 182.5,-692 188.5,-692"/>
<text text-anchor="middle" x="870.75" y="-1545.2" font-family="arial black" font-size="16.00">Model Free</text>
</g>
<g id="clust3" class="cluster"><title>clusterValue Gradient</title>
<path fill="#daf0f6" stroke="black" stroke-dasharray="5,2" d="M312.5,-1228C312.5,-1228 1545,-1228 1545,-1228 1551,-1228 1557,-1234 1557,-1240 1557,-1240 1557,-1516 1557,-1516 1557,-1522 1551,-1528 1545,-1528 1545,-1528 312.5,-1528 312.5,-1528 306.5,-1528 300.5,-1522 300.5,-1516 300.5,-1516 300.5,-1240 300.5,-1240 300.5,-1234 306.5,-1228 312.5,-1228"/>
<text text-anchor="middle" x="928.75" y="-1511.2" font-family="arial black" font-size="16.00">Value Gradient</text>
</g>
<g id="clust4" class="cluster"><title>clusterPolicy Gradient/Actor&#45;Critic</title>
<path fill="#daf0f6" stroke="black" stroke-dasharray="5,2" d="M311,-700C311,-700 1344.5,-700 1344.5,-700 1350.5,-700 1356.5,-706 1356.5,-712 1356.5,-712 1356.5,-1208 1356.5,-1208 1356.5,-1214 1350.5,-1220 1344.5,-1220 1344.5,-1220 311,-1220 311,-1220 305,-1220 299,-1214 299,-1208 299,-1208 299,-712 299,-712 299,-706 305,-700 311,-700"/>
<text text-anchor="middle" x="827.75" y="-1203.2" font-family="arial black" font-size="16.00">Policy Gradient/Actor&#45;Critic</text>
</g>
<g id="clust5" class="cluster"><title>clusterModel Based</title>
<path fill="#dafdda" stroke="black" d="M183,-234C183,-234 1444,-234 1444,-234 1450,-234 1456,-240 1456,-246 1456,-246 1456,-672 1456,-672 1456,-678 1450,-684 1444,-684 1444,-684 183,-684 183,-684 177,-684 171,-678 171,-672 171,-672 171,-246 171,-246 171,-240 177,-234 183,-234"/>
<text text-anchor="middle" x="813.5" y="-667.2" font-family="arial black" font-size="16.00">Model Based</text>
</g>
<g id="clust6" class="cluster"><title>clusterMeta&#45;RL</title>
<path fill="#f5f5da" stroke="black" d="M624,-34C624,-34 1326,-34 1326,-34 1332,-34 1338,-40 1338,-46 1338,-46 1338,-214 1338,-214 1338,-220 1332,-226 1326,-226 1326,-226 624,-226 624,-226 618,-226 612,-220 612,-214 612,-214 612,-46 612,-46 612,-40 618,-34 624,-34"/>
<text text-anchor="middle" x="975" y="-209.2" font-family="arial black" font-size="16.00">Meta&#45;RL</text>
</g>
<!-- 1950s -->
<g id="node1" class="node"><title>1950s</title>
<text text-anchor="middle" x="71" y="-12.3" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="14.00" fill="white">1950s</text>
</g>
<!-- 1980&#45;90s -->
<g id="node2" class="node"><title>1980&#45;90s</title>
<text text-anchor="middle" x="515.5" y="-12.3" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="14.00" fill="white">1980&#45;90s</text>
</g>
<!-- 1950s&#45;&gt;1980&#45;90s -->
<g id="edge1" class="edge"><title>1950s&#45;&gt;1980&#45;90s</title>
<path fill="none" stroke="white" d="M98.0284,-16C172.072,-16 384.298,-16 473.852,-16"/>
<polygon fill="white" stroke="white" points="473.897,-19.5001 483.897,-16 473.897,-12.5001 473.897,-19.5001"/>
</g>
<!-- 2000s -->
<g id="node3" class="node"><title>2000s</title>
<text text-anchor="middle" x="652" y="-12.3" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="14.00" fill="white">2000s</text>
</g>
<!-- 1980&#45;90s&#45;&gt;2000s -->
<g id="edge2" class="edge"><title>1980&#45;90s&#45;&gt;2000s</title>
<path fill="none" stroke="white" d="M547.02,-16C567.044,-16 593.335,-16 614.47,-16"/>
<polygon fill="white" stroke="white" points="614.685,-19.5001 624.685,-16 614.685,-12.5001 614.685,-19.5001"/>
</g>
<!-- 2010&#45;2015 -->
<g id="node4" class="node"><title>2010&#45;2015</title>
<text text-anchor="middle" x="816.5" y="-12.3" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="14.00" fill="white">2010&#45;2015</text>
</g>
<!-- 2000s&#45;&gt;2010&#45;2015 -->
<g id="edge3" class="edge"><title>2000s&#45;&gt;2010&#45;2015</title>
<path fill="none" stroke="white" d="M679.108,-16C703.827,-16 741.437,-16 770.907,-16"/>
<polygon fill="white" stroke="white" points="770.957,-19.5001 780.957,-16 770.957,-12.5001 770.957,-19.5001"/>
</g>
<!-- 2016 -->
<g id="node5" class="node"><title>2016</title>
<text text-anchor="middle" x="936.5" y="-12.3" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="14.00" fill="white">2016</text>
</g>
<!-- 2010&#45;2015&#45;&gt;2016 -->
<g id="edge4" class="edge"><title>2010&#45;2015&#45;&gt;2016</title>
<path fill="none" stroke="white" d="M852.266,-16C866.927,-16 884.004,-16 898.861,-16"/>
<polygon fill="white" stroke="white" points="899.22,-19.5001 909.22,-16 899.22,-12.5001 899.22,-19.5001"/>
</g>
<!-- 2017 -->
<g id="node6" class="node"><title>2017</title>
<text text-anchor="middle" x="1147.5" y="-12.3" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="14.00" fill="white">2017</text>
</g>
<!-- 2016&#45;&gt;2017 -->
<g id="edge5" class="edge"><title>2016&#45;&gt;2017</title>
<path fill="none" stroke="white" d="M963.547,-16C1000.57,-16 1068.55,-16 1110.41,-16"/>
<polygon fill="white" stroke="white" points="1110.45,-19.5001 1120.45,-16 1110.45,-12.5001 1110.45,-19.5001"/>
</g>
<!-- 2018 -->
<g id="node7" class="node"><title>2018</title>
<text text-anchor="middle" x="1302.5" y="-12.3" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="14.00" fill="white">2018</text>
</g>
<!-- 2017&#45;&gt;2018 -->
<g id="edge6" class="edge"><title>2017&#45;&gt;2018</title>
<path fill="none" stroke="white" d="M1174.8,-16C1199.73,-16 1237.38,-16 1265.17,-16"/>
<polygon fill="white" stroke="white" points="1265.29,-19.5001 1275.29,-16 1265.29,-12.5001 1265.29,-19.5001"/>
</g>
<!-- 2019 -->
<g id="node8" class="node"><title>2019</title>
<text text-anchor="middle" x="1417.5" y="-12.3" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="14.00" fill="white">2019</text>
</g>
<!-- 2018&#45;&gt;2019 -->
<g id="edge7" class="edge"><title>2018&#45;&gt;2019</title>
<path fill="none" stroke="white" d="M1329.71,-16C1344.68,-16 1363.69,-16 1380.1,-16"/>
<polygon fill="white" stroke="white" points="1380.43,-19.5001 1390.43,-16 1380.43,-12.5001 1380.43,-19.5001"/>
</g>
<!-- 2020 -->
<g id="node9" class="node"><title>2020</title>
<text text-anchor="middle" x="1517" y="-12.3" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="14.00" fill="white">2020</text>
</g>
<!-- 2019&#45;&gt;2020 -->
<g id="edge8" class="edge"><title>2019&#45;&gt;2020</title>
<path fill="none" stroke="white" d="M1444.63,-16C1455.34,-16 1467.92,-16 1479.51,-16"/>
<polygon fill="white" stroke="white" points="1479.8,-19.5001 1489.8,-16 1479.8,-12.5001 1479.8,-19.5001"/>
</g>
<!-- Reinforcement\nLearning -->
<g id="node10" class="node"><title>Reinforcement\nLearning</title>
<g id="a_node10"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#ReinforcementLearning" xlink:title="Reinforcement learning (RL) is an area of machine learning concerned with how
software agents ought to take actions in an environment in order to maximize
the notion of cumulative reward [from Wikipedia]

">
<text text-anchor="middle" x="71" y="-406.6" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="18.00">Reinforcement</text>
<text text-anchor="middle" x="71" y="-386.6" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="18.00">Learning</text>
</a>
</g>
</g>
<!-- Model Free -->
<g id="node11" class="node"><title>Model Free</title>
<g id="a_node11"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#ModelFree" xlink:title="In model free reinforcement learning, the agent directly tries to predict the
value/policy without having or trying to model the environment

">
<path fill="#ffe6cc" stroke="black" stroke-width="2" d="M252.5,-1012C252.5,-1012 196.5,-1012 196.5,-1012 190.5,-1012 184.5,-1006 184.5,-1000 184.5,-1000 184.5,-988 184.5,-988 184.5,-982 190.5,-976 196.5,-976 196.5,-976 252.5,-976 252.5,-976 258.5,-976 264.5,-982 264.5,-988 264.5,-988 264.5,-1000 264.5,-1000 264.5,-1006 258.5,-1012 252.5,-1012"/>
<text text-anchor="middle" x="224.5" y="-990.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">Model Free</text>
</a>
</g>
</g>
<!-- Reinforcement\nLearning&#45;&gt;Model Free -->
<g id="edge84" class="edge"><title>Reinforcement\nLearning&#45;&gt;Model Free</title>
<path fill="none" stroke="black" d="M78.2214,-425.352C102.475,-520.285 190.611,-865.264 216.378,-966.125"/>
<polygon fill="black" stroke="black" points="213.008,-967.072 218.874,-975.894 219.79,-965.339 213.008,-967.072"/>
</g>
<!-- Model Based -->
<g id="node49" class="node"><title>Model Based</title>
<g id="a_node49"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#ModelBased" xlink:title="In model&#45;based reinforcement learning, the agent uses the experience to try to
model the environment, and then uses the model to predict the value/policy

">
<path fill="#ffe6cc" stroke="black" stroke-width="2" d="M258,-419C258,-419 191,-419 191,-419 185,-419 179,-413 179,-407 179,-407 179,-395 179,-395 179,-389 185,-383 191,-383 191,-383 258,-383 258,-383 264,-383 270,-389 270,-395 270,-395 270,-407 270,-407 270,-413 264,-419 258,-419"/>
<text text-anchor="middle" x="224.5" y="-397.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">Model Based</text>
</a>
</g>
</g>
<!-- Reinforcement\nLearning&#45;&gt;Model Based -->
<g id="edge85" class="edge"><title>Reinforcement\nLearning&#45;&gt;Model Based</title>
<path fill="none" stroke="black" d="M142.428,-401C151.217,-401 160.103,-401 168.608,-401"/>
<polygon fill="black" stroke="black" points="168.762,-404.5 178.762,-401 168.762,-397.5 168.762,-404.5"/>
</g>
<!-- Meta&#45;RL -->
<g id="node67" class="node"><title>Meta&#45;RL</title>
<g id="a_node67"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#MetaRL" xlink:title="In meta reinforcement learning, the agent is trained over distribution of
tasks, and with the knowledge it tries to solve new unseen but related task.

(2001)">
<path fill="#ffe6cc" stroke="black" stroke-width="2" d="M672,-137C672,-137 632,-137 632,-137 626,-137 620,-131 620,-125 620,-125 620,-113 620,-113 620,-107 626,-101 632,-101 632,-101 672,-101 672,-101 678,-101 684,-107 684,-113 684,-113 684,-125 684,-125 684,-131 678,-137 672,-137"/>
<text text-anchor="middle" x="652" y="-115.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">Meta&#45;RL</text>
</a>
</g>
</g>
<!-- Reinforcement\nLearning&#45;&gt;Meta&#45;RL -->
<g id="edge86" class="edge"><title>Reinforcement\nLearning&#45;&gt;Meta&#45;RL</title>
<path fill="none" stroke="black" d="M75.3947,-376.911C85.5138,-315.7 121.39,-159 223.5,-159 223.5,-159 223.5,-159 516.5,-159 549.011,-159 584.176,-148.13 610.394,-137.723"/>
<polygon fill="black" stroke="black" points="612.062,-140.821 619.98,-133.781 609.4,-134.347 612.062,-140.821"/>
</g>
<!-- Value Gradient -->
<g id="node12" class="node"><title>Value Gradient</title>
<g id="a_node12"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#ValueGradient" xlink:title="The algorithm is learning the value function of each state or state&#45;action.
The policy is implicit, usually by just selecting the best value

">
<path fill="#ffe6cc" stroke="black" stroke-width="2" d="M397.5,-1299C397.5,-1299 320.5,-1299 320.5,-1299 314.5,-1299 308.5,-1293 308.5,-1287 308.5,-1287 308.5,-1275 308.5,-1275 308.5,-1269 314.5,-1263 320.5,-1263 320.5,-1263 397.5,-1263 397.5,-1263 403.5,-1263 409.5,-1269 409.5,-1275 409.5,-1275 409.5,-1287 409.5,-1287 409.5,-1293 403.5,-1299 397.5,-1299"/>
<text text-anchor="middle" x="359" y="-1277.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">Value Gradient</text>
</a>
</g>
</g>
<!-- Model Free&#45;&gt;Value Gradient -->
<g id="edge9" class="edge"><title>Model Free&#45;&gt;Value Gradient</title>
<path fill="none" stroke="black" d="M228.21,-1012.28C234.881,-1052.97 254.862,-1152.97 299,-1224 306.368,-1235.86 316.742,-1246.99 326.742,-1256.22"/>
<polygon fill="black" stroke="black" points="324.454,-1258.87 334.259,-1262.88 329.097,-1253.63 324.454,-1258.87"/>
</g>
<!-- Policy Gradient\n/Actor&#45;Critic -->
<g id="node13" class="node"><title>Policy Gradient\n/Actor&#45;Critic</title>
<g id="a_node13"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#PolicyGradientActorCritic" xlink:title="The algorithm works directly to optimize the policy, with or without value
function. If the value function is learned in addition to the policy, we would
get Actor&#45;Critic algorithm. Most policy gradient algorithms are Actor&#45;Critic.
The Critic updates value function parameters w and depending on the algorithm
it could be action&#45;value Q(a|s;w) or state&#45;value V(s;w). The Actor updates
policy parameters θ, in the direction suggested by the critic, π(a|s;θ). [from
Lilian Weng&#39; blog]

">
<path fill="#ffe6cc" stroke="black" stroke-width="2" d="M399,-1012C399,-1012 319,-1012 319,-1012 313,-1012 307,-1006 307,-1000 307,-1000 307,-988 307,-988 307,-982 313,-976 319,-976 319,-976 399,-976 399,-976 405,-976 411,-982 411,-988 411,-988 411,-1000 411,-1000 411,-1006 405,-1012 399,-1012"/>
<text text-anchor="middle" x="359" y="-997.4" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">Policy Gradient</text>
<text text-anchor="middle" x="359" y="-984.4" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">/Actor&#45;Critic</text>
</a>
</g>
</g>
<!-- Model Free&#45;&gt;Policy Gradient\n/Actor&#45;Critic -->
<g id="edge10" class="edge"><title>Model Free&#45;&gt;Policy Gradient\n/Actor&#45;Critic</title>
<path fill="none" stroke="black" d="M264.538,-994C274.707,-994 285.912,-994 296.925,-994"/>
<polygon fill="black" stroke="black" points="296.95,-997.5 306.95,-994 296.95,-990.5 296.95,-997.5"/>
</g>
<!-- SARSA -->
<g id="node25" class="node"><title>SARSA</title>
<g id="a_node25"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#SARSA" xlink:title="SARSA (State&#45;Action&#45;Reward&#45;State&#45;Action) is an on&#45;policy TD control method

(1994)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M533,-1272C533,-1272 498,-1272 498,-1272 492,-1272 486,-1266 486,-1260 486,-1260 486,-1248 486,-1248 486,-1242 492,-1236 498,-1236 498,-1236 533,-1236 533,-1236 539,-1236 545,-1242 545,-1248 545,-1248 545,-1260 545,-1260 545,-1266 539,-1272 533,-1272"/>
<text text-anchor="middle" x="515.5" y="-1250.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">SARSA</text>
</a>
</g>
</g>
<!-- Value Gradient&#45;&gt;SARSA -->
<g id="edge17" class="edge"><title>Value Gradient&#45;&gt;SARSA</title>
<path fill="none" stroke="black" d="M409.735,-1272.31C431.069,-1268.58 455.623,-1264.29 475.596,-1260.8"/>
<polygon fill="black" stroke="black" points="476.333,-1264.22 485.581,-1259.05 475.128,-1257.33 476.333,-1264.22"/>
</g>
<!-- Q&#45;learning -->
<g id="node26" class="node"><title>Q&#45;learning</title>
<g id="a_node26"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#Qlearning" xlink:title="Q&#45;learning an off&#45;policy TD control method. Unlike SARSA, it doesn&#39;t follow
the policy to find the next action but rather chooses most optimal action in a
greedy fashion

(1989)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M541.5,-1326C541.5,-1326 489.5,-1326 489.5,-1326 483.5,-1326 477.5,-1320 477.5,-1314 477.5,-1314 477.5,-1302 477.5,-1302 477.5,-1296 483.5,-1290 489.5,-1290 489.5,-1290 541.5,-1290 541.5,-1290 547.5,-1290 553.5,-1296 553.5,-1302 553.5,-1302 553.5,-1314 553.5,-1314 553.5,-1320 547.5,-1326 541.5,-1326"/>
<text text-anchor="middle" x="515.5" y="-1304.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">Q&#45;learning</text>
</a>
</g>
</g>
<!-- Value Gradient&#45;&gt;Q&#45;learning -->
<g id="edge18" class="edge"><title>Value Gradient&#45;&gt;Q&#45;learning</title>
<path fill="none" stroke="black" d="M409.735,-1289.69C428.108,-1292.9 448.87,-1296.53 467.037,-1299.71"/>
<polygon fill="black" stroke="black" points="466.659,-1303.19 477.112,-1301.47 467.864,-1296.3 466.659,-1303.19"/>
</g>
<!-- TD&#45;Gammon -->
<g id="node27" class="node"><title>TD&#45;Gammon</title>
<g id="a_node27"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#TDGammon" xlink:title="TD&#45;Gammon is a model&#45;free reinforcement learning algorithm similar to
Q&#45;learning, and uses a multi&#45;layer perceptron with one hidden layer as the
value function approximator. It learns the game entirely by playing against
itself and achieves superhuman level of play.

(1995)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M548,-1380C548,-1380 483,-1380 483,-1380 477,-1380 471,-1374 471,-1368 471,-1368 471,-1356 471,-1356 471,-1350 477,-1344 483,-1344 483,-1344 548,-1344 548,-1344 554,-1344 560,-1350 560,-1356 560,-1356 560,-1368 560,-1368 560,-1374 554,-1380 548,-1380"/>
<text text-anchor="middle" x="515.5" y="-1358.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">TD&#45;Gammon</text>
</a>
</g>
</g>
<!-- Value Gradient&#45;&gt;TD&#45;Gammon -->
<g id="edge19" class="edge"><title>Value Gradient&#45;&gt;TD&#45;Gammon</title>
<path fill="none" stroke="black" d="M387.075,-1299.22C404.147,-1310.3 426.884,-1324.33 448,-1335 452.292,-1337.17 456.801,-1339.3 461.364,-1341.35"/>
<polygon fill="black" stroke="black" points="460.19,-1344.66 470.755,-1345.44 462.985,-1338.24 460.19,-1344.66"/>
</g>
<!-- A3C -->
<g id="node23" class="node"><title>A3C</title>
<g id="a_node23"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#A3C" xlink:title="Asynchronous Advantage Actor&#45;Critic (A3C) is a classic policy gradient method
with the special focus on parallel training. In A3C, the critics learn the
state&#45;value function, V(s;w), while multiple actors are trained in parallel
and get synced with global parameters from time to time. Hence, A3C is good
for parallel training by default, i.e. on one machine with multi&#45;core CPU.
[from Lilian Weng&#39; blog]

(2016)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M951.5,-1105C951.5,-1105 921.5,-1105 921.5,-1105 915.5,-1105 909.5,-1099 909.5,-1093 909.5,-1093 909.5,-1081 909.5,-1081 909.5,-1075 915.5,-1069 921.5,-1069 921.5,-1069 951.5,-1069 951.5,-1069 957.5,-1069 963.5,-1075 963.5,-1081 963.5,-1081 963.5,-1093 963.5,-1093 963.5,-1099 957.5,-1105 951.5,-1105"/>
<text text-anchor="middle" x="936.5" y="-1083.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">A3C</text>
</a>
</g>
</g>
<!-- Policy Gradient\n/Actor&#45;Critic&#45;&gt;A3C -->
<g id="edge41" class="edge"><title>Policy Gradient\n/Actor&#45;Critic&#45;&gt;A3C</title>
<path fill="none" stroke="black" d="M409.382,-1012.07C439.363,-1021.46 478.644,-1031 514.5,-1031 514.5,-1031 514.5,-1031 817.5,-1031 849.471,-1031 882.017,-1047.99 904.808,-1063.25"/>
<polygon fill="black" stroke="black" points="902.835,-1066.14 913.044,-1068.97 906.831,-1060.39 902.835,-1066.14"/>
</g>
<!-- REINFORCE -->
<g id="node37" class="node"><title>REINFORCE</title>
<g id="a_node37"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#REINFORCE" xlink:title="REINFORCE (Monte&#45;Carlo policy gradient) is a pure policy gradient algorithm
that works without a value function. The agent collects a trajectory of one
episode using its current policy, and uses the returns to update the policy
parameter

(1992)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M548,-1012C548,-1012 483,-1012 483,-1012 477,-1012 471,-1006 471,-1000 471,-1000 471,-988 471,-988 471,-982 477,-976 483,-976 483,-976 548,-976 548,-976 554,-976 560,-982 560,-988 560,-988 560,-1000 560,-1000 560,-1006 554,-1012 548,-1012"/>
<text text-anchor="middle" x="515.5" y="-990.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">REINFORCE</text>
</a>
</g>
</g>
<!-- Policy Gradient\n/Actor&#45;Critic&#45;&gt;REINFORCE -->
<g id="edge37" class="edge"><title>Policy Gradient\n/Actor&#45;Critic&#45;&gt;REINFORCE</title>
<path fill="none" stroke="black" d="M411.017,-994C426.88,-994 444.416,-994 460.477,-994"/>
<polygon fill="black" stroke="black" points="460.785,-997.5 470.785,-994 460.785,-990.5 460.785,-997.5"/>
</g>
<!-- DPG -->
<g id="node38" class="node"><title>DPG</title>
<g id="a_node38"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#DPG" xlink:title="Deterministic Policy Gradient. Abstract: In this paper we consider
deterministic policy gradient algorithms for reinforcement learning with
continuous actions. The deterministic policy gradient has a particularly
appealing form: it is the expected gradient of the action&#45;value function. This
simple form means that the deterministic policy gradient can be estimated much
more efficiently than the usual stochastic policy gradient. To ensure adequate
exploration, we introduce an off&#45;policy actor&#45;critic algorithm that learns a
deterministic target policy from an exploratory behaviour policy. We
demonstrate that deterministic policy gradient algorithms can significantly
outperform their stochastic counterparts in high&#45;dimensional action spaces.

(2014)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M831.5,-992C831.5,-992 801.5,-992 801.5,-992 795.5,-992 789.5,-986 789.5,-980 789.5,-980 789.5,-968 789.5,-968 789.5,-962 795.5,-956 801.5,-956 801.5,-956 831.5,-956 831.5,-956 837.5,-956 843.5,-962 843.5,-968 843.5,-968 843.5,-980 843.5,-980 843.5,-986 837.5,-992 831.5,-992"/>
<text text-anchor="middle" x="816.5" y="-970.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">DPG</text>
</a>
</g>
</g>
<!-- Policy Gradient\n/Actor&#45;Critic&#45;&gt;DPG -->
<g id="edge38" class="edge"><title>Policy Gradient\n/Actor&#45;Critic&#45;&gt;DPG</title>
<path fill="none" stroke="black" d="M409.662,-975.951C422.022,-972.244 435.357,-968.896 448,-967 568.158,-948.981 712.869,-961.638 779.143,-969.307"/>
<polygon fill="black" stroke="black" points="779.121,-972.829 789.465,-970.535 779.948,-965.878 779.121,-972.829"/>
</g>
<!-- TRPO -->
<g id="node39" class="node"><title>TRPO</title>
<g id="a_node39"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#TRPO" xlink:title="Trust Region Policy Optimization (TRPO) improves training stability by
enforcing a KL divergence constraint to avoid parameter updates that change
the policy too much at one step.

(2015)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M831.5,-1086C831.5,-1086 801.5,-1086 801.5,-1086 795.5,-1086 789.5,-1080 789.5,-1074 789.5,-1074 789.5,-1062 789.5,-1062 789.5,-1056 795.5,-1050 801.5,-1050 801.5,-1050 831.5,-1050 831.5,-1050 837.5,-1050 843.5,-1056 843.5,-1062 843.5,-1062 843.5,-1074 843.5,-1074 843.5,-1080 837.5,-1086 831.5,-1086"/>
<text text-anchor="middle" x="816.5" y="-1064.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">TRPO</text>
</a>
</g>
</g>
<!-- Policy Gradient\n/Actor&#45;Critic&#45;&gt;TRPO -->
<g id="edge39" class="edge"><title>Policy Gradient\n/Actor&#45;Critic&#45;&gt;TRPO</title>
<path fill="none" stroke="black" d="M383.552,-1012.12C396.298,-1020.92 412.731,-1030.7 429,-1036 552.622,-1076.24 709.637,-1073.98 779.344,-1070.44"/>
<polygon fill="black" stroke="black" points="779.58,-1073.94 789.373,-1069.89 779.196,-1066.95 779.58,-1073.94"/>
</g>
<!-- GAE -->
<g id="node40" class="node"><title>GAE</title>
<g id="a_node40"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#GAE" xlink:title="Generalized Advantage Estimation

(2015)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M831.5,-1158C831.5,-1158 801.5,-1158 801.5,-1158 795.5,-1158 789.5,-1152 789.5,-1146 789.5,-1146 789.5,-1134 789.5,-1134 789.5,-1128 795.5,-1122 801.5,-1122 801.5,-1122 831.5,-1122 831.5,-1122 837.5,-1122 843.5,-1128 843.5,-1134 843.5,-1134 843.5,-1146 843.5,-1146 843.5,-1152 837.5,-1158 831.5,-1158"/>
<text text-anchor="middle" x="816.5" y="-1136.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">GAE</text>
</a>
</g>
</g>
<!-- Policy Gradient\n/Actor&#45;Critic&#45;&gt;GAE -->
<g id="edge40" class="edge"><title>Policy Gradient\n/Actor&#45;Critic&#45;&gt;GAE</title>
<path fill="none" stroke="black" d="M379.033,-1012.15C395.871,-1027.2 421.775,-1048.01 448,-1060 561.859,-1112.07 711.323,-1131.02 779.182,-1137.25"/>
<polygon fill="black" stroke="black" points="779.086,-1140.76 789.355,-1138.15 779.699,-1133.78 779.086,-1140.76"/>
</g>
<!-- ACKTR -->
<g id="node41" class="node"><title>ACKTR</title>
<g id="a_node41"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#ACKTR" xlink:title="Actor Critic using Kronecker&#45;Factored Trust Region (ACKTR) is applying trust
region optimization to deep reinforcement learning using a recently proposed
Kronecker&#45;factored approximation to the curvature.

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1165,-932C1165,-932 1130,-932 1130,-932 1124,-932 1118,-926 1118,-920 1118,-920 1118,-908 1118,-908 1118,-902 1124,-896 1130,-896 1130,-896 1165,-896 1165,-896 1171,-896 1177,-902 1177,-908 1177,-908 1177,-920 1177,-920 1177,-926 1171,-932 1165,-932"/>
<text text-anchor="middle" x="1147.5" y="-910.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">ACKTR</text>
</a>
</g>
</g>
<!-- Policy Gradient\n/Actor&#45;Critic&#45;&gt;ACKTR -->
<g id="edge42" class="edge"><title>Policy Gradient\n/Actor&#45;Critic&#45;&gt;ACKTR</title>
<path fill="none" stroke="black" d="M381.412,-975.755C398.436,-962.125 423.4,-944.019 448,-933 475.743,-920.573 484.101,-917 514.5,-917 514.5,-917 514.5,-917 937.5,-917 996.88,-917 1065.74,-915.774 1107.7,-914.89"/>
<polygon fill="black" stroke="black" points="1107.84,-918.388 1117.77,-914.673 1107.69,-911.389 1107.84,-918.388"/>
</g>
<!-- SVPG -->
<g id="node42" class="node"><title>SVPG</title>
<g id="a_node42"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#SVPG" xlink:title="Stein Variational Policy Gradient (SVPG)

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1162.5,-986C1162.5,-986 1132.5,-986 1132.5,-986 1126.5,-986 1120.5,-980 1120.5,-974 1120.5,-974 1120.5,-962 1120.5,-962 1120.5,-956 1126.5,-950 1132.5,-950 1132.5,-950 1162.5,-950 1162.5,-950 1168.5,-950 1174.5,-956 1174.5,-962 1174.5,-962 1174.5,-974 1174.5,-974 1174.5,-980 1168.5,-986 1162.5,-986"/>
<text text-anchor="middle" x="1147.5" y="-964.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">SVPG</text>
</a>
</g>
</g>
<!-- Policy Gradient\n/Actor&#45;Critic&#45;&gt;SVPG -->
<g id="edge43" class="edge"><title>Policy Gradient\n/Actor&#45;Critic&#45;&gt;SVPG</title>
<path fill="none" stroke="black" d="M389.586,-975.965C419.84,-959.266 468.756,-937 514.5,-937 514.5,-937 514.5,-937 937.5,-937 1004.2,-937 1021.51,-937.387 1087,-950 1094.74,-951.49 1102.9,-953.594 1110.62,-955.841"/>
<polygon fill="black" stroke="black" points="1109.85,-959.265 1120.43,-958.837 1111.89,-952.57 1109.85,-959.265"/>
</g>
<!-- SAC -->
<g id="node43" class="node"><title>SAC</title>
<g id="a_node43"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#SAC" xlink:title="Soft Actor Critic (SAC) is an algorithm that optimizes a stochastic policy in
an off&#45;policy way, forming a bridge between stochastic policy optimization and
DDPG&#45;style approaches.

(2018)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1317.5,-902C1317.5,-902 1287.5,-902 1287.5,-902 1281.5,-902 1275.5,-896 1275.5,-890 1275.5,-890 1275.5,-878 1275.5,-878 1275.5,-872 1281.5,-866 1287.5,-866 1287.5,-866 1317.5,-866 1317.5,-866 1323.5,-866 1329.5,-872 1329.5,-878 1329.5,-878 1329.5,-890 1329.5,-890 1329.5,-896 1323.5,-902 1317.5,-902"/>
<text text-anchor="middle" x="1302.5" y="-880.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">SAC</text>
</a>
</g>
</g>
<!-- Policy Gradient\n/Actor&#45;Critic&#45;&gt;SAC -->
<g id="edge44" class="edge"><title>Policy Gradient\n/Actor&#45;Critic&#45;&gt;SAC</title>
<path fill="none" stroke="black" d="M371.847,-975.975C395.009,-943.156 449.731,-877 514.5,-877 514.5,-877 514.5,-877 1148.5,-877 1188.34,-877 1233.96,-879.398 1264.94,-881.387"/>
<polygon fill="black" stroke="black" points="1265.05,-884.901 1275.26,-882.069 1265.51,-877.917 1265.05,-884.901"/>
</g>
<!-- IMPALA -->
<g id="node44" class="node"><title>IMPALA</title>
<g id="a_node44"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#IMPALA" xlink:title="Importance Weighted Actor&#45;Learner Architecture (IMPALA)

(2018)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1321.5,-1163C1321.5,-1163 1283.5,-1163 1283.5,-1163 1277.5,-1163 1271.5,-1157 1271.5,-1151 1271.5,-1151 1271.5,-1139 1271.5,-1139 1271.5,-1133 1277.5,-1127 1283.5,-1127 1283.5,-1127 1321.5,-1127 1321.5,-1127 1327.5,-1127 1333.5,-1133 1333.5,-1139 1333.5,-1139 1333.5,-1151 1333.5,-1151 1333.5,-1157 1327.5,-1163 1321.5,-1163"/>
<text text-anchor="middle" x="1302.5" y="-1141.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">IMPALA</text>
</a>
</g>
</g>
<!-- Policy Gradient\n/Actor&#45;Critic&#45;&gt;IMPALA -->
<g id="edge45" class="edge"><title>Policy Gradient\n/Actor&#45;Critic&#45;&gt;IMPALA</title>
<path fill="none" stroke="black" d="M365.577,-1012.22C380.999,-1058.5 428.846,-1177 514.5,-1177 514.5,-1177 514.5,-1177 871,-1177 1012.78,-1177 1049.5,-1194.02 1190,-1175 1214.18,-1171.73 1240.57,-1164.78 1261.61,-1158.4"/>
<polygon fill="black" stroke="black" points="1262.66,-1161.74 1271.17,-1155.43 1260.58,-1155.06 1262.66,-1161.74"/>
</g>
<!-- DQN -->
<g id="node14" class="node"><title>DQN</title>
<g id="a_node14"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#DQN" xlink:title="Deep Q Network (DQN) is Q&#45;Learning with deep neural network as state&#45;action
value estimator and uses a replay buffer to sample experiences from previous
trajectories to make learning more stable.

(2013)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M831.5,-1312C831.5,-1312 801.5,-1312 801.5,-1312 795.5,-1312 789.5,-1306 789.5,-1300 789.5,-1300 789.5,-1288 789.5,-1288 789.5,-1282 795.5,-1276 801.5,-1276 801.5,-1276 831.5,-1276 831.5,-1276 837.5,-1276 843.5,-1282 843.5,-1288 843.5,-1288 843.5,-1300 843.5,-1300 843.5,-1306 837.5,-1312 831.5,-1312"/>
<text text-anchor="middle" x="816.5" y="-1290.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">DQN</text>
</a>
</g>
</g>
<!-- DDPG -->
<g id="node15" class="node"><title>DDPG</title>
<g id="a_node15"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#DDPG" xlink:title="Deep Deterministic Policy Gradient (DDPG).

(2015)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M831.5,-809C831.5,-809 801.5,-809 801.5,-809 795.5,-809 789.5,-803 789.5,-797 789.5,-797 789.5,-785 789.5,-785 789.5,-779 795.5,-773 801.5,-773 801.5,-773 831.5,-773 831.5,-773 837.5,-773 843.5,-779 843.5,-785 843.5,-785 843.5,-797 843.5,-797 843.5,-803 837.5,-809 831.5,-809"/>
<text text-anchor="middle" x="816.5" y="-787.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">DDPG</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;DDPG -->
<g id="edge11" class="edge"><title>DQN&#45;&gt;DDPG</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M805.569,-1275.89C797.685,-1262.1 787.352,-1242.42 781,-1224 755.888,-1151.17 685.053,-954.043 716,-883.5 729.236,-853.329 757.707,-828.661 780.978,-812.41"/>
<polygon fill="darkgray" stroke="darkgray" points="782.947,-815.304 789.278,-806.809 779.031,-809.501 782.947,-815.304"/>
<text text-anchor="middle" x="732.5" y="-886.5" font-family="sans-serif" font-size="10.00" fill="darkgray">replay buffer</text>
</g>
<!-- ACER -->
<g id="node16" class="node"><title>ACER</title>
<g id="a_node16"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#ACER" xlink:title="Actor&#45;Critic with Experience Replay (ACER) combines several ideas of previous
algorithms: it uses multiple workers (as A2C), implements a replay buffer (as
in DQN), uses Retrace for Q&#45;value estimation, importance sampling and a trust
region. ACER is A3C&#39;s off&#45;policy counterpart. ACER proposes several designs to
overcome the major obstacle to making A3C off policy, that is how to control
the stability of the off&#45;policy estimator. (source: Lilian Weng&#39;s blog)

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1162.5,-1166C1162.5,-1166 1132.5,-1166 1132.5,-1166 1126.5,-1166 1120.5,-1160 1120.5,-1154 1120.5,-1154 1120.5,-1142 1120.5,-1142 1120.5,-1136 1126.5,-1130 1132.5,-1130 1132.5,-1130 1162.5,-1130 1162.5,-1130 1168.5,-1130 1174.5,-1136 1174.5,-1142 1174.5,-1142 1174.5,-1154 1174.5,-1154 1174.5,-1160 1168.5,-1166 1162.5,-1166"/>
<text text-anchor="middle" x="1147.5" y="-1144.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">ACER</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;ACER -->
<g id="edge12" class="edge"><title>DQN&#45;&gt;ACER</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M831.159,-1275.89C844.138,-1260.2 865.172,-1238.44 889,-1228 965.215,-1194.6 1059.74,-1169.01 1110.4,-1156.49"/>
<polygon fill="darkgray" stroke="darkgray" points="1111.34,-1159.86 1120.22,-1154.09 1109.68,-1153.06 1111.34,-1159.86"/>
<text text-anchor="middle" x="936.5" y="-1231" font-family="sans-serif" font-size="10.00" fill="darkgray">replay buffer</text>
</g>
<!-- DDQN -->
<g id="node17" class="node"><title>DDQN</title>
<g id="a_node17"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#DDQN" xlink:title="Double DQN adds another neural network, making separate network for policy and
target. The target network is only updated after certain number of
steps/episodes. This makes the learning more stable.

(2015)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M831.5,-1492C831.5,-1492 801.5,-1492 801.5,-1492 795.5,-1492 789.5,-1486 789.5,-1480 789.5,-1480 789.5,-1468 789.5,-1468 789.5,-1462 795.5,-1456 801.5,-1456 801.5,-1456 831.5,-1456 831.5,-1456 837.5,-1456 843.5,-1462 843.5,-1468 843.5,-1468 843.5,-1480 843.5,-1480 843.5,-1486 837.5,-1492 831.5,-1492"/>
<text text-anchor="middle" x="816.5" y="-1470.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">DDQN</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;DDQN -->
<g id="edge22" class="edge"><title>DQN&#45;&gt;DDQN</title>
<path fill="none" stroke="black" d="M789.203,-1303.38C769.546,-1311.74 744.463,-1326.09 732.5,-1348 713.332,-1383.11 710.48,-1404.61 732.5,-1438 742.941,-1453.83 761.946,-1462.73 779.242,-1467.72"/>
<polygon fill="black" stroke="black" points="778.597,-1471.16 789.15,-1470.22 780.313,-1464.38 778.597,-1471.16"/>
</g>
<!-- DQN+HER -->
<g id="node19" class="node"><title>DQN+HER</title>
<g id="a_node19"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#DQNHER" xlink:title="DQN with Hindsight Experience Replay (HER)

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1174,-1272C1174,-1272 1121,-1272 1121,-1272 1115,-1272 1109,-1266 1109,-1260 1109,-1260 1109,-1248 1109,-1248 1109,-1242 1115,-1236 1121,-1236 1121,-1236 1174,-1236 1174,-1236 1180,-1236 1186,-1242 1186,-1248 1186,-1248 1186,-1260 1186,-1260 1186,-1266 1180,-1272 1174,-1272"/>
<text text-anchor="middle" x="1147.5" y="-1250.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">DQN+HER</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;DQN+HER -->
<g id="edge26" class="edge"><title>DQN&#45;&gt;DQN+HER</title>
<path fill="none" stroke="black" d="M843.644,-1289.83C852.05,-1288.54 861.41,-1287.15 870,-1286 950.092,-1275.27 1043.53,-1264.9 1098.7,-1259"/>
<polygon fill="black" stroke="black" points="1099.31,-1262.46 1108.88,-1257.92 1098.56,-1255.5 1099.31,-1262.46"/>
</g>
<!-- APE&#45;X DQN -->
<g id="node21" class="node"><title>APE&#45;X DQN</title>
<g id="a_node21"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#APEXDQN" xlink:title="DQN with Distributed Prioritized Experience Replay

(2018)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1332,-1309C1332,-1309 1273,-1309 1273,-1309 1267,-1309 1261,-1303 1261,-1297 1261,-1297 1261,-1285 1261,-1285 1261,-1279 1267,-1273 1273,-1273 1273,-1273 1332,-1273 1332,-1273 1338,-1273 1344,-1279 1344,-1285 1344,-1285 1344,-1297 1344,-1297 1344,-1303 1338,-1309 1332,-1309"/>
<text text-anchor="middle" x="1302.5" y="-1287.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">APE&#45;X DQN</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;APE&#45;X DQN -->
<g id="edge28" class="edge"><title>DQN&#45;&gt;APE&#45;X DQN</title>
<path fill="none" stroke="black" d="M843.522,-1293.84C920.594,-1293.36 1148.66,-1291.95 1250.91,-1291.31"/>
<polygon fill="black" stroke="black" points="1251.01,-1294.81 1260.99,-1291.25 1250.97,-1287.81 1251.01,-1294.81"/>
</g>
<!-- DRQN -->
<g id="node28" class="node"><title>DRQN</title>
<g id="a_node28"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#DRQN" xlink:title="Deep Recurrent Q&#45;Learning. Adding recurrency to a Deep Q&#45;Network (DQN) by
replacing the first post&#45;convolutional fully&#45;connected layer with a recurrent
LSTM

(2015)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M831.5,-1384C831.5,-1384 801.5,-1384 801.5,-1384 795.5,-1384 789.5,-1378 789.5,-1372 789.5,-1372 789.5,-1360 789.5,-1360 789.5,-1354 795.5,-1348 801.5,-1348 801.5,-1348 831.5,-1348 831.5,-1348 837.5,-1348 843.5,-1354 843.5,-1360 843.5,-1360 843.5,-1372 843.5,-1372 843.5,-1378 837.5,-1384 831.5,-1384"/>
<text text-anchor="middle" x="816.5" y="-1362.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">DRQN</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;DRQN -->
<g id="edge21" class="edge"><title>DQN&#45;&gt;DRQN</title>
<path fill="none" stroke="black" d="M816.5,-1312.28C816.5,-1320.83 816.5,-1329.37 816.5,-1337.92"/>
<polygon fill="black" stroke="black" points="813,-1337.95 816.5,-1347.95 820,-1337.95 813,-1337.95"/>
</g>
<!-- PER -->
<g id="node29" class="node"><title>PER</title>
<g id="a_node29"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#PER" xlink:title="Prioritized Experience Replay (PER) improves data efficiency by replaying
transitions from which there is more to learn more often

(2015)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M831.5,-1438C831.5,-1438 801.5,-1438 801.5,-1438 795.5,-1438 789.5,-1432 789.5,-1426 789.5,-1426 789.5,-1414 789.5,-1414 789.5,-1408 795.5,-1402 801.5,-1402 801.5,-1402 831.5,-1402 831.5,-1402 837.5,-1402 843.5,-1408 843.5,-1414 843.5,-1414 843.5,-1426 843.5,-1426 843.5,-1432 837.5,-1438 831.5,-1438"/>
<text text-anchor="middle" x="816.5" y="-1416.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">PER</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;PER -->
<g id="edge23" class="edge"><title>DQN&#45;&gt;PER</title>
<path fill="none" stroke="black" d="M789.203,-1303.38C769.546,-1311.74 744.463,-1326.09 732.5,-1348 724.833,-1362.04 723.692,-1370.64 732.5,-1384 742.941,-1399.83 761.946,-1408.73 779.242,-1413.72"/>
<polygon fill="black" stroke="black" points="778.597,-1417.16 789.15,-1416.22 780.313,-1410.38 778.597,-1417.16"/>
</g>
<!-- QR&#45;DQN -->
<g id="node30" class="node"><title>QR&#45;DQN</title>
<g id="a_node30"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#QRDQN" xlink:title="Distributional Reinforcement Learning with Quantile Regression (QR&#45;DQN). In
QR&#45;DQN, distribution of values values are used for each state&#45;action pair
instead of a single mean value

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1168,-1346C1168,-1346 1127,-1346 1127,-1346 1121,-1346 1115,-1340 1115,-1334 1115,-1334 1115,-1322 1115,-1322 1115,-1316 1121,-1310 1127,-1310 1127,-1310 1168,-1310 1168,-1310 1174,-1310 1180,-1316 1180,-1322 1180,-1322 1180,-1334 1180,-1334 1180,-1340 1174,-1346 1168,-1346"/>
<text text-anchor="middle" x="1147.5" y="-1324.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">QR&#45;DQN</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;QR&#45;DQN -->
<g id="edge24" class="edge"><title>DQN&#45;&gt;QR&#45;DQN</title>
<path fill="none" stroke="black" d="M843.591,-1296.7C900.404,-1302.57 1035.72,-1316.55 1104.63,-1323.67"/>
<polygon fill="black" stroke="black" points="1104.39,-1327.17 1114.7,-1324.71 1105.11,-1320.2 1104.39,-1327.17"/>
</g>
<!-- C51 -->
<g id="node31" class="node"><title>C51</title>
<g id="a_node31"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#C51" xlink:title="C51 Algorithm. The core idea of Distributional Bellman is to ask the following
questions. If we can model the Distribution of the total future rewards, why
restrict ourselves to the expected value (i.e. Q function)? There are several
benefits to learning an approximate distribution rather than its approximate
expectation. [source: flyyufelix&#39;s blog]

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1162.5,-1400C1162.5,-1400 1132.5,-1400 1132.5,-1400 1126.5,-1400 1120.5,-1394 1120.5,-1388 1120.5,-1388 1120.5,-1376 1120.5,-1376 1120.5,-1370 1126.5,-1364 1132.5,-1364 1132.5,-1364 1162.5,-1364 1162.5,-1364 1168.5,-1364 1174.5,-1370 1174.5,-1376 1174.5,-1376 1174.5,-1388 1174.5,-1388 1174.5,-1394 1168.5,-1400 1162.5,-1400"/>
<text text-anchor="middle" x="1147.5" y="-1378.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">C51</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;C51 -->
<g id="edge25" class="edge"><title>DQN&#45;&gt;C51</title>
<path fill="none" stroke="black" d="M843.591,-1300.98C902.019,-1316.61 1043.48,-1354.44 1110.34,-1372.33"/>
<polygon fill="black" stroke="black" points="1109.84,-1375.82 1120.41,-1375.02 1111.65,-1369.06 1109.84,-1375.82"/>
</g>
<!-- IQN -->
<g id="node32" class="node"><title>IQN</title>
<g id="a_node32"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#IQN" xlink:title="Implicit Quantile Networks (IQN). From the abstract: In this work, we build on
recent advances in distributional reinforcement learning to give a generally
applicable, flexible, and state&#45;of&#45;the&#45;art distributional variant of DQN. We
achieve this by using quantile regression to approximate the full quantile
function for the state&#45;action return distribution. By reparameterizing a
distribution over the sample space, this yields an implicitly defined return
distribution and gives rise to a large class of risk&#45;sensitive policies. We
demonstrate improved performance on the 57 Atari 2600 games in the ALE, and
use our algorithm&#39;s implicitly defined distributions to study the effects of
risk&#45;sensitive policies in Atari games.

(2018)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1317.5,-1420C1317.5,-1420 1287.5,-1420 1287.5,-1420 1281.5,-1420 1275.5,-1414 1275.5,-1408 1275.5,-1408 1275.5,-1396 1275.5,-1396 1275.5,-1390 1281.5,-1384 1287.5,-1384 1287.5,-1384 1317.5,-1384 1317.5,-1384 1323.5,-1384 1329.5,-1390 1329.5,-1396 1329.5,-1396 1329.5,-1408 1329.5,-1408 1329.5,-1414 1323.5,-1420 1317.5,-1420"/>
<text text-anchor="middle" x="1302.5" y="-1398.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">IQN</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;IQN -->
<g id="edge27" class="edge"><title>DQN&#45;&gt;IQN</title>
<path fill="none" stroke="black" d="M843.553,-1308.62C856.921,-1315.83 873.594,-1324.37 889,-1331 974.599,-1367.84 1001.15,-1363.75 1087,-1400 1095.24,-1403.48 1096.29,-1406.95 1105,-1409 1141.77,-1417.68 1152.24,-1410.18 1190,-1409 1215.16,-1408.21 1243.46,-1406.42 1265.25,-1404.85"/>
<polygon fill="black" stroke="black" points="1265.67,-1408.33 1275.39,-1404.1 1265.16,-1401.35 1265.67,-1408.33"/>
</g>
<!-- R2D2 -->
<g id="node33" class="node"><title>R2D2</title>
<g id="a_node33"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#R2D2" xlink:title="Recurrent Replay Distributed DQN (R2D2). (from the abstract) Building on the
recent successes of distributed training of RL agents, in this paper we
investigate the training of RNN&#45;based RL agents from distributed prioritized
experience replay. We study the effects of parameter lag resulting in
representational drift and recurrent state staleness and empirically derive an
improved training strategy. Using a single network architecture and fixed set
of hyper&#45;parameters, the resulting agent, Recurrent Replay Distributed DQN,
quadruples the previous state of the art on Atari&#45;57, and matches the state of
the art on DMLab&#45;30. It is the first agent to exceed human&#45;level performance
in 52 of the 57 Atari games.

(2019)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1432.5,-1448C1432.5,-1448 1402.5,-1448 1402.5,-1448 1396.5,-1448 1390.5,-1442 1390.5,-1436 1390.5,-1436 1390.5,-1424 1390.5,-1424 1390.5,-1418 1396.5,-1412 1402.5,-1412 1402.5,-1412 1432.5,-1412 1432.5,-1412 1438.5,-1412 1444.5,-1418 1444.5,-1424 1444.5,-1424 1444.5,-1436 1444.5,-1436 1444.5,-1442 1438.5,-1448 1432.5,-1448"/>
<text text-anchor="middle" x="1417.5" y="-1426.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">R2D2</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;R2D2 -->
<g id="edge29" class="edge"><title>DQN&#45;&gt;R2D2</title>
<path fill="none" stroke="black" d="M835.842,-1312.07C849.53,-1324.72 869.211,-1341.22 889,-1352 1034.96,-1431.54 1088.95,-1421.3 1255,-1429 1298.09,-1431 1347.72,-1430.96 1380.46,-1430.6"/>
<polygon fill="black" stroke="black" points="1380.51,-1434.1 1390.47,-1430.48 1380.43,-1427.1 1380.51,-1434.1"/>
</g>
<!-- TD3 -->
<g id="node18" class="node"><title>TD3</title>
<g id="a_node18"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#TD3" xlink:title="Twin Delayed DDPG (TD3). TD3 addresses function approximation error in DDPG by
introducing twin Q&#45;value approximation network and less frequent updates

(2018)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1317.5,-1033C1317.5,-1033 1287.5,-1033 1287.5,-1033 1281.5,-1033 1275.5,-1027 1275.5,-1021 1275.5,-1021 1275.5,-1009 1275.5,-1009 1275.5,-1003 1281.5,-997 1287.5,-997 1287.5,-997 1317.5,-997 1317.5,-997 1323.5,-997 1329.5,-1003 1329.5,-1009 1329.5,-1009 1329.5,-1021 1329.5,-1021 1329.5,-1027 1323.5,-1033 1317.5,-1033"/>
<text text-anchor="middle" x="1302.5" y="-1011.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">TD3</text>
</a>
</g>
</g>
<!-- DDPG&#45;&gt;TD3 -->
<g id="edge51" class="edge"><title>DDPG&#45;&gt;TD3</title>
<path fill="none" stroke="black" d="M843.652,-775.216C910.715,-736.985 1090.51,-648.961 1190,-739 1219.81,-765.974 1191.03,-882.56 1208,-919 1221.42,-947.799 1247.12,-973.189 1268.19,-990.613"/>
<polygon fill="black" stroke="black" points="1266.02,-993.355 1276,-996.892 1270.4,-987.899 1266.02,-993.355"/>
</g>
<!-- DDPG+HER -->
<g id="node20" class="node"><title>DDPG+HER</title>
<g id="a_node20"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#DDPGHER" xlink:title="Hindsight Experience Replay (HER)

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1178,-838C1178,-838 1117,-838 1117,-838 1111,-838 1105,-832 1105,-826 1105,-826 1105,-814 1105,-814 1105,-808 1111,-802 1117,-802 1117,-802 1178,-802 1178,-802 1184,-802 1190,-808 1190,-814 1190,-814 1190,-826 1190,-826 1190,-832 1184,-838 1178,-838"/>
<text text-anchor="middle" x="1147.5" y="-816.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">DDPG+HER</text>
</a>
</g>
</g>
<!-- DDPG&#45;&gt;DDPG+HER -->
<g id="edge47" class="edge"><title>DDPG&#45;&gt;DDPG+HER</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M843.591,-793.3C897.768,-798.075 1023.34,-809.144 1094.62,-815.427"/>
<polygon fill="darkgray" stroke="darkgray" points="1094.47,-818.927 1104.73,-816.319 1095.08,-811.954 1094.47,-818.927"/>
</g>
<!-- APE&#45;X DDPG -->
<g id="node22" class="node"><title>APE&#45;X DDPG</title>
<g id="a_node22"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#APEXDDPG" xlink:title="DDPG with Distributed Prioritized Experience Replay

(2018)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1336.5,-744C1336.5,-744 1268.5,-744 1268.5,-744 1262.5,-744 1256.5,-738 1256.5,-732 1256.5,-732 1256.5,-720 1256.5,-720 1256.5,-714 1262.5,-708 1268.5,-708 1268.5,-708 1336.5,-708 1336.5,-708 1342.5,-708 1348.5,-714 1348.5,-720 1348.5,-720 1348.5,-732 1348.5,-732 1348.5,-738 1342.5,-744 1336.5,-744"/>
<text text-anchor="middle" x="1302.5" y="-722.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">APE&#45;X DDPG</text>
</a>
</g>
</g>
<!-- DDPG&#45;&gt;APE&#45;X DDPG -->
<g id="edge50" class="edge"><title>DDPG&#45;&gt;APE&#45;X DDPG</title>
<path fill="none" stroke="black" d="M843.577,-784.803C857.094,-781.682 873.893,-777.947 889,-775 984.524,-756.366 1008.33,-750.226 1105,-739 1152.39,-733.497 1206.49,-730.131 1245.92,-728.207"/>
<polygon fill="black" stroke="black" points="1246.29,-731.693 1256.12,-727.725 1245.96,-724.701 1246.29,-731.693"/>
</g>
<!-- MADDPG -->
<g id="node45" class="node"><title>MADDPG</title>
<g id="a_node45"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#MADDPG" xlink:title="Multi&#45;agent DDPG (MADDPG) extends DDPG to an environment where multiple agents
are coordinating to complete tasks with only local information. In the
viewpoint of one agent, the environment is non&#45;stationary as policies of other
agents are quickly upgraded and remain unknown. MADDPG is an actor&#45;critic
model redesigned particularly for handling such a changing environment and
interactions between agents (from Lilian Weng&#39;s blog)

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1170.5,-784C1170.5,-784 1124.5,-784 1124.5,-784 1118.5,-784 1112.5,-778 1112.5,-772 1112.5,-772 1112.5,-760 1112.5,-760 1112.5,-754 1118.5,-748 1124.5,-748 1124.5,-748 1170.5,-748 1170.5,-748 1176.5,-748 1182.5,-754 1182.5,-760 1182.5,-760 1182.5,-772 1182.5,-772 1182.5,-778 1176.5,-784 1170.5,-784"/>
<text text-anchor="middle" x="1147.5" y="-762.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">MADDPG</text>
</a>
</g>
</g>
<!-- DDPG&#45;&gt;MADDPG -->
<g id="edge48" class="edge"><title>DDPG&#45;&gt;MADDPG</title>
<path fill="none" stroke="black" d="M843.591,-789.017C899.754,-784.75 1032.63,-774.652 1102.24,-769.363"/>
<polygon fill="black" stroke="black" points="1102.73,-772.836 1112.44,-768.588 1102.2,-765.856 1102.73,-772.836"/>
</g>
<!-- D4PG -->
<g id="node46" class="node"><title>D4PG</title>
<g id="a_node46"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#D4PG" xlink:title="Distributed Distributional Deep Deterministic Policy Gradient (D4PG) adopts
the very successful distributional perspective on reinforcement learning and
adapts it to the continuous control setting. It combines this within a
distributed framework. It also combines this technique with a number of
additional, simple improvements such as the use of N&#45;step returns and
prioritized experience replay [from the paper&#39;s abstract]

(2018)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1317.5,-848C1317.5,-848 1287.5,-848 1287.5,-848 1281.5,-848 1275.5,-842 1275.5,-836 1275.5,-836 1275.5,-824 1275.5,-824 1275.5,-818 1281.5,-812 1287.5,-812 1287.5,-812 1317.5,-812 1317.5,-812 1323.5,-812 1329.5,-818 1329.5,-824 1329.5,-824 1329.5,-836 1329.5,-836 1329.5,-842 1323.5,-848 1317.5,-848"/>
<text text-anchor="middle" x="1302.5" y="-826.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">D4PG</text>
</a>
</g>
</g>
<!-- DDPG&#45;&gt;D4PG -->
<g id="edge49" class="edge"><title>DDPG&#45;&gt;D4PG</title>
<path fill="none" stroke="black" d="M843.543,-799.02C904.045,-816.746 1058.93,-856.935 1190,-847 1215.34,-845.08 1243.64,-840.738 1265.39,-836.923"/>
<polygon fill="black" stroke="black" points="1266.27,-840.32 1275.49,-835.106 1265.03,-833.431 1266.27,-840.32"/>
</g>
<!-- DDQN&#45;&gt;TD3 -->
<g id="edge13" class="edge"><title>DDQN&#45;&gt;TD3</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M842.319,-1455.89C845.698,-1453.05 849.023,-1450.05 852,-1447 929.91,-1367.28 912.953,-1313.05 1002,-1246 1073.35,-1192.28 1119.53,-1229.88 1190,-1175 1236.64,-1138.68 1271.52,-1077.49 1288.99,-1042.23"/>
<polygon fill="darkgray" stroke="darkgray" points="1292.24,-1043.55 1293.45,-1033.02 1285.94,-1040.5 1292.24,-1043.55"/>
<text text-anchor="middle" x="1044.5" y="-1249" font-family="sans-serif" font-size="10.00" fill="darkgray">double Q&#45;learning</text>
</g>
<!-- RAINBOW -->
<g id="node24" class="node"><title>RAINBOW</title>
<g id="a_node24"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#RAINBOW" xlink:title="Combines six DQN extensions, namely Double Q&#45;Learning, prioritized replay,
dueling networks, multi&#45;step learning, distributional DQN, and noisy DQN into
single model to achieve state of the art performance

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1173,-1494C1173,-1494 1122,-1494 1122,-1494 1116,-1494 1110,-1488 1110,-1482 1110,-1482 1110,-1470 1110,-1470 1110,-1464 1116,-1458 1122,-1458 1122,-1458 1173,-1458 1173,-1458 1179,-1458 1185,-1464 1185,-1470 1185,-1470 1185,-1482 1185,-1482 1185,-1488 1179,-1494 1173,-1494"/>
<text text-anchor="middle" x="1147.5" y="-1472.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">RAINBOW</text>
</a>
</g>
</g>
<!-- DDQN&#45;&gt;RAINBOW -->
<g id="edge31" class="edge"><title>DDQN&#45;&gt;RAINBOW</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M843.575,-1478.08C857.092,-1479.99 873.891,-1482.05 889,-1483 962.492,-1487.61 1047.97,-1483.39 1099.61,-1479.8"/>
<polygon fill="darkgray" stroke="darkgray" points="1100.07,-1483.27 1109.79,-1479.07 1099.57,-1476.29 1100.07,-1483.27"/>
</g>
<!-- Duelling&#45;DQN -->
<g id="node34" class="node"><title>Duelling&#45;DQN</title>
<g id="a_node34"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#DuellingDQN" xlink:title="Duelling DQN represents two separate estimators: one for the state value
function and one for the state&#45;dependent action advantage function. The main
benefit of this factoring is to generalize learning across actions without
imposing any change to the underlying reinforcement learning algorithm.

(2016)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M972,-1474C972,-1474 901,-1474 901,-1474 895,-1474 889,-1468 889,-1462 889,-1462 889,-1450 889,-1450 889,-1444 895,-1438 901,-1438 901,-1438 972,-1438 972,-1438 978,-1438 984,-1444 984,-1450 984,-1450 984,-1462 984,-1462 984,-1468 978,-1474 972,-1474"/>
<text text-anchor="middle" x="936.5" y="-1452.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">Duelling&#45;DQN</text>
</a>
</g>
</g>
<!-- DDQN&#45;&gt;Duelling&#45;DQN -->
<g id="edge30" class="edge"><title>DDQN&#45;&gt;Duelling&#45;DQN</title>
<path fill="none" stroke="black" d="M843.688,-1470.01C854.085,-1468.42 866.475,-1466.53 878.736,-1464.66"/>
<polygon fill="black" stroke="black" points="879.601,-1468.07 888.959,-1463.1 878.545,-1461.15 879.601,-1468.07"/>
</g>
<!-- DQN+HER&#45;&gt;DDPG+HER -->
<g id="edge14" class="edge"><title>DQN+HER&#45;&gt;DDPG+HER</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M1115.71,-1235.78C1111.62,-1232.32 1107.87,-1228.38 1105,-1224 1071.53,-1172.93 1032.88,-1012.65 1051.5,-954.5 1066.64,-907.234 1105.16,-862.427 1128.54,-838.342"/>
<text text-anchor="middle" x="1044.5" y="-957.5" font-family="sans-serif" font-size="10.00" fill="darkgray">HER</text>
</g>
<!-- APE&#45;X DQN&#45;&gt;APE&#45;X DDPG -->
<g id="edge15" class="edge"><title>APE&#45;X DQN&#45;&gt;APE&#45;X DDPG</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M1285.74,-1272.87C1275.04,-1260.17 1261.93,-1242.27 1255,-1224 1228.58,-1154.32 1205.91,-963.674 1220,-890.5 1231.12,-832.774 1268.87,-773.048 1289.23,-744.019"/>
<text text-anchor="middle" x="1222.5" y="-893.5" font-family="sans-serif" font-size="10.00" fill="darkgray">APE&#45;X</text>
</g>
<!-- A3C&#45;&gt;ACER -->
<g id="edge56" class="edge"><title>A3C&#45;&gt;ACER</title>
<path fill="none" stroke="black" d="M963.547,-1094.6C1000.65,-1105.43 1068.84,-1125.33 1110.67,-1137.54"/>
<polygon fill="black" stroke="black" points="1109.87,-1140.96 1120.45,-1140.4 1111.83,-1134.24 1109.87,-1140.96"/>
</g>
<!-- A3C&#45;&gt;RAINBOW -->
<g id="edge16" class="edge"><title>A3C&#45;&gt;RAINBOW</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M963.69,-1101.25C998.825,-1121.96 1059.93,-1164.02 1087,-1219 1124.47,-1295.1 1077.95,-1328.6 1105,-1409 1109.8,-1423.26 1118.13,-1437.63 1126.08,-1449.39"/>
<polygon fill="darkgray" stroke="darkgray" points="1123.4,-1451.66 1132.01,-1457.82 1129.12,-1447.63 1123.4,-1451.66"/>
</g>
<!-- A2C -->
<g id="node48" class="node"><title>A2C</title>
<g id="a_node48"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#A2C" xlink:title="A2C is a synchronous, deterministic variant of Asynchronous Advantage Actor
Critic (A3C). It uses multiple workers to avoid the use of a replay buffer.

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1162.5,-1094C1162.5,-1094 1132.5,-1094 1132.5,-1094 1126.5,-1094 1120.5,-1088 1120.5,-1082 1120.5,-1082 1120.5,-1070 1120.5,-1070 1120.5,-1064 1126.5,-1058 1132.5,-1058 1132.5,-1058 1162.5,-1058 1162.5,-1058 1168.5,-1058 1174.5,-1064 1174.5,-1070 1174.5,-1070 1174.5,-1082 1174.5,-1082 1174.5,-1088 1168.5,-1094 1162.5,-1094"/>
<text text-anchor="middle" x="1147.5" y="-1072.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">A2C</text>
</a>
</g>
</g>
<!-- A3C&#45;&gt;A2C -->
<g id="edge55" class="edge"><title>A3C&#45;&gt;A2C</title>
<path fill="none" stroke="black" d="M963.547,-1085.63C1000.57,-1083.68 1068.55,-1080.1 1110.41,-1077.9"/>
<polygon fill="black" stroke="black" points="1110.65,-1081.39 1120.45,-1077.37 1110.28,-1074.4 1110.65,-1081.39"/>
</g>
<!-- Q&#45;learning&#45;&gt;DQN -->
<g id="edge20" class="edge"><title>Q&#45;learning&#45;&gt;DQN</title>
<path fill="none" stroke="black" d="M553.764,-1306.26C611.535,-1303.55 721.999,-1298.38 779.186,-1295.7"/>
<polygon fill="black" stroke="black" points="779.394,-1299.19 789.22,-1295.23 779.067,-1292.2 779.394,-1299.19"/>
</g>
<!-- PER&#45;&gt;RAINBOW -->
<g id="edge32" class="edge"><title>PER&#45;&gt;RAINBOW</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M843.649,-1419.64C876.603,-1419.6 934.879,-1420.86 984,-1429 1030.92,-1436.78 1041.35,-1444.67 1087,-1458 1091.25,-1459.24 1095.67,-1460.55 1100.09,-1461.87"/>
<polygon fill="darkgray" stroke="darkgray" points="1099.13,-1465.23 1109.72,-1464.75 1101.14,-1458.53 1099.13,-1465.23"/>
</g>
<!-- QR&#45;DQN&#45;&gt;RAINBOW -->
<g id="edge34" class="edge"><title>QR&#45;DQN&#45;&gt;RAINBOW</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M1114.67,-1329.87C1090.9,-1333.15 1060.29,-1341.76 1044.5,-1364 1026.24,-1389.73 1026.79,-1408.88 1044.5,-1435 1057.03,-1453.48 1079.36,-1463.63 1100.03,-1469.21"/>
<polygon fill="darkgray" stroke="darkgray" points="1099.46,-1472.67 1110,-1471.59 1101.09,-1465.86 1099.46,-1472.67"/>
</g>
<!-- NGU -->
<g id="node35" class="node"><title>NGU</title>
<g id="a_node35"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#NGU" xlink:title="Never Give Up (NGU). (from the abstract) We propose a reinforcement learning
agent to solve hard exploration games by learning a range of directed
exploratory policies. We construct an episodic memory&#45;based intrinsic reward
using k&#45;nearest neighbors over the agent&#39;s recent experience to train the
directed exploratory policies, thereby encouraging the agent to repeatedly
revisit all states in its environment. A self&#45;supervised inverse dynamics
model is used to train the embeddings of the nearest neighbour lookup, biasing
the novelty signal towards what the agent can control. We employ the framework
of Universal Value Function Approximators (UVFA) to simultaneously learn many
directed exploration policies with the same neural network, with different
trade&#45;offs between exploration and exploitation. By using the same neural
network for different degrees of exploration/exploitation, transfer is
demonstrated from predominantly exploratory policies yielding effective
exploitative policies. The proposed method can be incorporated to run with
modern distributed RL agents that collect large amounts of experience from
many actors running in parallel on separate environment instances. Our method
doubles the performance of the base agent in all hard exploration in the
Atari&#45;57 suite while maintaining a very high score across the remaining games,
obtaining a median human normalised score of 1344.0%. Notably, the proposed
method is the first algorithm to achieve non&#45;zero rewards (with a mean score
of 8,400) in the game of Pitfall! without using demonstrations or hand&#45;crafted
features.

(2020)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1532,-1422C1532,-1422 1502,-1422 1502,-1422 1496,-1422 1490,-1416 1490,-1410 1490,-1410 1490,-1398 1490,-1398 1490,-1392 1496,-1386 1502,-1386 1502,-1386 1532,-1386 1532,-1386 1538,-1386 1544,-1392 1544,-1398 1544,-1398 1544,-1410 1544,-1410 1544,-1416 1538,-1422 1532,-1422"/>
<text text-anchor="middle" x="1517" y="-1400.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">NGU</text>
</a>
</g>
</g>
<!-- R2D2&#45;&gt;NGU -->
<g id="edge35" class="edge"><title>R2D2&#45;&gt;NGU</title>
<path fill="none" stroke="black" d="M1444.63,-1423.03C1455.45,-1420.15 1468.18,-1416.75 1479.87,-1413.64"/>
<polygon fill="black" stroke="black" points="1481.04,-1416.95 1489.8,-1410.99 1479.24,-1410.18 1481.04,-1416.95"/>
</g>
<!-- Duelling&#45;DQN&#45;&gt;RAINBOW -->
<g id="edge33" class="edge"><title>Duelling&#45;DQN&#45;&gt;RAINBOW</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M984.144,-1460.46C1018.49,-1463.75 1065.01,-1468.2 1099.45,-1471.5"/>
<polygon fill="darkgray" stroke="darkgray" points="1099.43,-1475.01 1109.72,-1472.48 1100.1,-1468.04 1099.43,-1475.01"/>
</g>
<!-- Agent57 -->
<g id="node36" class="node"><title>Agent57</title>
<g id="a_node36"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#Agent57" xlink:title="(from the abstract) Atari games have been a long&#45;standing benchmark in the
reinforcement learning (RL) community for the past decade. This benchmark was
proposed to test general competency of RL algorithms. Previous work has
achieved good average performance by doing outstandingly well on many games of
the set, but very poorly in several of the most challenging games. We propose
Agent57, the first deep RL agent that outperforms the standard human benchmark
on all 57 Atari games. To achieve this result, we train a neural network which
parameterizes a family of policies ranging from very exploratory to purely
exploitative. We propose an adaptive mechanism to choose which policy to
prioritize throughout the training process. Additionally, we utilize a novel
parameterization of the architecture that allows for more consistent and
stable learning.

(2020)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1537,-1494C1537,-1494 1497,-1494 1497,-1494 1491,-1494 1485,-1488 1485,-1482 1485,-1482 1485,-1470 1485,-1470 1485,-1464 1491,-1458 1497,-1458 1497,-1458 1537,-1458 1537,-1458 1543,-1458 1549,-1464 1549,-1470 1549,-1470 1549,-1482 1549,-1482 1549,-1488 1543,-1494 1537,-1494"/>
<text text-anchor="middle" x="1517" y="-1472.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">Agent57</text>
</a>
</g>
</g>
<!-- NGU&#45;&gt;Agent57 -->
<g id="edge36" class="edge"><title>NGU&#45;&gt;Agent57</title>
<path fill="none" stroke="black" d="M1517,-1422.28C1517,-1430.83 1517,-1439.37 1517,-1447.92"/>
<polygon fill="black" stroke="black" points="1513.5,-1447.95 1517,-1457.95 1520.5,-1447.95 1513.5,-1447.95"/>
</g>
<!-- DPG&#45;&gt;DDPG -->
<g id="edge46" class="edge"><title>DPG&#45;&gt;DDPG</title>
<path fill="none" stroke="black" d="M816.5,-955.978C816.5,-910.405 816.5,-864.833 816.5,-819.26"/>
<polygon fill="black" stroke="black" points="820,-819.228 816.5,-809.229 813,-819.229 820,-819.228"/>
</g>
<!-- TRPO&#45;&gt;ACER -->
<g id="edge53" class="edge"><title>TRPO&#45;&gt;ACER</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M839.946,-1086.12C853.387,-1096 871.262,-1107.5 889,-1114 964.294,-1141.61 1059.25,-1147.27 1110.21,-1148.15"/>
<polygon fill="darkgray" stroke="darkgray" points="1110.35,-1151.65 1120.39,-1148.27 1110.44,-1144.65 1110.35,-1151.65"/>
<text text-anchor="middle" x="936.5" y="-1141" font-family="sans-serif" font-size="10.00" fill="darkgray">TRPO technique</text>
</g>
<!-- TRPO&#45;&gt;GAE -->
<g id="edge52" class="edge"><title>TRPO&#45;&gt;GAE</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M816.5,-1086.28C816.5,-1094.83 816.5,-1103.37 816.5,-1111.92"/>
<polygon fill="darkgray" stroke="darkgray" points="813,-1111.95 816.5,-1121.95 820,-1111.95 813,-1111.95"/>
</g>
<!-- PPO -->
<g id="node47" class="node"><title>PPO</title>
<g id="a_node47"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#PPO" xlink:title="Proximal Policy Optimization (PPO) is similar to TRPO but uses simpler
mechanism while retaining similar performance.

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1162.5,-1040C1162.5,-1040 1132.5,-1040 1132.5,-1040 1126.5,-1040 1120.5,-1034 1120.5,-1028 1120.5,-1028 1120.5,-1016 1120.5,-1016 1120.5,-1010 1126.5,-1004 1132.5,-1004 1132.5,-1004 1162.5,-1004 1162.5,-1004 1168.5,-1004 1174.5,-1010 1174.5,-1016 1174.5,-1016 1174.5,-1028 1174.5,-1028 1174.5,-1034 1168.5,-1040 1162.5,-1040"/>
<text text-anchor="middle" x="1147.5" y="-1018.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">PPO</text>
</a>
</g>
</g>
<!-- TRPO&#45;&gt;PPO -->
<g id="edge54" class="edge"><title>TRPO&#45;&gt;PPO</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M843.591,-1064.35C902.019,-1056.18 1043.48,-1036.4 1110.34,-1027.06"/>
<polygon fill="darkgray" stroke="darkgray" points="1110.99,-1030.5 1120.41,-1025.65 1110.02,-1023.57 1110.99,-1030.5"/>
</g>
<!-- PPO&#45;&gt;SAC -->
<!-- A2C&#45;&gt;ACER -->
<!-- A2C&#45;&gt;ACKTR -->
<!-- A2C&#45;&gt;SVPG -->
<!-- A2C&#45;&gt;IMPALA -->
<!-- Dyna&#45;Q -->
<g id="node50" class="node"><title>Dyna&#45;Q</title>
<g id="a_node50"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#DynaQ" xlink:title="Dyna&#45;Q uses the experience drawn from real interaction with the environment to
improve the value function/policy (called direct RL, using Q&#45;learning) and the
model of the environment (called model learning). The model is then used to
create experiences (called planning) to improve the value function/policy.

(1990)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M533,-438C533,-438 498,-438 498,-438 492,-438 486,-432 486,-426 486,-426 486,-414 486,-414 486,-408 492,-402 498,-402 498,-402 533,-402 533,-402 539,-402 545,-408 545,-414 545,-414 545,-426 545,-426 545,-432 539,-438 533,-438"/>
<text text-anchor="middle" x="515.5" y="-416.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">Dyna&#45;Q</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;Dyna&#45;Q -->
<g id="edge62" class="edge"><title>Model Based&#45;&gt;Dyna&#45;Q</title>
<path fill="none" stroke="black" d="M270.023,-403.927C326.281,-407.626 422.195,-413.931 475.631,-417.445"/>
<polygon fill="black" stroke="black" points="475.52,-420.945 485.728,-418.108 475.979,-413.96 475.52,-420.945"/>
</g>
<!-- MCTS -->
<g id="node51" class="node"><title>MCTS</title>
<g id="a_node51"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#MCTS" xlink:title="Monte Carlo Tree Search (MCTS) selects the next action by performing rollout
algorithm, which estimates action values for a given policy by averaging the
returns of many simulated trajectories that start with each possible action
and then follow the given policy. Unlike Monte Carlo control, the goal of a
rollout algorithm is not to estimate a complete optimal action&#45;value function,
q&#45;star, or a complete action&#45;value function,q&#45;pi, for a given policy pi.
Instead, they produce Monte Carlo estimates of action values only for each
current state, and once an action is selected, this estimation will be
discarded and fresh calculation will be performed on the next state. MCTS
enchances this rollout algorithm by the addition of a means for accumulating
value estimates obtained from the Monte Carlo simulations in order to
successively direct simulations toward more highly&#45;rewarding trajectories.

(2006)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M667,-530C667,-530 637,-530 637,-530 631,-530 625,-524 625,-518 625,-518 625,-506 625,-506 625,-500 631,-494 637,-494 637,-494 667,-494 667,-494 673,-494 679,-500 679,-506 679,-506 679,-518 679,-518 679,-524 673,-530 667,-530"/>
<text text-anchor="middle" x="652" y="-508.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">MCTS</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;MCTS -->
<g id="edge63" class="edge"><title>Model Based&#45;&gt;MCTS</title>
<path fill="none" stroke="black" d="M249.873,-419.143C289.14,-447.116 369.926,-499.578 448,-519 504.98,-533.174 573.716,-525.949 614.81,-519.165"/>
<polygon fill="black" stroke="black" points="615.632,-522.575 624.887,-517.418 614.436,-515.678 615.632,-522.575"/>
</g>
<!-- PILCO -->
<g id="node52" class="node"><title>PILCO</title>
<g id="a_node52"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#PILCO" xlink:title="(from the abstract) In this paper, we introduce PILCO, a practical, data&#45;
efficient model&#45;based policy search method. PILCO reduces model bias, one of
the key problems of model&#45;based reinforcement learning, in a principled way.
By learning a probabilistic dynamics model and explicitly incorporating model
uncertainty into long&#45;term planning, PILCO can cope with very little data and
facilitates learning froms cratch in only a few trials. Policy evaluationis
performed in closed form using state&#45;of&#45;the&#45;art approximate inference.
Furthermore, policy gradients are computed analytically for policy
improvement. We report unprecedented learning efficiency on challenging and
high&#45;dimensional control tasks.

(2011)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M831.5,-550C831.5,-550 801.5,-550 801.5,-550 795.5,-550 789.5,-544 789.5,-538 789.5,-538 789.5,-526 789.5,-526 789.5,-520 795.5,-514 801.5,-514 801.5,-514 831.5,-514 831.5,-514 837.5,-514 843.5,-520 843.5,-526 843.5,-526 843.5,-538 843.5,-538 843.5,-544 837.5,-550 831.5,-550"/>
<text text-anchor="middle" x="816.5" y="-528.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">PILCO</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;PILCO -->
<g id="edge64" class="edge"><title>Model Based&#45;&gt;PILCO</title>
<path fill="none" stroke="black" d="M231.846,-419.148C246.952,-458.668 289.026,-549 358,-549 358,-549 358,-549 653,-549 696.641,-549 746.518,-542.86 779.374,-537.96"/>
<polygon fill="black" stroke="black" points="780.059,-541.396 789.413,-536.422 778.999,-534.477 780.059,-541.396"/>
</g>
<!-- I2A -->
<g id="node53" class="node"><title>I2A</title>
<g id="a_node53"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#I2A" xlink:title="(from the abstract) We introduce Imagination&#45;Augmented Agents (I2As), a novel
architecture for deep reinforcement learning combining model&#45;free and model&#45;
based aspects. In contrast to most existing model&#45;based reinforcement learning
and planning methods, which prescribe how a model should be used to arrive at
a policy, I2As learn to interpret predictions from a learned environment model
to construct implicit plans in arbitrary ways, by using the predictions as
additional context in deep policy networks. I2As show improved data
efficiency, performance, and robustness to model misspecification compared to
several baselines.

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1162.5,-468C1162.5,-468 1132.5,-468 1132.5,-468 1126.5,-468 1120.5,-462 1120.5,-456 1120.5,-456 1120.5,-444 1120.5,-444 1120.5,-438 1126.5,-432 1132.5,-432 1132.5,-432 1162.5,-432 1162.5,-432 1168.5,-432 1174.5,-438 1174.5,-444 1174.5,-444 1174.5,-456 1174.5,-456 1174.5,-462 1168.5,-468 1162.5,-468"/>
<text text-anchor="middle" x="1147.5" y="-446.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">I2A</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;I2A -->
<g id="edge65" class="edge"><title>Model Based&#45;&gt;I2A</title>
<path fill="none" stroke="black" d="M230.394,-419.299C243.268,-462.841 282.407,-569 358,-569 358,-569 358,-569 937.5,-569 956.831,-569 1057.57,-506.726 1111.81,-472.277"/>
<polygon fill="black" stroke="black" points="1113.86,-475.121 1120.42,-466.797 1110.1,-469.216 1113.86,-475.121"/>
</g>
<!-- MBMF -->
<g id="node54" class="node"><title>MBMF</title>
<g id="a_node54"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#MBMF" xlink:title="(from the abstract) Neural Network Dynamics for Model&#45;Based Deep Reinforcement
Learning with Model&#45;Free Fine&#45;Tuning. We demonstrate that medium&#45;sized neural
network models can in fact be combined with model predictive control (MPC) to
achieve excellent sample complexity in a model&#45;based reinforcement learning
algorithm, producing stable and plausible gaits to accomplish various complex
locomotion tasks. We also propose using deep neural network dynamics models to
initialize a model&#45;free learner, in order to combine the sample efficiency of
model&#45;based approaches with the high task&#45;specific performance of model&#45;free
methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure
model&#45;based approach trained on just random action data can follow arbitrary
trajectories with excellent sample efficiency, and that our hybrid algorithm
can accelerate model&#45;free learning on high&#45;speed benchmark tasks, achieving
sample efficiency gains of 3&#45;5x on swimmer, cheetah, hopper, and ant agents.

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1162.5,-522C1162.5,-522 1132.5,-522 1132.5,-522 1126.5,-522 1120.5,-516 1120.5,-510 1120.5,-510 1120.5,-498 1120.5,-498 1120.5,-492 1126.5,-486 1132.5,-486 1132.5,-486 1162.5,-486 1162.5,-486 1168.5,-486 1174.5,-492 1174.5,-498 1174.5,-498 1174.5,-510 1174.5,-510 1174.5,-516 1168.5,-522 1162.5,-522"/>
<text text-anchor="middle" x="1147.5" y="-500.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">MBMF</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;MBMF -->
<g id="edge66" class="edge"><title>Model Based&#45;&gt;MBMF</title>
<path fill="none" stroke="black" d="M229.12,-419.026C239.726,-466.081 275.344,-589 358,-589 358,-589 358,-589 937.5,-589 972.643,-589 1061.2,-547.24 1111.03,-522.262"/>
<polygon fill="black" stroke="black" points="1112.8,-525.286 1120.16,-517.657 1109.65,-519.036 1112.8,-525.286"/>
</g>
<!-- Exit -->
<g id="node55" class="node"><title>Exit</title>
<g id="a_node55"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#Exit" xlink:title="Expert Iteration (ExIt) is a novel reinforcement learning algorithm which
decomposes the problem into separate planning and generalisation tasks.
Planning new policies is performed by tree search, while a deep neural network
generalises those plans. Subsequently, tree search is improved by using the
neural network policy to guide search, increasing the strength of new plans.
In contrast, standard deep Reinforcement Learning algorithms rely on a neural
network not only to generalise plans, but to discover them too. We show that
ExIt outperforms REINFORCE for training a neural network to play the board
game Hex, and our final tree search agent, trained tabula rasa, defeats MoHex
1.0, the most recent Olympiad Champion player to be publicly released. (from
the abstract)

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1162.5,-576C1162.5,-576 1132.5,-576 1132.5,-576 1126.5,-576 1120.5,-570 1120.5,-564 1120.5,-564 1120.5,-552 1120.5,-552 1120.5,-546 1126.5,-540 1132.5,-540 1132.5,-540 1162.5,-540 1162.5,-540 1168.5,-540 1174.5,-546 1174.5,-552 1174.5,-552 1174.5,-564 1174.5,-564 1174.5,-570 1168.5,-576 1162.5,-576"/>
<text text-anchor="middle" x="1147.5" y="-554.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">Exit</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;Exit -->
<g id="edge67" class="edge"><title>Model Based&#45;&gt;Exit</title>
<path fill="none" stroke="black" d="M228.148,-419.095C236.677,-469.7 268.301,-609 358,-609 358,-609 358,-609 937.5,-609 999.78,-609 1069.6,-587.362 1110.71,-572.274"/>
<polygon fill="black" stroke="black" points="1112.14,-575.477 1120.28,-568.691 1109.69,-568.922 1112.14,-575.477"/>
</g>
<!-- AlphaZero -->
<g id="node56" class="node"><title>AlphaZero</title>
<g id="a_node56"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#AlphaZero" xlink:title="AlphaZero generalises tabula rasa reinforcement learning from games of self&#45;
play approach. Starting from random play, and given no domain knowledge except
the game rules, AlphaZero achieved within 24 hours a superhuman level of play
in the games of chess and shogi (Japanese chess) as well as Go, and
convincingly defeated a world&#45;champion program in each case. (from the
abstract)

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1173,-630C1173,-630 1122,-630 1122,-630 1116,-630 1110,-624 1110,-618 1110,-618 1110,-606 1110,-606 1110,-600 1116,-594 1122,-594 1122,-594 1173,-594 1173,-594 1179,-594 1185,-600 1185,-606 1185,-606 1185,-618 1185,-618 1185,-624 1179,-630 1173,-630"/>
<text text-anchor="middle" x="1147.5" y="-608.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">AlphaZero</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;AlphaZero -->
<g id="edge68" class="edge"><title>Model Based&#45;&gt;AlphaZero</title>
<path fill="none" stroke="black" d="M227.342,-419.273C233.898,-473.376 261.171,-629 358,-629 358,-629 358,-629 937.5,-629 993.497,-629 1057.66,-622.863 1099.95,-617.963"/>
<polygon fill="black" stroke="black" points="1100.43,-621.431 1109.95,-616.781 1099.61,-614.479 1100.43,-621.431"/>
</g>
<!-- MVE -->
<g id="node57" class="node"><title>MVE</title>
<g id="a_node57"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#MVE" xlink:title="(from the abstract) Recent model&#45;free reinforcement learning algorithms have
proposed incorporating learned dynamics models as a source of additional data
with the intention of reducing sample complexity. Such methods hold the
promise of incorporating imagined data coupled with a notion of model
uncertainty to accelerate the learning of continuous control tasks.
Unfortunately, they rely on heuristics that limit usage of the dynamics model.
We present model&#45;based value expansion, which controls for uncertainty in the
model by only allowing imagination to fixed depth. By enabling wider use of
learned dynamics models within a model&#45;free reinforcement learning algorithm,
we improve value estimation, which, in turn, reduces the sample complexity of
learning.

(2018)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1317.5,-650C1317.5,-650 1287.5,-650 1287.5,-650 1281.5,-650 1275.5,-644 1275.5,-638 1275.5,-638 1275.5,-626 1275.5,-626 1275.5,-620 1281.5,-614 1287.5,-614 1287.5,-614 1317.5,-614 1317.5,-614 1323.5,-614 1329.5,-620 1329.5,-626 1329.5,-626 1329.5,-638 1329.5,-638 1329.5,-644 1323.5,-650 1317.5,-650"/>
<text text-anchor="middle" x="1302.5" y="-628.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">MVE</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;MVE -->
<g id="edge69" class="edge"><title>Model Based&#45;&gt;MVE</title>
<path fill="none" stroke="black" d="M226.626,-419.236C231.238,-476.509 253.779,-649 358,-649 358,-649 358,-649 1148.5,-649 1188.72,-649 1234.52,-643.125 1265.45,-638.283"/>
<polygon fill="black" stroke="black" points="1266.02,-641.736 1275.34,-636.69 1264.91,-634.825 1266.02,-641.736"/>
</g>
<!-- STEVE -->
<g id="node58" class="node"><title>STEVE</title>
<g id="a_node58"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#STEVE" xlink:title="(from the abstract) Integrating model&#45;free and model&#45;based approaches in
reinforcement learning has the potential to achieve the high performance of
model&#45;free algorithms with low sample complexity. However, this is difficult
because an imperfect dynamics model can degrade the performance of the
learning algorithm, and in sufficiently complex environments, the dynamics
model will almost always be imperfect. As a result, a key challenge is to
combine model&#45;based approaches with model&#45;free learning in such a way that
errors in the model do not degrade performance. We propose stochastic ensemble
value expansion (STEVE), a novel model&#45;based technique that addresses this
issue. By dynamically interpolating between model rollouts of various horizon
lengths for each individual example, STEVE ensures that the model is only
utilized when doing so does not introduce significant errors. Our approach
outperforms model&#45;free baselines on challenging continuous control benchmarks
with an order&#45;of&#45;magnitude increase in sample efficiency, and in contrast to
previous model&#45;based approaches, performance does not degrade in complex
environments.

(2018)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1319,-278C1319,-278 1286,-278 1286,-278 1280,-278 1274,-272 1274,-266 1274,-266 1274,-254 1274,-254 1274,-248 1280,-242 1286,-242 1286,-242 1319,-242 1319,-242 1325,-242 1331,-248 1331,-254 1331,-254 1331,-266 1331,-266 1331,-272 1325,-278 1319,-278"/>
<text text-anchor="middle" x="1302.5" y="-256.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">STEVE</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;STEVE -->
<g id="edge70" class="edge"><title>Model Based&#45;&gt;STEVE</title>
<path fill="none" stroke="black" d="M229.836,-382.718C238.005,-352.342 258.603,-291.6 299,-259 320.143,-241.937 330.831,-243 358,-243 358,-243 358,-243 1148.5,-243 1188.12,-243 1233.16,-248.7 1264.05,-253.5"/>
<polygon fill="black" stroke="black" points="1263.54,-256.961 1273.96,-255.083 1264.64,-250.049 1263.54,-256.961"/>
</g>
<!-- ME&#45;TRPO -->
<g id="node59" class="node"><title>ME&#45;TRPO</title>
<g id="a_node59"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#METRPO" xlink:title="(from the abstract) Model&#45;free reinforcement learning (RL) methods are
succeeding in a growing number of tasks, aided by recent advances in deep
learning. However, they tend to suffer from high sample complexity, which
hinders their use in real&#45;world domains. Alternatively, model&#45;based
reinforcement learning promises to reduce sample complexity, but tends to
require careful tuning and to date have succeeded mainly in restrictive
domains where simple models are sufficient for learning. In this paper, we
analyze the behavior of vanilla model&#45;based reinforcement learning methods
when deep neural networks are used to learn both the model and the policy, and
show that the learned policy tends to exploit regions where insufficient data
is available for the model to be learned, causing instability in training. To
overcome this issue, we propose to use an ensemble of models to maintain the
model uncertainty and regularize the learning process. We further show that
the use of likelihood ratio derivatives yields much more stable learning than
backpropagation through time. Altogether, our approach Model&#45;Ensemble Trust&#45;
Region Policy Optimization (ME&#45;TRPO) significantly reduces the sample
complexity compared to model&#45;free deep RL methods on challenging continuous
control benchmark tasks.

(2018)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1326.5,-332C1326.5,-332 1278.5,-332 1278.5,-332 1272.5,-332 1266.5,-326 1266.5,-320 1266.5,-320 1266.5,-308 1266.5,-308 1266.5,-302 1272.5,-296 1278.5,-296 1278.5,-296 1326.5,-296 1326.5,-296 1332.5,-296 1338.5,-302 1338.5,-308 1338.5,-308 1338.5,-320 1338.5,-320 1338.5,-326 1332.5,-332 1326.5,-332"/>
<text text-anchor="middle" x="1302.5" y="-310.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">ME&#45;TRPO</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;ME&#45;TRPO -->
<g id="edge71" class="edge"><title>Model Based&#45;&gt;ME&#45;TRPO</title>
<path fill="none" stroke="black" d="M231.532,-382.666C241.329,-355.789 263.281,-305.968 299,-279 320.683,-262.629 330.831,-263 358,-263 358,-263 358,-263 1148.5,-263 1188.47,-263 1199.29,-265.745 1237,-279 1242.3,-280.863 1251.6,-285.594 1261.52,-290.997"/>
<polygon fill="black" stroke="black" points="1259.86,-294.081 1270.3,-295.863 1263.25,-287.957 1259.86,-294.081"/>
</g>
<!-- MB&#45;MPO -->
<g id="node60" class="node"><title>MB&#45;MPO</title>
<g id="a_node60"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#MBMPO" xlink:title="(from the abstract) Model&#45;based reinforcement learning approaches carry the
promise of being data efficient. However, due to challenges in learning
dynamics models that sufficiently match the real&#45;world dynamics, they struggle
to achieve the same asymptotic performance as model&#45;free methods. We propose
Model&#45;Based Meta&#45;Policy&#45;Optimization (MB&#45;MPO), an approach that foregoes the
strong reliance on accurate learned dynamics models. Using an ensemble of
learned dynamic models, MB&#45;MPO meta&#45;learns a policy that can quickly adapt to
any model in the ensemble with one policy gradient step. This steers the meta&#45;
policy towards internalizing consistent dynamics predictions among the
ensemble while shifting the burden of behaving optimally w.r.t. the model
discrepancies towards the adaptation step. Our experiments show that MB&#45;MPO is
more robust to model imperfections than previous model&#45;based approaches.
Finally, we demonstrate that our approach is able to match the asymptotic
performance of model&#45;free methods while requiring significantly less
experience.

(2018)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1323.5,-386C1323.5,-386 1281.5,-386 1281.5,-386 1275.5,-386 1269.5,-380 1269.5,-374 1269.5,-374 1269.5,-362 1269.5,-362 1269.5,-356 1275.5,-350 1281.5,-350 1281.5,-350 1323.5,-350 1323.5,-350 1329.5,-350 1335.5,-356 1335.5,-362 1335.5,-362 1335.5,-374 1335.5,-374 1335.5,-380 1329.5,-386 1323.5,-386"/>
<text text-anchor="middle" x="1302.5" y="-364.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">MB&#45;MPO</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;MB&#45;MPO -->
<g id="edge72" class="edge"><title>Model Based&#45;&gt;MB&#45;MPO</title>
<path fill="none" stroke="black" d="M233.654,-382.923C244.984,-359.888 267.685,-320.489 299,-299 321.402,-283.627 330.831,-283 358,-283 358,-283 358,-283 1148.5,-283 1188.39,-283 1205.76,-273.191 1237,-298 1253.22,-310.884 1241.33,-325.432 1255,-341 1256.91,-343.17 1259.03,-345.21 1261.29,-347.12"/>
<polygon fill="black" stroke="black" points="1259.27,-349.981 1269.4,-353.109 1263.43,-344.351 1259.27,-349.981"/>
</g>
<!-- World Models -->
<g id="node61" class="node"><title>World Models</title>
<g id="a_node61"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#WorldModels" xlink:title="(from the abstract) A generative recurrent neural network is quickly trained
in an unsupervised manner to model popular reinforcement learning environments
through compressed spatio&#45;temporal representations. The world model&#39;s
extracted features are fed into compact and simple policies trained by
evolution, achieving state of the art results in various environments. We also
train our agent entirely inside of an environment generated by its own
internal world model, and transfer this policy back into the actual
environment.

(2018)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1338,-440C1338,-440 1267,-440 1267,-440 1261,-440 1255,-434 1255,-428 1255,-428 1255,-416 1255,-416 1255,-410 1261,-404 1267,-404 1267,-404 1338,-404 1338,-404 1344,-404 1350,-410 1350,-416 1350,-416 1350,-428 1350,-428 1350,-434 1344,-440 1338,-440"/>
<text text-anchor="middle" x="1302.5" y="-418.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">World Models</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;World Models -->
<g id="edge73" class="edge"><title>Model Based&#45;&gt;World Models</title>
<path fill="none" stroke="black" d="M234.253,-382.714C244.429,-363.506 263.066,-333.92 288,-318 314.817,-300.878 326.183,-303 358,-303 358,-303 358,-303 1148.5,-303 1188.39,-303 1207.93,-290.675 1237,-318 1262.61,-342.072 1234.09,-366.752 1255,-395 1255.46,-395.619 1255.94,-396.226 1256.43,-396.822"/>
<polygon fill="black" stroke="black" points="1254.08,-399.42 1263.68,-403.911 1258.98,-394.416 1254.08,-399.42"/>
</g>
<!-- PETS -->
<g id="node62" class="node"><title>PETS</title>
<g id="a_node62"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#PETS" xlink:title="(from the abstract) Model&#45;based reinforcement learning (RL) algorithms can
attain excellent sample efficiency, but often lag behind the best model&#45;free
algorithms in terms of asymptotic performance. This is especially true with
high&#45;capacity parametric function approximators, such as deep networks. In
this paper, we study how to bridge this gap, by employing uncertainty&#45;aware
dynamics models. We propose a new algorithm called probabilistic ensembles
with trajectory sampling (PETS) that combines uncertainty&#45;aware deep network
dynamics models with sampling&#45;based uncertainty propagation. Our comparison to
state&#45;of&#45;the&#45;art model&#45;based and model&#45;free deep RL algorithms shows that our
approach matches the asymptotic performance of model&#45;free algorithms on
several challenging benchmark tasks, while requiring significantly fewer
samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and
Proximal Policy Optimization respectively on the half&#45;cheetah task).

(2018)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1317.5,-494C1317.5,-494 1287.5,-494 1287.5,-494 1281.5,-494 1275.5,-488 1275.5,-482 1275.5,-482 1275.5,-470 1275.5,-470 1275.5,-464 1281.5,-458 1287.5,-458 1287.5,-458 1317.5,-458 1317.5,-458 1323.5,-458 1329.5,-464 1329.5,-470 1329.5,-470 1329.5,-482 1329.5,-482 1329.5,-488 1323.5,-494 1317.5,-494"/>
<text text-anchor="middle" x="1302.5" y="-472.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">PETS</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;PETS -->
<g id="edge74" class="edge"><title>Model Based&#45;&gt;PETS</title>
<path fill="none" stroke="black" d="M238.572,-382.953C249.989,-368.546 267.857,-349.01 288,-338 315.919,-322.739 326.183,-323 358,-323 358,-323 358,-323 1148.5,-323 1188.39,-323 1208.89,-309.69 1237,-338 1272.21,-373.465 1226.67,-407.829 1255,-449 1258.03,-453.405 1262.03,-457.19 1266.42,-460.413"/>
<polygon fill="black" stroke="black" points="1264.85,-463.562 1275.18,-465.932 1268.59,-457.64 1264.85,-463.562"/>
</g>
<!-- PlaNet -->
<g id="node63" class="node"><title>PlaNet</title>
<g id="a_node63"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#PlaNet" xlink:title="(from the abstract) We propose the Deep Planning Network (PlaNet), a purely
model&#45;based agent that learns the environment dynamics from images and chooses
actions through fast online planning in latent space. To achieve high
performance, the dynamics model must accurately predict the rewards ahead for
multiple time steps. We approach this using a latent dynamics model with both
deterministic and stochastic transition components. Moreover, we propose a
multi&#45;step variational inference objective that we name latent overshooting.
Using only pixel observations, our agent solves continuous control tasks with
contact dynamics, partial observability, and sparse rewards, which exceed the
difficulty of tasks that were previously solved by planning with learned
models. PlaNet uses substantially fewer episodes and reaches final performance
close to and sometimes higher than strong model&#45;free algorithms.

(2018)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1317.5,-548C1317.5,-548 1287.5,-548 1287.5,-548 1281.5,-548 1275.5,-542 1275.5,-536 1275.5,-536 1275.5,-524 1275.5,-524 1275.5,-518 1281.5,-512 1287.5,-512 1287.5,-512 1317.5,-512 1317.5,-512 1323.5,-512 1329.5,-518 1329.5,-524 1329.5,-524 1329.5,-536 1329.5,-536 1329.5,-542 1323.5,-548 1317.5,-548"/>
<text text-anchor="middle" x="1302.5" y="-526.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">PlaNet</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;PlaNet -->
<g id="edge75" class="edge"><title>Model Based&#45;&gt;PlaNet</title>
<path fill="none" stroke="black" d="M247.101,-382.999C258.601,-374.258 273.364,-364.307 288,-358 317.22,-345.409 326.183,-343 358,-343 358,-343 358,-343 1148.5,-343 1163.6,-343 1236.37,-432.827 1237,-434 1251.92,-461.959 1235.82,-477.769 1255,-503 1258.17,-507.166 1262.18,-510.803 1266.51,-513.942"/>
<polygon fill="black" stroke="black" points="1264.78,-516.987 1275.1,-519.373 1268.52,-511.07 1264.78,-516.987"/>
</g>
<!-- SimPLe -->
<g id="node64" class="node"><title>SimPLe</title>
<g id="a_node64"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#SimPLe" xlink:title="Simulated Policy Learning (SimPLe) is a complete model&#45;based deep RL algorithm
based on video prediction models and present a comparison of several model
architectures, including a novel architecture that yields the best results in
our setting. Our experiments evaluate SimPLe on a range of Atari games in low
data regime of 100k interactions between the agent and the environment, which
corresponds to two hours of real&#45;time play. In most games SimPLe outperforms
state&#45;of&#45;the&#45;art model&#45;free algorithms, in some games by over an order of
magnitude. (from the abstract)

(2019)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1436,-568C1436,-568 1399,-568 1399,-568 1393,-568 1387,-562 1387,-556 1387,-556 1387,-544 1387,-544 1387,-538 1393,-532 1399,-532 1399,-532 1436,-532 1436,-532 1442,-532 1448,-538 1448,-544 1448,-544 1448,-556 1448,-556 1448,-562 1442,-568 1436,-568"/>
<text text-anchor="middle" x="1417.5" y="-546.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">SimPLe</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;SimPLe -->
<g id="edge76" class="edge"><title>Model Based&#45;&gt;SimPLe</title>
<path fill="none" stroke="black" d="M266.412,-382.89C292.171,-373.103 326.411,-363 358,-363 358,-363 358,-363 653,-363 772.733,-363 1089.76,-341.522 1190,-407 1208.4,-419.017 1228.71,-473.649 1237,-494 1247.99,-520.969 1231.75,-539.464 1255,-557 1289.82,-583.262 1341.87,-575.063 1377.41,-564.562"/>
<polygon fill="black" stroke="black" points="1378.47,-567.897 1386.96,-561.566 1376.37,-561.218 1378.47,-567.897"/>
</g>
<!-- MuZero -->
<g id="node65" class="node"><title>MuZero</title>
<g id="a_node65"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#MuZero" xlink:title="(from the abstract) Constructing agents with planning capabilities has long
been one of the main challenges in the pursuit of artificial intelligence.
Tree&#45;based planning methods have enjoyed huge success in challenging domains,
such as chess and Go, where a perfect simulator is available. However, in
real&#45;world problems the dynamics governing the environment are often complex
and unknown. In this work we present the MuZero algorithm which, by combining
a tree&#45;based search with a learned model, achieves superhuman performance in a
range of challenging and visually complex domains, without any knowledge of
their underlying dynamics. MuZero learns a model that, when applied
iteratively, predicts the quantities most directly relevant to planning: the
reward, the action&#45;selection policy, and the value function. When evaluated on
57 different Atari games &#45; the canonical video game environment for testing AI
techniques, in which model&#45;based planning approaches have historically
struggled &#45; our new algorithm achieved a new state of the art. When evaluated
on Go, chess and shogi, without any knowledge of the game rules, MuZero
matched the superhuman performance of the AlphaZero algorithm that was
supplied with the game rules.

(2019)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1435,-622C1435,-622 1400,-622 1400,-622 1394,-622 1388,-616 1388,-610 1388,-610 1388,-598 1388,-598 1388,-592 1394,-586 1400,-586 1400,-586 1435,-586 1435,-586 1441,-586 1447,-592 1447,-598 1447,-598 1447,-610 1447,-610 1447,-616 1441,-622 1435,-622"/>
<text text-anchor="middle" x="1417.5" y="-600.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">MuZero</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;MuZero -->
<g id="edge77" class="edge"><title>Model Based&#45;&gt;MuZero</title>
<path fill="none" stroke="black" d="M270.067,-391.895C295.677,-387.43 328.517,-383 358,-383 358,-383 358,-383 602,-383 667.484,-383 1136.9,-384.675 1190,-423 1207.13,-435.366 1199.65,-447.592 1208,-467 1228.05,-513.592 1215.37,-539.345 1255,-571 1289.9,-598.877 1342.28,-605.028 1377.85,-605.582"/>
<polygon fill="black" stroke="black" points="1377.86,-609.082 1387.86,-605.599 1377.87,-602.082 1377.86,-609.082"/>
</g>
<!-- Prioritized Sweeping -->
<g id="node66" class="node"><title>Prioritized Sweeping</title>
<g id="a_node66"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#PrioritizedSweeping" xlink:title="Prioritized Sweeping/Queue&#45;Dyna is similar to Dyna, and it improves Dyna by
updating value based on priority rather than randomly. Values are also
associated with state rather than state&#45;action.

(1993)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M571,-510C571,-510 460,-510 460,-510 454,-510 448,-504 448,-498 448,-498 448,-486 448,-486 448,-480 454,-474 460,-474 460,-474 571,-474 571,-474 577,-474 583,-480 583,-486 583,-486 583,-498 583,-498 583,-504 577,-510 571,-510"/>
<text text-anchor="middle" x="515.5" y="-488.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">Prioritized Sweeping</text>
</a>
</g>
</g>
<!-- Dyna&#45;Q&#45;&gt;Prioritized Sweeping -->
<g id="edge78" class="edge"><title>Dyna&#45;Q&#45;&gt;Prioritized Sweeping</title>
<path fill="none" stroke="black" d="M515.5,-438.281C515.5,-446.828 515.5,-455.374 515.5,-463.921"/>
<polygon fill="black" stroke="black" points="512,-463.954 515.5,-473.954 519,-463.954 512,-463.954"/>
</g>
<!-- DMRL -->
<g id="node68" class="node"><title>DMRL</title>
<g id="a_node68"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#DMRL" xlink:title="Deep Meta RL. (from the abstract) In recent years deep reinforcement learning
(RL) systems have attained superhuman performance in a number of challenging
task domains. However, a major limitation of such applications is their demand
for massive amounts of training data. A critical present objective is thus to
develop deep RL methods that can adapt rapidly to new tasks. In the present
work we introduce a novel approach to this challenge, which we refer to as
deep meta&#45;reinforcement learning. Previous work has shown that recurrent
networks can support meta&#45;learning in a fully supervised context. We extend
this approach to the RL setting. What emerges is a system that is trained
using one RL algorithm, but whose recurrent dynamics implement a second, quite
separate RL procedure. This second, learned RL algorithm can differ from the
original one in arbitrary ways. Importantly, because it is learned, it is
configured to exploit structure in the training domain. We unpack these points
in a series of seven proof&#45;of&#45;concept experiments, each of which examines a
key aspect of deep meta&#45;RL. We consider prospects for extending and scaling up
the approach, and also point out some potentially important implications for
neuroscience.

(2016)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M951.5,-118C951.5,-118 921.5,-118 921.5,-118 915.5,-118 909.5,-112 909.5,-106 909.5,-106 909.5,-94 909.5,-94 909.5,-88 915.5,-82 921.5,-82 921.5,-82 951.5,-82 951.5,-82 957.5,-82 963.5,-88 963.5,-94 963.5,-94 963.5,-106 963.5,-106 963.5,-112 957.5,-118 951.5,-118"/>
<text text-anchor="middle" x="936.5" y="-96.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">DMRL</text>
</a>
</g>
</g>
<!-- Meta&#45;RL&#45;&gt;DMRL -->
<g id="edge79" class="edge"><title>Meta&#45;RL&#45;&gt;DMRL</title>
<path fill="none" stroke="black" d="M684.041,-116.912C736.69,-113.371 843.224,-106.206 899.225,-102.44"/>
<polygon fill="black" stroke="black" points="899.653,-105.919 909.395,-101.756 899.183,-98.9347 899.653,-105.919"/>
</g>
<!-- RL^2 -->
<g id="node69" class="node"><title>RL^2</title>
<g id="a_node69"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#RL2" xlink:title="(from the abstract) Deep reinforcement learning (deep RL) has been successful
in learning sophisticated behaviors automatically; however, the learning
process requires a huge number of trials. In contrast, animals can learn new
tasks in just a few trials, benefiting from their prior knowledge about the
world. This paper seeks to bridge this gap. Rather than designing a &quot;fast&quot;
reinforcement learning algorithm, we propose to represent it as a recurrent
neural network (RNN) and learn it from data. In our proposed method, RL2, the
algorithm is encoded in the weights of the RNN, which are learned slowly
through a general&#45;purpose (&quot;slow&quot;) RL algorithm. The RNN receives all
information a typical RL algorithm would receive, including observations,
actions, rewards, and termination flags; and it retains its state across
episodes in a given Markov Decision Process (MDP). The activations of the RNN
store the state of the &quot;fast&quot; RL algorithm on the current (previously unseen)
MDP. We evaluate RL2 experimentally on both small&#45;scale and large&#45;scale
problems. On the small&#45;scale side, we train it to solve randomly generated
multi&#45;arm bandit problems and finite MDPs. After RL2 is trained, its
performance on new MDPs is close to human&#45;designed algorithms with optimality
guarantees. On the large&#45;scale side, we test RL2 on a vision&#45;based navigation
task and show that it scales up to high&#45;dimensional problems.

(2016)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M951.5,-172C951.5,-172 921.5,-172 921.5,-172 915.5,-172 909.5,-166 909.5,-160 909.5,-160 909.5,-148 909.5,-148 909.5,-142 915.5,-136 921.5,-136 921.5,-136 951.5,-136 951.5,-136 957.5,-136 963.5,-142 963.5,-148 963.5,-148 963.5,-160 963.5,-160 963.5,-166 957.5,-172 951.5,-172"/>
<text text-anchor="middle" x="936.5" y="-150.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">RL^2</text>
</a>
</g>
</g>
<!-- Meta&#45;RL&#45;&gt;RL^2 -->
<g id="edge80" class="edge"><title>Meta&#45;RL&#45;&gt;RL^2</title>
<path fill="none" stroke="black" d="M684.041,-122.846C736.69,-129.369 843.224,-142.568 899.225,-149.506"/>
<polygon fill="black" stroke="black" points="899.041,-153.01 909.395,-150.766 899.902,-146.063 899.041,-153.01"/>
</g>
<!-- MAML -->
<g id="node70" class="node"><title>MAML</title>
<g id="a_node70"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#MAML" xlink:title="(from the abstract) We propose an algorithm for meta&#45;learning that is model&#45;
agnostic, in the sense that it is compatible with any model trained with
gradient descent and applicable to a variety of different learning problems,
including classification, regression, and reinforcement learning. The goal of
meta&#45;learning is to train a model on a variety of learning tasks, such that it
can solve new learning tasks using only a small number of training samples. In
our approach, the parameters of the model are explicitly trained such that a
small number of gradient steps with a small amount of training data from a new
task will produce good generalization performance on that task. In effect, our
method trains the model to be easy to fine&#45;tune. We demonstrate that this
approach leads to state&#45;of&#45;the&#45;art performance on two few&#45;shot image
classification benchmarks, produces good results on few&#45;shot regression, and
accelerates fine&#45;tuning for policy gradient reinforcement learning with neural
network policies.

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1162.5,-192C1162.5,-192 1132.5,-192 1132.5,-192 1126.5,-192 1120.5,-186 1120.5,-180 1120.5,-180 1120.5,-168 1120.5,-168 1120.5,-162 1126.5,-156 1132.5,-156 1132.5,-156 1162.5,-156 1162.5,-156 1168.5,-156 1174.5,-162 1174.5,-168 1174.5,-168 1174.5,-180 1174.5,-180 1174.5,-186 1168.5,-192 1162.5,-192"/>
<text text-anchor="middle" x="1147.5" y="-170.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">MAML</text>
</a>
</g>
</g>
<!-- Meta&#45;RL&#45;&gt;MAML -->
<g id="edge81" class="edge"><title>Meta&#45;RL&#45;&gt;MAML</title>
<path fill="none" stroke="black" d="M684.036,-128.761C689.983,-130.557 696.172,-132.377 702,-134 784.553,-156.994 803.953,-170.476 889,-181 930.903,-186.185 941.788,-181.909 984,-181 1027.43,-180.065 1077.35,-177.726 1110.27,-176.008"/>
<polygon fill="black" stroke="black" points="1110.54,-179.499 1120.34,-175.474 1110.17,-172.509 1110.54,-179.499"/>
</g>
<!-- SNAIL -->
<g id="node71" class="node"><title>SNAIL</title>
<g id="a_node71"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#SNAIL" xlink:title="(from the abstract) Deep neural networks excel in regimes with large amounts
of data, but tend to struggle when data is scarce or when they need to adapt
quickly to changes in the task. In response, recent work in meta&#45;learning
proposes training a meta&#45;learner on a distribution of similar tasks, in the
hopes of generalization to novel but related tasks by learning a high&#45;level
strategy that captures the essence of the problem it is asked to solve.
However, many recent meta&#45;learning approaches are extensively hand&#45;designed,
either using architectures specialized to a particular application, or hard&#45;
coding algorithmic components that constrain how the meta&#45;learner solves the
task. We propose a class of simple and generic meta&#45;learner architectures that
use a novel combination of temporal convolutions and soft attention; the
former to aggregate information from past experience and the latter to
pinpoint specific pieces of information. In the most extensive set of meta&#45;
learning experiments to date, we evaluate the resulting Simple Neural
AttentIve Learner (or SNAIL) on several heavily&#45;benchmarked tasks. On all
tasks, in both supervised and reinforcement learning, SNAIL attains state&#45;of&#45;
the&#45;art performance by significant margins.

(2017)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1162.5,-78C1162.5,-78 1132.5,-78 1132.5,-78 1126.5,-78 1120.5,-72 1120.5,-66 1120.5,-66 1120.5,-54 1120.5,-54 1120.5,-48 1126.5,-42 1132.5,-42 1132.5,-42 1162.5,-42 1162.5,-42 1168.5,-42 1174.5,-48 1174.5,-54 1174.5,-54 1174.5,-66 1174.5,-66 1174.5,-72 1168.5,-78 1162.5,-78"/>
<text text-anchor="middle" x="1147.5" y="-56.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">SNAIL</text>
</a>
</g>
</g>
<!-- Meta&#45;RL&#45;&gt;SNAIL -->
<g id="edge82" class="edge"><title>Meta&#45;RL&#45;&gt;SNAIL</title>
<path fill="none" stroke="black" d="M661.795,-100.816C669.962,-86.237 683.626,-66.8083 702,-58 773.612,-23.6694 1017.39,-45.8335 1110.21,-55.8421"/>
<polygon fill="black" stroke="black" points="1109.93,-59.3322 1120.25,-56.9412 1110.69,-52.3737 1109.93,-59.3322"/>
</g>
<!-- ProMP -->
<g id="node72" class="node"><title>ProMP</title>
<g id="a_node72"><a xlink:href="https://github.com/bennylp/RL-Taxonomy#ProMP" xlink:title="ProMP: Proximal Meta&#45;Policy Search (from the abstract) Credit assignment in
Meta&#45;reinforcement learning (Meta&#45;RL) is still poorly understood. Existing
methods either neglect credit assignment to pre&#45;adaptation behavior or
implement it naively. This leads to poor sample&#45;efficiency during meta&#45;
training as well as ineffective task identification strategies. This paper
provides a theoretical analysis of credit assignment in gradient&#45;based Meta&#45;
RL. Building on the gained insights we develop a novel meta&#45;learning algorithm
that overcomes both the issue of poor credit assignment and previous
difficulties in estimating meta&#45;policy gradients. By controlling the
statistical distance of both pre&#45;adaptation and adapted policies during meta&#45;
policy search, the proposed algorithm endows efficient and stable meta&#45;
learning. Our approach leads to superior pre&#45;adaptation policy behavior and
consistently outperforms previous Meta&#45;RL algorithms in sample&#45;efficiency,
wall&#45;clock time, and asymptotic performance.

(2018)">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1318,-115C1318,-115 1287,-115 1287,-115 1281,-115 1275,-109 1275,-103 1275,-103 1275,-91 1275,-91 1275,-85 1281,-79 1287,-79 1287,-79 1318,-79 1318,-79 1324,-79 1330,-85 1330,-91 1330,-91 1330,-103 1330,-103 1330,-109 1324,-115 1318,-115"/>
<text text-anchor="middle" x="1302.5" y="-93.9" font-family="Helvetica,sans-Serif" font-weight="bold" font-size="12.00">ProMP</text>
</a>
</g>
</g>
<!-- Meta&#45;RL&#45;&gt;ProMP -->
<g id="edge83" class="edge"><title>Meta&#45;RL&#45;&gt;ProMP</title>
<path fill="none" stroke="black" d="M684.07,-111.849C690.014,-110.531 696.193,-109.195 702,-108 825.704,-82.5469 857.914,-65.73 984,-73 1038.05,-76.1163 1051.05,-82.5108 1105,-87 1160.5,-91.6185 1225.02,-94.4104 1264.55,-95.829"/>
<polygon fill="black" stroke="black" points="1264.65,-99.3345 1274.77,-96.1849 1264.89,-92.3388 1264.65,-99.3345"/>
</g>
</g>
</svg>
