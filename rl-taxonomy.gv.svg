<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN"
 "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<!-- Generated by graphviz version 2.38.0 (20140413.2041)
 -->
<!-- Title: %3 Pages: 1 -->
<svg width="2045pt" height="1283pt"
 viewBox="0.00 0.00 2045.00 1283.00" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 1279)">
<title>%3</title>
<polygon fill="white" stroke="none" points="-4,4 -4,-1279 2041,-1279 2041,4 -4,4"/>
<g id="clust2" class="cluster"><title>clusterModel Free</title>
<polygon fill="#f7fdff" stroke="black" points="62,-8 62,-1212 1197,-1212 1197,-8 62,-8"/>
<text text-anchor="middle" x="629.5" y="-1195.2" font-family="sans-serif" font-size="16.00">Model Free</text>
</g>
<g id="clust3" class="cluster"><title>clusterValue Gradient</title>
<path fill="#f1f7fe" stroke="black" stroke-dasharray="5,2" d="M774,-16C774,-16 1177,-16 1177,-16 1183,-16 1189,-22 1189,-28 1189,-28 1189,-1077 1189,-1077 1189,-1083 1183,-1089 1177,-1089 1177,-1089 774,-1089 774,-1089 768,-1089 762,-1083 762,-1077 762,-1077 762,-28 762,-28 762,-22 768,-16 774,-16"/>
<text text-anchor="middle" x="975.5" y="-1072.2" font-family="sans-serif" font-size="16.00">Value Gradient</text>
</g>
<g id="clust4" class="cluster"><title>clusterPolicy Gradient/Actor&#45;Critic</title>
<path fill="#f1f7fe" stroke="black" stroke-dasharray="5,2" d="M82,-162C82,-162 728,-162 728,-162 734,-162 740,-168 740,-174 740,-174 740,-1077 740,-1077 740,-1083 734,-1089 728,-1089 728,-1089 82,-1089 82,-1089 76,-1089 70,-1083 70,-1077 70,-1077 70,-174 70,-174 70,-168 76,-162 82,-162"/>
<text text-anchor="middle" x="405" y="-1072.2" font-family="sans-serif" font-size="16.00">Policy Gradient/Actor&#45;Critic</text>
</g>
<g id="clust5" class="cluster"><title>clusterModel Based</title>
<polygon fill="#f7fdff" stroke="black" points="1205,-89 1205,-1212 1819,-1212 1819,-89 1205,-89"/>
<text text-anchor="middle" x="1512" y="-1195.2" font-family="sans-serif" font-size="16.00">Model Based</text>
</g>
<g id="clust6" class="cluster"><title>clusterMeta&#45;RL</title>
<polygon fill="#f7fdff" stroke="black" points="1827,-162 1827,-1212 2029,-1212 2029,-162 1827,-162"/>
<text text-anchor="middle" x="1928" y="-1195.2" font-family="sans-serif" font-size="16.00">Meta&#45;RL</text>
</g>
<!-- Past -->
<g id="node1" class="node"><title>Past</title>
<text text-anchor="middle" x="27" y="-1130.3" font-family="sans-serif" font-size="14.00" fill="darkblue">Past</text>
</g>
<!-- 1989 -->
<g id="node2" class="node"><title>1989</title>
<text text-anchor="middle" x="27" y="-934.3" font-family="sans-serif" font-size="14.00" fill="darkblue">1989</text>
</g>
<!-- Past&#45;&gt;1989 -->
<g id="edge1" class="edge"><title>Past&#45;&gt;1989</title>
<path fill="none" stroke="darkblue" d="M27,-1115.78C27,-1082.06 27,-1007.06 27,-966.428"/>
<polygon fill="darkblue" stroke="darkblue" points="30.5001,-966.178 27,-956.178 23.5001,-966.178 30.5001,-966.178"/>
</g>
<!-- 1992 -->
<g id="node3" class="node"><title>1992</title>
<text text-anchor="middle" x="27" y="-861.3" font-family="sans-serif" font-size="14.00" fill="darkblue">1992</text>
</g>
<!-- 1989&#45;&gt;1992 -->
<g id="edge2" class="edge"><title>1989&#45;&gt;1992</title>
<path fill="none" stroke="darkblue" d="M27,-919.813C27,-911.789 27,-902.047 27,-893.069"/>
<polygon fill="darkblue" stroke="darkblue" points="30.5001,-893.029 27,-883.029 23.5001,-893.029 30.5001,-893.029"/>
</g>
<!-- 1994 -->
<g id="node4" class="node"><title>1994</title>
<text text-anchor="middle" x="27" y="-788.3" font-family="sans-serif" font-size="14.00" fill="darkblue">1994</text>
</g>
<!-- 1992&#45;&gt;1994 -->
<g id="edge3" class="edge"><title>1992&#45;&gt;1994</title>
<path fill="none" stroke="darkblue" d="M27,-846.813C27,-838.789 27,-829.047 27,-820.069"/>
<polygon fill="darkblue" stroke="darkblue" points="30.5001,-820.029 27,-810.029 23.5001,-820.029 30.5001,-820.029"/>
</g>
<!-- 1995 -->
<g id="node5" class="node"><title>1995</title>
<text text-anchor="middle" x="27" y="-715.3" font-family="sans-serif" font-size="14.00" fill="darkblue">1995</text>
</g>
<!-- 1994&#45;&gt;1995 -->
<g id="edge4" class="edge"><title>1994&#45;&gt;1995</title>
<path fill="none" stroke="darkblue" d="M27,-773.813C27,-765.789 27,-756.047 27,-747.069"/>
<polygon fill="darkblue" stroke="darkblue" points="30.5001,-747.029 27,-737.029 23.5001,-747.029 30.5001,-747.029"/>
</g>
<!-- 2011 -->
<g id="node6" class="node"><title>2011</title>
<text text-anchor="middle" x="27" y="-642.3" font-family="sans-serif" font-size="14.00" fill="darkblue">2011</text>
</g>
<!-- 1995&#45;&gt;2011 -->
<g id="edge5" class="edge"><title>1995&#45;&gt;2011</title>
<path fill="none" stroke="darkblue" d="M27,-700.813C27,-692.789 27,-683.047 27,-674.069"/>
<polygon fill="darkblue" stroke="darkblue" points="30.5001,-674.029 27,-664.029 23.5001,-674.029 30.5001,-674.029"/>
</g>
<!-- 2013 -->
<g id="node7" class="node"><title>2013</title>
<text text-anchor="middle" x="27" y="-569.3" font-family="sans-serif" font-size="14.00" fill="darkblue">2013</text>
</g>
<!-- 2011&#45;&gt;2013 -->
<g id="edge6" class="edge"><title>2011&#45;&gt;2013</title>
<path fill="none" stroke="darkblue" d="M27,-627.813C27,-619.789 27,-610.047 27,-601.069"/>
<polygon fill="darkblue" stroke="darkblue" points="30.5001,-601.029 27,-591.029 23.5001,-601.029 30.5001,-601.029"/>
</g>
<!-- 2014 -->
<g id="node8" class="node"><title>2014</title>
<text text-anchor="middle" x="27" y="-496.3" font-family="sans-serif" font-size="14.00" fill="darkblue">2014</text>
</g>
<!-- 2013&#45;&gt;2014 -->
<g id="edge7" class="edge"><title>2013&#45;&gt;2014</title>
<path fill="none" stroke="darkblue" d="M27,-554.813C27,-546.789 27,-537.047 27,-528.069"/>
<polygon fill="darkblue" stroke="darkblue" points="30.5001,-528.029 27,-518.029 23.5001,-528.029 30.5001,-528.029"/>
</g>
<!-- 2015 -->
<g id="node9" class="node"><title>2015</title>
<text text-anchor="middle" x="27" y="-423.3" font-family="sans-serif" font-size="14.00" fill="darkblue">2015</text>
</g>
<!-- 2014&#45;&gt;2015 -->
<g id="edge8" class="edge"><title>2014&#45;&gt;2015</title>
<path fill="none" stroke="darkblue" d="M27,-481.813C27,-473.789 27,-464.047 27,-455.069"/>
<polygon fill="darkblue" stroke="darkblue" points="30.5001,-455.029 27,-445.029 23.5001,-455.029 30.5001,-455.029"/>
</g>
<!-- 2016 -->
<g id="node10" class="node"><title>2016</title>
<text text-anchor="middle" x="27" y="-350.3" font-family="sans-serif" font-size="14.00" fill="darkblue">2016</text>
</g>
<!-- 2015&#45;&gt;2016 -->
<g id="edge9" class="edge"><title>2015&#45;&gt;2016</title>
<path fill="none" stroke="darkblue" d="M27,-408.813C27,-400.789 27,-391.047 27,-382.069"/>
<polygon fill="darkblue" stroke="darkblue" points="30.5001,-382.029 27,-372.029 23.5001,-382.029 30.5001,-382.029"/>
</g>
<!-- 2017 -->
<g id="node11" class="node"><title>2017</title>
<text text-anchor="middle" x="27" y="-267.3" font-family="sans-serif" font-size="14.00" fill="darkblue">2017</text>
</g>
<!-- 2016&#45;&gt;2017 -->
<g id="edge10" class="edge"><title>2016&#45;&gt;2017</title>
<path fill="none" stroke="darkblue" d="M27,-335.822C27,-325.19 27,-311.306 27,-299.204"/>
<polygon fill="darkblue" stroke="darkblue" points="30.5001,-299.153 27,-289.153 23.5001,-299.153 30.5001,-299.153"/>
</g>
<!-- 2018 -->
<g id="node12" class="node"><title>2018</title>
<text text-anchor="middle" x="27" y="-184.3" font-family="sans-serif" font-size="14.00" fill="darkblue">2018</text>
</g>
<!-- 2017&#45;&gt;2018 -->
<g id="edge11" class="edge"><title>2017&#45;&gt;2018</title>
<path fill="none" stroke="darkblue" d="M27,-252.822C27,-242.19 27,-228.306 27,-216.204"/>
<polygon fill="darkblue" stroke="darkblue" points="30.5001,-216.153 27,-206.153 23.5001,-216.153 30.5001,-216.153"/>
</g>
<!-- 2019 -->
<g id="node13" class="node"><title>2019</title>
<text text-anchor="middle" x="27" y="-111.3" font-family="sans-serif" font-size="14.00" fill="darkblue">2019</text>
</g>
<!-- 2018&#45;&gt;2019 -->
<g id="edge12" class="edge"><title>2018&#45;&gt;2019</title>
<path fill="none" stroke="darkblue" d="M27,-169.813C27,-161.789 27,-152.047 27,-143.069"/>
<polygon fill="darkblue" stroke="darkblue" points="30.5001,-143.029 27,-133.029 23.5001,-143.029 30.5001,-143.029"/>
</g>
<!-- 2020 -->
<g id="node14" class="node"><title>2020</title>
<text text-anchor="middle" x="27" y="-38.3" font-family="sans-serif" font-size="14.00" fill="darkblue">2020</text>
</g>
<!-- 2019&#45;&gt;2020 -->
<g id="edge13" class="edge"><title>2019&#45;&gt;2020</title>
<path fill="none" stroke="darkblue" d="M27,-96.8129C27,-88.7895 27,-79.0475 27,-70.0691"/>
<polygon fill="darkblue" stroke="darkblue" points="30.5001,-70.0288 27,-60.0288 23.5001,-70.0289 30.5001,-70.0288"/>
</g>
<!-- Reinforcement Learning -->
<g id="node15" class="node"><title>Reinforcement Learning</title>
<g id="a_node15"><a xlink:title="Reinforcement learning (RL) is an area of machine learning concerned with how
software agents ought to take actions in an environment in order to maximize
the notion of cumulative reward [from Wikipedia]

">
<path fill="#ffe6cc" stroke="black" stroke-width="2" d="M1636,-1275C1636,-1275 1504,-1275 1504,-1275 1498,-1275 1492,-1269 1492,-1263 1492,-1263 1492,-1251 1492,-1251 1492,-1245 1498,-1239 1504,-1239 1504,-1239 1636,-1239 1636,-1239 1642,-1239 1648,-1245 1648,-1251 1648,-1251 1648,-1263 1648,-1263 1648,-1269 1642,-1275 1636,-1275"/>
<text text-anchor="middle" x="1570" y="-1253.9" font-family="sans-serif" font-size="12.00">Reinforcement Learning</text>
</a>
</g>
</g>
<!-- Model Free -->
<g id="node16" class="node"><title>Model Free</title>
<g id="a_node16"><a xlink:title="In model free reinforcement learning, the agent directly tries to predict the
value/policy without having or trying to model the environment

">
<path fill="#ffe6cc" stroke="black" stroke-width="2" d="M1009,-1152C1009,-1152 953,-1152 953,-1152 947,-1152 941,-1146 941,-1140 941,-1140 941,-1128 941,-1128 941,-1122 947,-1116 953,-1116 953,-1116 1009,-1116 1009,-1116 1015,-1116 1021,-1122 1021,-1128 1021,-1128 1021,-1140 1021,-1140 1021,-1146 1015,-1152 1009,-1152"/>
<text text-anchor="middle" x="981" y="-1130.9" font-family="sans-serif" font-size="12.00">Model Free</text>
</a>
</g>
</g>
<!-- Reinforcement Learning&#45;&gt;Model Free -->
<g id="edge86" class="edge"><title>Reinforcement Learning&#45;&gt;Model Free</title>
<path fill="none" stroke="black" d="M1491.76,-1251.63C1416.48,-1246.27 1299.83,-1234.9 1201,-1212 1140.95,-1198.09 1074.47,-1173.47 1030.48,-1155.81"/>
<polygon fill="black" stroke="black" points="1031.75,-1152.55 1021.17,-1152.04 1029.12,-1159.04 1031.75,-1152.55"/>
</g>
<!-- Model Based -->
<g id="node54" class="node"><title>Model Based</title>
<g id="a_node54"><a xlink:title="In model&#45;based reinforcement learning, the agent uses the experience to try to
model the environment, and then uses the model to predict the value/policy

">
<path fill="#ffe6cc" stroke="black" stroke-width="2" d="M1603,-1152C1603,-1152 1537,-1152 1537,-1152 1531,-1152 1525,-1146 1525,-1140 1525,-1140 1525,-1128 1525,-1128 1525,-1122 1531,-1116 1537,-1116 1537,-1116 1603,-1116 1603,-1116 1609,-1116 1615,-1122 1615,-1128 1615,-1128 1615,-1140 1615,-1140 1615,-1146 1609,-1152 1603,-1152"/>
<text text-anchor="middle" x="1570" y="-1130.9" font-family="sans-serif" font-size="12.00">Model Based</text>
</a>
</g>
</g>
<!-- Reinforcement Learning&#45;&gt;Model Based -->
<g id="edge87" class="edge"><title>Reinforcement Learning&#45;&gt;Model Based</title>
<path fill="none" stroke="black" d="M1570,-1238.92C1570,-1219.04 1570,-1185.92 1570,-1162.42"/>
<polygon fill="black" stroke="black" points="1573.5,-1162.38 1570,-1152.38 1566.5,-1162.38 1573.5,-1162.38"/>
</g>
<!-- Meta&#45;RL -->
<g id="node69" class="node"><title>Meta&#45;RL</title>
<g id="a_node69"><a xlink:title="In meta reinforcement learning, the agent is trained over distribution of
tasks, and with the knowledge it tries to solve new unseen but related task.

">
<path fill="#ffe6cc" stroke="black" stroke-width="2" d="M1900,-1152C1900,-1152 1860,-1152 1860,-1152 1854,-1152 1848,-1146 1848,-1140 1848,-1140 1848,-1128 1848,-1128 1848,-1122 1854,-1116 1860,-1116 1860,-1116 1900,-1116 1900,-1116 1906,-1116 1912,-1122 1912,-1128 1912,-1128 1912,-1140 1912,-1140 1912,-1146 1906,-1152 1900,-1152"/>
<text text-anchor="middle" x="1880" y="-1130.9" font-family="sans-serif" font-size="12.00">Meta&#45;RL</text>
</a>
</g>
</g>
<!-- Reinforcement Learning&#45;&gt;Meta&#45;RL -->
<g id="edge88" class="edge"><title>Reinforcement Learning&#45;&gt;Meta&#45;RL</title>
<path fill="none" stroke="black" d="M1648,-1248.82C1711.2,-1241.85 1794.31,-1229.71 1823,-1212 1842.59,-1199.91 1857.65,-1178.4 1867.36,-1161.17"/>
<polygon fill="black" stroke="black" points="1870.57,-1162.6 1872.2,-1152.14 1864.4,-1159.3 1870.57,-1162.6"/>
</g>
<!-- Value Gradient -->
<g id="node17" class="node"><title>Value Gradient</title>
<g id="a_node17"><a xlink:title="The algorithm is learning the value function of each state or state&#45;action.
The policy is implicit, usually by just selecting the best value

">
<path fill="#ffe6cc" stroke="black" stroke-width="2" d="M1019.5,-1029C1019.5,-1029 942.5,-1029 942.5,-1029 936.5,-1029 930.5,-1023 930.5,-1017 930.5,-1017 930.5,-1005 930.5,-1005 930.5,-999 936.5,-993 942.5,-993 942.5,-993 1019.5,-993 1019.5,-993 1025.5,-993 1031.5,-999 1031.5,-1005 1031.5,-1005 1031.5,-1017 1031.5,-1017 1031.5,-1023 1025.5,-1029 1019.5,-1029"/>
<text text-anchor="middle" x="981" y="-1007.9" font-family="sans-serif" font-size="12.00">Value Gradient</text>
</a>
</g>
</g>
<!-- Model Free&#45;&gt;Value Gradient -->
<g id="edge14" class="edge"><title>Model Free&#45;&gt;Value Gradient</title>
<path fill="none" stroke="black" d="M981,-1115.92C981,-1096.04 981,-1062.92 981,-1039.42"/>
<polygon fill="black" stroke="black" points="984.5,-1039.38 981,-1029.38 977.5,-1039.38 984.5,-1039.38"/>
</g>
<!-- Policy Gradient/Actor&#45;Critic -->
<g id="node18" class="node"><title>Policy Gradient/Actor&#45;Critic</title>
<g id="a_node18"><a xlink:title="The algorithm works directly to optimize the policy, with or without value
function. If the value function is learned in addition to the policy, we would
get Actor&#45;Critic algorithm. Most policy gradient algorithms are Actor&#45;Critic.
The Critic updates value function parameters w and depending on the algorithm
it could be action&#45;value Q(a|s;w) or state&#45;value V(s;w). The Actor updates
policy parameters θ, in the direction suggested by the critic, π(a|s;θ). [from
Lilian Weng&#39; blog]

">
<path fill="#ffe6cc" stroke="black" stroke-width="2" d="M559.5,-1029C559.5,-1029 410.5,-1029 410.5,-1029 404.5,-1029 398.5,-1023 398.5,-1017 398.5,-1017 398.5,-1005 398.5,-1005 398.5,-999 404.5,-993 410.5,-993 410.5,-993 559.5,-993 559.5,-993 565.5,-993 571.5,-999 571.5,-1005 571.5,-1005 571.5,-1017 571.5,-1017 571.5,-1023 565.5,-1029 559.5,-1029"/>
<text text-anchor="middle" x="485" y="-1007.9" font-family="sans-serif" font-size="12.00">Policy Gradient/Actor&#45;Critic</text>
</a>
</g>
</g>
<!-- Model Free&#45;&gt;Policy Gradient/Actor&#45;Critic -->
<g id="edge15" class="edge"><title>Model Free&#45;&gt;Policy Gradient/Actor&#45;Critic</title>
<path fill="none" stroke="black" d="M940.911,-1126.09C896.109,-1118.13 821.516,-1104.2 758,-1089 686.276,-1071.84 605.043,-1048.4 549.971,-1031.91"/>
<polygon fill="black" stroke="black" points="550.926,-1028.54 540.342,-1029.02 548.912,-1035.25 550.926,-1028.54"/>
</g>
<!-- SARSA -->
<g id="node30" class="node"><title>SARSA</title>
<g id="a_node30"><a xlink:title="SARSA (State&#45;Action&#45;Reward&#45;State&#45;Action) is an on&#45;policy TD control method

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M939,-810C939,-810 907,-810 907,-810 901,-810 895,-804 895,-798 895,-798 895,-786 895,-786 895,-780 901,-774 907,-774 907,-774 939,-774 939,-774 945,-774 951,-780 951,-786 951,-786 951,-798 951,-798 951,-804 945,-810 939,-810"/>
<text text-anchor="middle" x="923" y="-788.9" font-family="sans-serif" font-size="12.00">SARSA</text>
</a>
</g>
</g>
<!-- Value Gradient&#45;&gt;SARSA -->
<g id="edge22" class="edge"><title>Value Gradient&#45;&gt;SARSA</title>
<path fill="none" stroke="black" d="M960.142,-992.718C950.051,-983.057 938.874,-970.131 933,-956 914.44,-911.352 916.171,-853.852 919.395,-820.438"/>
<polygon fill="black" stroke="black" points="922.876,-820.801 920.47,-810.483 915.916,-820.049 922.876,-820.801"/>
</g>
<!-- Q&#45;learning -->
<g id="node31" class="node"><title>Q&#45;learning</title>
<g id="a_node31"><a xlink:title="Q&#45;learning an off&#45;policy TD control method. Unlike SARSA, it doesn&#39;t follow
the policy to find the next action but rather chooses most optimal action in a
greedy fashion

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1007.5,-956C1007.5,-956 954.5,-956 954.5,-956 948.5,-956 942.5,-950 942.5,-944 942.5,-944 942.5,-932 942.5,-932 942.5,-926 948.5,-920 954.5,-920 954.5,-920 1007.5,-920 1007.5,-920 1013.5,-920 1019.5,-926 1019.5,-932 1019.5,-932 1019.5,-944 1019.5,-944 1019.5,-950 1013.5,-956 1007.5,-956"/>
<text text-anchor="middle" x="981" y="-934.9" font-family="sans-serif" font-size="12.00">Q&#45;learning</text>
</a>
</g>
</g>
<!-- Value Gradient&#45;&gt;Q&#45;learning -->
<g id="edge23" class="edge"><title>Value Gradient&#45;&gt;Q&#45;learning</title>
<path fill="none" stroke="black" d="M981,-992.813C981,-984.789 981,-975.047 981,-966.069"/>
<polygon fill="black" stroke="black" points="984.5,-966.029 981,-956.029 977.5,-966.029 984.5,-966.029"/>
</g>
<!-- TD&#45;Gammon -->
<g id="node32" class="node"><title>TD&#45;Gammon</title>
<g id="a_node32"><a xlink:title="TD&#45;Gammon is a model&#45;free reinforcement learning algorithm similar to
Q&#45;learning, and uses a multi&#45;layer perceptron with one hidden layer as the
value function approximator. It learns the game entirely by playing against
itself and achieves superhuman level of play.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1077,-737C1077,-737 1011,-737 1011,-737 1005,-737 999,-731 999,-725 999,-725 999,-713 999,-713 999,-707 1005,-701 1011,-701 1011,-701 1077,-701 1077,-701 1083,-701 1089,-707 1089,-713 1089,-713 1089,-725 1089,-725 1089,-731 1083,-737 1077,-737"/>
<text text-anchor="middle" x="1044" y="-715.9" font-family="sans-serif" font-size="12.00">TD&#45;Gammon</text>
</a>
</g>
</g>
<!-- Value Gradient&#45;&gt;TD&#45;Gammon -->
<g id="edge24" class="edge"><title>Value Gradient&#45;&gt;TD&#45;Gammon</title>
<path fill="none" stroke="black" d="M1007.93,-992.946C1024.27,-980.306 1042,-961.426 1042,-939 1042,-939 1042,-939 1042,-791 1042,-776.646 1042.41,-760.665 1042.86,-747.506"/>
<polygon fill="black" stroke="black" points="1046.37,-747.335 1043.24,-737.213 1039.38,-747.077 1046.37,-747.335"/>
</g>
<!-- A3C -->
<g id="node28" class="node"><title>A3C</title>
<g id="a_node28"><a xlink:title="Asynchronous Advantage Actor&#45;Critic (A3C) is a classic policy gradient method
with the special focus on parallel training. In A3C, the critics learn the
state&#45;value function, V(s;w), while multiple actors are trained in parallel
and get synced with global parameters from time to time. Hence, A3C is good
for parallel training by default, i.e. on one machine with multi&#45;core CPU.
[from Lilian Weng&#39; blog]

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M678,-372C678,-372 648,-372 648,-372 642,-372 636,-366 636,-360 636,-360 636,-348 636,-348 636,-342 642,-336 648,-336 648,-336 678,-336 678,-336 684,-336 690,-342 690,-348 690,-348 690,-360 690,-360 690,-366 684,-372 678,-372"/>
<text text-anchor="middle" x="663" y="-350.9" font-family="sans-serif" font-size="12.00">A3C</text>
</a>
</g>
</g>
<!-- Policy Gradient/Actor&#45;Critic&#45;&gt;A3C -->
<g id="edge46" class="edge"><title>Policy Gradient/Actor&#45;Critic&#45;&gt;A3C</title>
<path fill="none" stroke="black" d="M514.91,-992.911C532.373,-980.496 551,-961.888 551,-939 551,-939 551,-939 551,-426 551,-393.054 596.541,-385.982 626.622,-373.687"/>
<polygon fill="black" stroke="black" points="628.151,-376.836 635.768,-369.472 625.222,-370.478 628.151,-376.836"/>
</g>
<!-- REINFORCE -->
<g id="node42" class="node"><title>REINFORCE</title>
<g id="a_node42"><a xlink:title="REINFORCE (Monte&#45;Carlo policy gradient) is a pure policy gradient algorithm
that works without a value function. The agent collects a trajectory of one
episode using its current policy, and uses the returns to update the policy
parameter

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M474,-883C474,-883 412,-883 412,-883 406,-883 400,-877 400,-871 400,-871 400,-859 400,-859 400,-853 406,-847 412,-847 412,-847 474,-847 474,-847 480,-847 486,-853 486,-859 486,-859 486,-871 486,-871 486,-877 480,-883 474,-883"/>
<text text-anchor="middle" x="443" y="-861.9" font-family="sans-serif" font-size="12.00">REINFORCE</text>
</a>
</g>
</g>
<!-- Policy Gradient/Actor&#45;Critic&#45;&gt;REINFORCE -->
<g id="edge42" class="edge"><title>Policy Gradient/Actor&#45;Critic&#45;&gt;REINFORCE</title>
<path fill="none" stroke="black" d="M480.017,-992.916C472.818,-968.233 459.436,-922.351 450.887,-893.041"/>
<polygon fill="black" stroke="black" points="454.136,-891.682 447.976,-883.062 447.416,-893.642 454.136,-891.682"/>
</g>
<!-- DPG -->
<g id="node43" class="node"><title>DPG</title>
<g id="a_node43"><a xlink:title="Deterministic Policy Gradient. Abstract: In this paper we consider
deterministic policy gradient algorithms for reinforcement learning with
continuous actions. The deterministic policy gradient has a particularly
appealing form: it is the expected gradient of the action&#45;value function. This
simple form means that the deterministic policy gradient can be estimated much
more efficiently than the usual stochastic policy gradient. To ensure adequate
exploration, we introduce an off&#45;policy actor&#45;critic algorithm that learns a
deterministic target policy from an exploratory behaviour policy. We
demonstrate that deterministic policy gradient algorithms can significantly
outperform their stochastic counterparts in high&#45;dimensional action spaces.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M394,-518C394,-518 364,-518 364,-518 358,-518 352,-512 352,-506 352,-506 352,-494 352,-494 352,-488 358,-482 364,-482 364,-482 394,-482 394,-482 400,-482 406,-488 406,-494 406,-494 406,-506 406,-506 406,-512 400,-518 394,-518"/>
<text text-anchor="middle" x="379" y="-496.9" font-family="sans-serif" font-size="12.00">DPG</text>
</a>
</g>
</g>
<!-- Policy Gradient/Actor&#45;Critic&#45;&gt;DPG -->
<g id="edge43" class="edge"><title>Policy Gradient/Actor&#45;Critic&#45;&gt;DPG</title>
<path fill="none" stroke="black" d="M426.36,-992.985C402.309,-982.08 380,-964.966 380,-939 380,-939 380,-939 380,-572 380,-557.65 379.793,-541.67 379.568,-528.509"/>
<polygon fill="black" stroke="black" points="383.062,-528.149 379.378,-518.215 376.063,-528.278 383.062,-528.149"/>
</g>
<!-- TRPO -->
<g id="node44" class="node"><title>TRPO</title>
<g id="a_node44"><a xlink:title="Trust Region Policy Optimization (TRPO) improves training stability by
enforcing a KL divergence constraint to avoid parameter updates that change
the policy too much at one step.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M520,-445C520,-445 490,-445 490,-445 484,-445 478,-439 478,-433 478,-433 478,-421 478,-421 478,-415 484,-409 490,-409 490,-409 520,-409 520,-409 526,-409 532,-415 532,-421 532,-421 532,-433 532,-433 532,-439 526,-445 520,-445"/>
<text text-anchor="middle" x="505" y="-423.9" font-family="sans-serif" font-size="12.00">TRPO</text>
</a>
</g>
</g>
<!-- Policy Gradient/Actor&#45;Critic&#45;&gt;TRPO -->
<g id="edge44" class="edge"><title>Policy Gradient/Actor&#45;Critic&#45;&gt;TRPO</title>
<path fill="none" stroke="black" d="M492.675,-992.872C498.315,-978.722 505,-957.953 505,-939 505,-939 505,-939 505,-499 505,-484.651 505,-468.671 505,-455.511"/>
<polygon fill="black" stroke="black" points="508.5,-455.216 505,-445.216 501.5,-455.216 508.5,-455.216"/>
</g>
<!-- GAE -->
<g id="node45" class="node"><title>GAE</title>
<g id="a_node45"><a xlink:title="Generalized Advantage Estimation

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M612,-445C612,-445 582,-445 582,-445 576,-445 570,-439 570,-433 570,-433 570,-421 570,-421 570,-415 576,-409 582,-409 582,-409 612,-409 612,-409 618,-409 624,-415 624,-421 624,-421 624,-433 624,-433 624,-439 618,-445 612,-445"/>
<text text-anchor="middle" x="597" y="-423.9" font-family="sans-serif" font-size="12.00">GAE</text>
</a>
</g>
</g>
<!-- Policy Gradient/Actor&#45;Critic&#45;&gt;GAE -->
<g id="edge45" class="edge"><title>Policy Gradient/Actor&#45;Critic&#45;&gt;GAE</title>
<path fill="none" stroke="black" d="M538.556,-992.942C561.883,-981.854 584,-964.592 584,-939 584,-939 584,-939 584,-499 584,-484.267 586.742,-468.091 589.707,-454.905"/>
<polygon fill="black" stroke="black" points="593.14,-455.599 592.085,-445.056 586.336,-453.956 593.14,-455.599"/>
</g>
<!-- ACKTR -->
<g id="node46" class="node"><title>ACKTR</title>
<g id="a_node46"><a xlink:title="Actor Critic using Kronecker&#45;Factored Trust Region (ACKTR) is applying trust
region optimization to deep reinforcement learning using a recently proposed
Kronecker&#45;factored approximation to the curvature.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M142,-289C142,-289 110,-289 110,-289 104,-289 98,-283 98,-277 98,-277 98,-265 98,-265 98,-259 104,-253 110,-253 110,-253 142,-253 142,-253 148,-253 154,-259 154,-265 154,-265 154,-277 154,-277 154,-283 148,-289 142,-289"/>
<text text-anchor="middle" x="126" y="-267.9" font-family="sans-serif" font-size="12.00">ACKTR</text>
</a>
</g>
</g>
<!-- Policy Gradient/Actor&#45;Critic&#45;&gt;ACKTR -->
<g id="edge47" class="edge"><title>Policy Gradient/Actor&#45;Critic&#45;&gt;ACKTR</title>
<path fill="none" stroke="black" d="M398.2,-1005.28C313.4,-998.225 197,-980.906 197,-939 197,-939 197,-939 197,-353 197,-329.324 179.634,-309.254 162.078,-295.015"/>
<polygon fill="black" stroke="black" points="164.133,-292.18 154.061,-288.891 159.883,-297.743 164.133,-292.18"/>
</g>
<!-- SVPG -->
<g id="node47" class="node"><title>SVPG</title>
<g id="a_node47"><a xlink:title="Stein Variational Policy Gradient (SVPG)

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M214,-289C214,-289 184,-289 184,-289 178,-289 172,-283 172,-277 172,-277 172,-265 172,-265 172,-259 178,-253 184,-253 184,-253 214,-253 214,-253 220,-253 226,-259 226,-265 226,-265 226,-277 226,-277 226,-283 220,-289 214,-289"/>
<text text-anchor="middle" x="199" y="-267.9" font-family="sans-serif" font-size="12.00">SVPG</text>
</a>
</g>
</g>
<!-- Policy Gradient/Actor&#45;Critic&#45;&gt;SVPG -->
<g id="edge48" class="edge"><title>Policy Gradient/Actor&#45;Critic&#45;&gt;SVPG</title>
<path fill="none" stroke="black" d="M398.282,-999.76C332.386,-989.474 253,-970.539 253,-939 253,-939 253,-939 253,-353 253,-331.412 239.556,-311.201 226.084,-296.367"/>
<polygon fill="black" stroke="black" points="228.565,-293.898 219.108,-289.122 223.522,-298.753 228.565,-293.898"/>
</g>
<!-- SAC -->
<g id="node48" class="node"><title>SAC</title>
<g id="a_node48"><a xlink:title="Soft Actor Critic (SAC) is an algorithm that optimizes a stochastic policy in
an off&#45;policy way, forming a bridge between stochastic policy optimization and
DDPG&#45;style approaches.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M342,-206C342,-206 312,-206 312,-206 306,-206 300,-200 300,-194 300,-194 300,-182 300,-182 300,-176 306,-170 312,-170 312,-170 342,-170 342,-170 348,-170 354,-176 354,-182 354,-182 354,-194 354,-194 354,-200 348,-206 342,-206"/>
<text text-anchor="middle" x="327" y="-184.9" font-family="sans-serif" font-size="12.00">SAC</text>
</a>
</g>
</g>
<!-- Policy Gradient/Actor&#45;Critic&#45;&gt;SAC -->
<g id="edge49" class="edge"><title>Policy Gradient/Actor&#45;Critic&#45;&gt;SAC</title>
<path fill="none" stroke="black" d="M398.401,-1003.41C279.163,-993.159 79,-971.155 79,-939 79,-939 79,-939 79,-311.5 79,-285.123 70.9097,-272.196 89,-253 150.056,-188.211 200.395,-232.989 290.064,-205.777"/>
<polygon fill="black" stroke="black" points="291.321,-209.049 299.734,-202.61 289.141,-202.396 291.321,-209.049"/>
</g>
<!-- IMPALA -->
<g id="node49" class="node"><title>IMPALA</title>
<g id="a_node49"><a xlink:title="Importance Weighted Actor&#45;Learner Architecture (IMPALA)

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M588.5,-206C588.5,-206 551.5,-206 551.5,-206 545.5,-206 539.5,-200 539.5,-194 539.5,-194 539.5,-182 539.5,-182 539.5,-176 545.5,-170 551.5,-170 551.5,-170 588.5,-170 588.5,-170 594.5,-170 600.5,-176 600.5,-182 600.5,-182 600.5,-194 600.5,-194 600.5,-200 594.5,-206 588.5,-206"/>
<text text-anchor="middle" x="570" y="-184.9" font-family="sans-serif" font-size="12.00">IMPALA</text>
</a>
</g>
</g>
<!-- Policy Gradient/Actor&#45;Critic&#45;&gt;IMPALA -->
<g id="edge50" class="edge"><title>Policy Gradient/Actor&#45;Critic&#45;&gt;IMPALA</title>
<path fill="none" stroke="black" d="M571.627,-996.796C629.474,-985.447 695,-966.517 695,-939 695,-939 695,-939 695,-463 695,-368.954 772.929,-331.41 721,-253 720.169,-251.746 654.557,-224.226 610.104,-205.686"/>
<polygon fill="black" stroke="black" points="611.272,-202.381 600.695,-201.764 608.578,-208.842 611.272,-202.381"/>
</g>
<!-- DQN -->
<g id="node19" class="node"><title>DQN</title>
<g id="a_node19"><a xlink:title="Deep Q Network (DQN) is Q&#45;Learning with deep neural network as state&#45;action
value estimator and uses a replay buffer to sample experiences from previous
trajectories to make learning more stable.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M995,-591C995,-591 965,-591 965,-591 959,-591 953,-585 953,-579 953,-579 953,-567 953,-567 953,-561 959,-555 965,-555 965,-555 995,-555 995,-555 1001,-555 1007,-561 1007,-567 1007,-567 1007,-579 1007,-579 1007,-585 1001,-591 995,-591"/>
<text text-anchor="middle" x="980" y="-569.9" font-family="sans-serif" font-size="12.00">DQN</text>
</a>
</g>
</g>
<!-- DDPG -->
<g id="node20" class="node"><title>DDPG</title>
<g id="a_node20"><a xlink:title="Deep Deterministic Policy Gradient (DDPG).

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M385,-445C385,-445 355,-445 355,-445 349,-445 343,-439 343,-433 343,-433 343,-421 343,-421 343,-415 349,-409 355,-409 355,-409 385,-409 385,-409 391,-409 397,-415 397,-421 397,-421 397,-433 397,-433 397,-439 391,-445 385,-445"/>
<text text-anchor="middle" x="370" y="-423.9" font-family="sans-serif" font-size="12.00">DDPG</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;DDPG -->
<g id="edge16" class="edge"><title>DQN&#45;&gt;DDPG</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M952.605,-565.533C854.363,-542.341 519.633,-463.323 407.187,-436.779"/>
<polygon fill="darkgray" stroke="darkgray" points="407.857,-433.341 397.32,-434.449 406.248,-440.153 407.857,-433.341"/>
<text text-anchor="middle" x="776.5" y="-497.5" font-family="sans-serif" font-size="10.00" fill="darkgray">replay buffer</text>
</g>
<!-- ACER -->
<g id="node21" class="node"><title>ACER</title>
<g id="a_node21"><a xlink:title="Actor&#45;Critic with Experience Replay (ACER) combines several ideas of previous
algorithms: it uses multiple workers (as A2C), implements a replay buffer (as
in DQN), uses Retrace for Q&#45;value estimation, importance sampling and a trust
region. ACER is A3C&#39;s off&#45;policy counterpart. ACER proposes several designs to
overcome the major obstacle to making A3C off policy, that is how to control
the stability of the off&#45;policy estimator. (source: Lilian Weng&#39;s blog)

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M700,-289C700,-289 670,-289 670,-289 664,-289 658,-283 658,-277 658,-277 658,-265 658,-265 658,-259 664,-253 670,-253 670,-253 700,-253 700,-253 706,-253 712,-259 712,-265 712,-265 712,-277 712,-277 712,-283 706,-289 700,-289"/>
<text text-anchor="middle" x="685" y="-267.9" font-family="sans-serif" font-size="12.00">ACER</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;ACER -->
<g id="edge17" class="edge"><title>DQN&#45;&gt;ACER</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M952.581,-569.889C895.161,-564.866 765.242,-550.138 737,-518 674.846,-447.272 756.643,-392.862 718,-307 716.434,-303.521 714.376,-300.168 712.051,-297.009"/>
<polygon fill="darkgray" stroke="darkgray" points="714.547,-294.538 705.43,-289.14 709.19,-299.045 714.547,-294.538"/>
<text text-anchor="middle" x="751.5" y="-424.5" font-family="sans-serif" font-size="10.00" fill="darkgray">replay buffer</text>
</g>
<!-- DDQN -->
<g id="node22" class="node"><title>DDQN</title>
<g id="a_node22"><a xlink:title="Double DQN adds another neural network, making separate network for policy and
target. The target network is only updated after certain number of
steps/episodes. This makes the learning more stable.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1022,-445C1022,-445 992,-445 992,-445 986,-445 980,-439 980,-433 980,-433 980,-421 980,-421 980,-415 986,-409 992,-409 992,-409 1022,-409 1022,-409 1028,-409 1034,-415 1034,-421 1034,-421 1034,-433 1034,-433 1034,-439 1028,-445 1022,-445"/>
<text text-anchor="middle" x="1007" y="-423.9" font-family="sans-serif" font-size="12.00">DDQN</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;DDQN -->
<g id="edge27" class="edge"><title>DQN&#45;&gt;DDQN</title>
<path fill="none" stroke="black" d="M983.203,-554.916C987.831,-530.233 996.434,-484.351 1001.93,-455.041"/>
<polygon fill="black" stroke="black" points="1005.4,-455.536 1003.8,-445.062 998.518,-454.246 1005.4,-455.536"/>
</g>
<!-- DQN+HER -->
<g id="node24" class="node"><title>DQN+HER</title>
<g id="a_node24"><a xlink:title="DQN with Hindsight Experience Replay (HER)

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M837.5,-289C837.5,-289 782.5,-289 782.5,-289 776.5,-289 770.5,-283 770.5,-277 770.5,-277 770.5,-265 770.5,-265 770.5,-259 776.5,-253 782.5,-253 782.5,-253 837.5,-253 837.5,-253 843.5,-253 849.5,-259 849.5,-265 849.5,-265 849.5,-277 849.5,-277 849.5,-283 843.5,-289 837.5,-289"/>
<text text-anchor="middle" x="810" y="-267.9" font-family="sans-serif" font-size="12.00">DQN+HER</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;DQN+HER -->
<g id="edge31" class="edge"><title>DQN&#45;&gt;DQN+HER</title>
<path fill="none" stroke="black" d="M952.861,-570.333C927.072,-567.387 888.666,-559.276 864,-537 851.136,-525.383 849,-518.333 849,-501 849,-501 849,-501 849,-353 849,-332.435 852.752,-325.61 844,-307 842.304,-303.395 840.084,-299.933 837.59,-296.683"/>
<polygon fill="black" stroke="black" points="840.174,-294.321 830.965,-289.082 834.897,-298.921 840.174,-294.321"/>
</g>
<!-- APE&#45;X DQN -->
<g id="node26" class="node"><title>APE&#45;X DQN</title>
<g id="a_node26"><a xlink:title="DQN with Distributed Prioritized Experience Replay

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M899,-206C899,-206 839,-206 839,-206 833,-206 827,-200 827,-194 827,-194 827,-182 827,-182 827,-176 833,-170 839,-170 839,-170 899,-170 899,-170 905,-170 911,-176 911,-182 911,-182 911,-194 911,-194 911,-200 905,-206 899,-206"/>
<text text-anchor="middle" x="869" y="-184.9" font-family="sans-serif" font-size="12.00">APE&#45;X DQN</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;APE&#45;X DQN -->
<g id="edge33" class="edge"><title>DQN&#45;&gt;APE&#45;X DQN</title>
<path fill="none" stroke="black" d="M952.846,-566.997C919.985,-559.193 869,-540.69 869,-501 869,-501 869,-501 869,-270 869,-252.062 869,-231.888 869,-216.162"/>
<polygon fill="black" stroke="black" points="872.5,-216.066 869,-206.066 865.5,-216.066 872.5,-216.066"/>
</g>
<!-- DRQN -->
<g id="node33" class="node"><title>DRQN</title>
<g id="a_node33"><a xlink:title="Deep Recurrent Q&#45;Learning. Adding recurrency to a Deep Q&#45;Network (DQN) by
replacing the first post&#45;convolutional fully&#45;connected layer with a recurrent
LSTM

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M950,-445C950,-445 920,-445 920,-445 914,-445 908,-439 908,-433 908,-433 908,-421 908,-421 908,-415 914,-409 920,-409 920,-409 950,-409 950,-409 956,-409 962,-415 962,-421 962,-421 962,-433 962,-433 962,-439 956,-445 950,-445"/>
<text text-anchor="middle" x="935" y="-423.9" font-family="sans-serif" font-size="12.00">DRQN</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;DRQN -->
<g id="edge26" class="edge"><title>DQN&#45;&gt;DRQN</title>
<path fill="none" stroke="black" d="M974.661,-554.916C966.914,-530.125 952.484,-483.948 943.33,-454.656"/>
<polygon fill="black" stroke="black" points="946.655,-453.563 940.332,-445.062 939.974,-455.651 946.655,-453.563"/>
</g>
<!-- PER -->
<g id="node34" class="node"><title>PER</title>
<g id="a_node34"><a xlink:title="Prioritized Experience Replay (PER) improves data efficiency by replaying
transitions from which there is more to learn more often

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1094,-445C1094,-445 1064,-445 1064,-445 1058,-445 1052,-439 1052,-433 1052,-433 1052,-421 1052,-421 1052,-415 1058,-409 1064,-409 1064,-409 1094,-409 1094,-409 1100,-409 1106,-415 1106,-421 1106,-421 1106,-433 1106,-433 1106,-439 1100,-445 1094,-445"/>
<text text-anchor="middle" x="1079" y="-423.9" font-family="sans-serif" font-size="12.00">PER</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;PER -->
<g id="edge28" class="edge"><title>DQN&#45;&gt;PER</title>
<path fill="none" stroke="black" d="M991.745,-554.916C1009.01,-529.8 1041.37,-482.734 1061.46,-453.514"/>
<polygon fill="black" stroke="black" points="1064.49,-455.285 1067.27,-445.062 1058.72,-451.32 1064.49,-455.285"/>
</g>
<!-- QR&#45;DQN -->
<g id="node35" class="node"><title>QR&#45;DQN</title>
<g id="a_node35"><a xlink:title="Distributional Reinforcement Learning with Quantile Regression (QR&#45;DQN). In
QR&#45;DQN, distribution of values values are used for each state&#45;action pair
instead of a single mean value

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M943.5,-289C943.5,-289 900.5,-289 900.5,-289 894.5,-289 888.5,-283 888.5,-277 888.5,-277 888.5,-265 888.5,-265 888.5,-259 894.5,-253 900.5,-253 900.5,-253 943.5,-253 943.5,-253 949.5,-253 955.5,-259 955.5,-265 955.5,-265 955.5,-277 955.5,-277 955.5,-283 949.5,-289 943.5,-289"/>
<text text-anchor="middle" x="922" y="-267.9" font-family="sans-serif" font-size="12.00">QR&#45;DQN</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;QR&#45;DQN -->
<g id="edge29" class="edge"><title>DQN&#45;&gt;QR&#45;DQN</title>
<path fill="none" stroke="black" d="M952.697,-563.629C925.892,-553.516 889,-533.663 889,-501 889,-501 889,-501 889,-353 889,-333.635 897.19,-313.462 905.409,-298.092"/>
<polygon fill="black" stroke="black" points="908.568,-299.618 910.458,-289.193 902.479,-296.163 908.568,-299.618"/>
</g>
<!-- C51 -->
<g id="node36" class="node"><title>C51</title>
<g id="a_node36"><a xlink:title="C51 Algorithm. The core idea of Distributional Bellman is to ask the following
questions. If we can model the Distribution of the total future rewards, why
restrict ourselves to the expected value (i.e. Q function)? There are several
benefits to learning an approximate distribution rather than its approximate
expectation. [source: flyyufelix&#39;s blog]

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1129,-289C1129,-289 1099,-289 1099,-289 1093,-289 1087,-283 1087,-277 1087,-277 1087,-265 1087,-265 1087,-259 1093,-253 1099,-253 1099,-253 1129,-253 1129,-253 1135,-253 1141,-259 1141,-265 1141,-265 1141,-277 1141,-277 1141,-283 1135,-289 1129,-289"/>
<text text-anchor="middle" x="1114" y="-267.9" font-family="sans-serif" font-size="12.00">C51</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;C51 -->
<g id="edge30" class="edge"><title>DQN&#45;&gt;C51</title>
<path fill="none" stroke="black" d="M1007.27,-570.611C1049.28,-566.842 1125,-552.854 1125,-501 1125,-501 1125,-501 1125,-353 1125,-335.012 1122.3,-314.98 1119.58,-299.338"/>
<polygon fill="black" stroke="black" points="1122.98,-298.486 1117.72,-289.289 1116.1,-299.758 1122.98,-298.486"/>
</g>
<!-- IQN -->
<g id="node37" class="node"><title>IQN</title>
<g id="a_node37"><a xlink:title="Implicit Quantile Networks (IQN). From the abstract: In this work, we build on
recent advances in distributional reinforcement learning to give a generally
applicable, flexible, and state&#45;of&#45;the&#45;art distributional variant of DQN. We
achieve this by using quantile regression to approximate the full quantile
function for the state&#45;action return distribution. By reparameterizing a
distribution over the sample space, this yields an implicitly defined return
distribution and gives rise to a large class of risk&#45;sensitive policies. We
demonstrate improved performance on the 57 Atari 2600 games in the ALE, and
use our algorithm&#39;s implicitly defined distributions to study the effects of
risk&#45;sensitive policies in Atari games.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1149,-206C1149,-206 1119,-206 1119,-206 1113,-206 1107,-200 1107,-194 1107,-194 1107,-182 1107,-182 1107,-176 1113,-170 1119,-170 1119,-170 1149,-170 1149,-170 1155,-170 1161,-176 1161,-182 1161,-182 1161,-194 1161,-194 1161,-200 1155,-206 1149,-206"/>
<text text-anchor="middle" x="1134" y="-184.9" font-family="sans-serif" font-size="12.00">IQN</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;IQN -->
<g id="edge32" class="edge"><title>DQN&#45;&gt;IQN</title>
<path fill="none" stroke="black" d="M1007.34,-567.106C1057.69,-557.31 1160,-533.417 1160,-501 1160,-501 1160,-501 1160,-270 1160,-251.221 1153.59,-231.115 1147.13,-215.632"/>
<polygon fill="black" stroke="black" points="1150.2,-213.913 1142.95,-206.186 1143.8,-216.747 1150.2,-213.913"/>
</g>
<!-- R2D2 -->
<g id="node38" class="node"><title>R2D2</title>
<g id="a_node38"><a xlink:title="Recurrent Replay Distributed DQN (R2D2). (from the abstract) Building on the
recent successes of distributed training of RL agents, in this paper we
investigate the training of RNN&#45;based RL agents from distributed prioritized
experience replay. We study the effects of parameter lag resulting in
representational drift and recurrent state staleness and empirically derive an
improved training strategy. Using a single network architecture and fixed set
of hyper&#45;parameters, the resulting agent, Recurrent Replay Distributed DQN,
quadruples the previous state of the art on Atari&#45;57, and matches the state of
the art on DMLab&#45;30. It is the first agent to exceed human&#45;level performance
in 52 of the 57 Atari games.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1143,-133C1143,-133 1113,-133 1113,-133 1107,-133 1101,-127 1101,-121 1101,-121 1101,-109 1101,-109 1101,-103 1107,-97 1113,-97 1113,-97 1143,-97 1143,-97 1149,-97 1155,-103 1155,-109 1155,-109 1155,-121 1155,-121 1155,-127 1149,-133 1143,-133"/>
<text text-anchor="middle" x="1128" y="-111.9" font-family="sans-serif" font-size="12.00">R2D2</text>
</a>
</g>
</g>
<!-- DQN&#45;&gt;R2D2 -->
<g id="edge34" class="edge"><title>DQN&#45;&gt;R2D2</title>
<path fill="none" stroke="black" d="M1007.39,-568.143C1062.16,-559.659 1180,-537.2 1180,-501 1180,-501 1180,-501 1180,-187 1180,-169.16 1169.18,-152.823 1157.41,-140.33"/>
<polygon fill="black" stroke="black" points="1159.63,-137.599 1150.05,-133.068 1154.71,-142.583 1159.63,-137.599"/>
</g>
<!-- TD3 -->
<g id="node23" class="node"><title>TD3</title>
<g id="a_node23"><a xlink:title="Twin Delayed DDPG (TD3). TD3 addresses function approximation error in DDPG by
introducing twin Q&#45;value approximation network and less frequent updates

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M690,-206C690,-206 660,-206 660,-206 654,-206 648,-200 648,-194 648,-194 648,-182 648,-182 648,-176 654,-170 660,-170 660,-170 690,-170 690,-170 696,-170 702,-176 702,-182 702,-182 702,-194 702,-194 702,-200 696,-206 690,-206"/>
<text text-anchor="middle" x="675" y="-184.9" font-family="sans-serif" font-size="12.00">TD3</text>
</a>
</g>
</g>
<!-- DDPG&#45;&gt;TD3 -->
<g id="edge56" class="edge"><title>DDPG&#45;&gt;TD3</title>
<path fill="none" stroke="black" d="M372.355,-408.812C378.265,-366.194 393.633,-258.665 399,-253 464.776,-183.566 516.012,-228.376 609,-206 618.378,-203.744 628.475,-201.2 637.883,-198.782"/>
<polygon fill="black" stroke="black" points="638.951,-202.121 647.754,-196.226 637.196,-195.344 638.951,-202.121"/>
</g>
<!-- DDPG+HER -->
<g id="node25" class="node"><title>DDPG+HER</title>
<g id="a_node25"><a xlink:title="Hindsight Experience Replay (HER)

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M318,-289C318,-289 256,-289 256,-289 250,-289 244,-283 244,-277 244,-277 244,-265 244,-265 244,-259 250,-253 256,-253 256,-253 318,-253 318,-253 324,-253 330,-259 330,-265 330,-265 330,-277 330,-277 330,-283 324,-289 318,-289"/>
<text text-anchor="middle" x="287" y="-267.9" font-family="sans-serif" font-size="12.00">DDPG+HER</text>
</a>
</g>
</g>
<!-- DDPG&#45;&gt;DDPG+HER -->
<g id="edge52" class="edge"><title>DDPG&#45;&gt;DDPG+HER</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M356.089,-408.653C351.967,-403.16 347.597,-396.963 344,-391 325.592,-360.486 308.508,-323.114 297.936,-298.486"/>
<polygon fill="darkgray" stroke="darkgray" points="301.076,-296.925 293.951,-289.084 294.631,-299.657 301.076,-296.925"/>
</g>
<!-- APE&#45;X DDPG -->
<g id="node27" class="node"><title>APE&#45;X DDPG</title>
<g id="a_node27"><a xlink:title="DDPG with Distributed Prioritized Experience Replay

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M451.5,-206C451.5,-206 384.5,-206 384.5,-206 378.5,-206 372.5,-200 372.5,-194 372.5,-194 372.5,-182 372.5,-182 372.5,-176 378.5,-170 384.5,-170 384.5,-170 451.5,-170 451.5,-170 457.5,-170 463.5,-176 463.5,-182 463.5,-182 463.5,-194 463.5,-194 463.5,-200 457.5,-206 451.5,-206"/>
<text text-anchor="middle" x="418" y="-184.9" font-family="sans-serif" font-size="12.00">APE&#45;X DDPG</text>
</a>
</g>
</g>
<!-- DDPG&#45;&gt;APE&#45;X DDPG -->
<g id="edge55" class="edge"><title>DDPG&#45;&gt;APE&#45;X DDPG</title>
<path fill="none" stroke="black" d="M369.125,-408.888C368.002,-377.353 367.844,-308.24 384,-253 387.863,-239.792 394.631,-226.188 401.117,-214.937"/>
<polygon fill="black" stroke="black" points="404.277,-216.474 406.429,-206.1 398.278,-212.867 404.277,-216.474"/>
</g>
<!-- MADDPG -->
<g id="node50" class="node"><title>MADDPG</title>
<g id="a_node50"><a xlink:title="Multi&#45;agent DDPG (MADDPG) extends DDPG to an environment where multiple agents
are coordinating to complete tasks with only local information. In the
viewpoint of one agent, the environment is non&#45;stationary as policies of other
agents are quickly upgraded and remain unknown. MADDPG is an actor&#45;critic
model redesigned particularly for handling such a changing environment and
interactions between agents (from Lilian Weng&#39;s blog)

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M465.5,-289C465.5,-289 420.5,-289 420.5,-289 414.5,-289 408.5,-283 408.5,-277 408.5,-277 408.5,-265 408.5,-265 408.5,-259 414.5,-253 420.5,-253 420.5,-253 465.5,-253 465.5,-253 471.5,-253 477.5,-259 477.5,-265 477.5,-265 477.5,-277 477.5,-277 477.5,-283 471.5,-289 465.5,-289"/>
<text text-anchor="middle" x="443" y="-267.9" font-family="sans-serif" font-size="12.00">MADDPG</text>
</a>
</g>
</g>
<!-- DDPG&#45;&gt;MADDPG -->
<g id="edge53" class="edge"><title>DDPG&#45;&gt;MADDPG</title>
<path fill="none" stroke="black" d="M382.958,-408.652C386.768,-403.159 390.775,-396.962 394,-391 410.463,-360.564 424.941,-323.4 433.809,-298.789"/>
<polygon fill="black" stroke="black" points="437.214,-299.66 437.256,-289.066 430.616,-297.321 437.214,-299.66"/>
</g>
<!-- D4PG -->
<g id="node51" class="node"><title>D4PG</title>
<g id="a_node51"><a xlink:title="Distributed Distributional Deep Deterministic Policy Gradient (D4PG) adopts
the very successful distributional perspective on reinforcement learning and
adapts it to the continuous control setting. It combines this within a
distributed framework. It also combines this technique with a number of
additional, simple improvements such as the use of N&#45;step returns and
prioritized experience replay [from the paper&#39;s abstract]

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M270,-206C270,-206 240,-206 240,-206 234,-206 228,-200 228,-194 228,-194 228,-182 228,-182 228,-176 234,-170 240,-170 240,-170 270,-170 270,-170 276,-170 282,-176 282,-182 282,-182 282,-194 282,-194 282,-200 276,-206 270,-206"/>
<text text-anchor="middle" x="255" y="-184.9" font-family="sans-serif" font-size="12.00">D4PG</text>
</a>
</g>
</g>
<!-- DDPG&#45;&gt;D4PG -->
<g id="edge54" class="edge"><title>DDPG&#45;&gt;D4PG</title>
<path fill="none" stroke="black" d="M367.285,-408.932C360.503,-366.595 343.077,-259.672 339,-253 327.44,-234.083 308.059,-218.757 290.825,-207.851"/>
<polygon fill="black" stroke="black" points="292.43,-204.732 282.061,-202.555 288.81,-210.723 292.43,-204.732"/>
</g>
<!-- DDQN&#45;&gt;TD3 -->
<g id="edge18" class="edge"><title>DDQN&#45;&gt;TD3</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M979.921,-413.086C976.918,-411.692 973.899,-410.306 971,-409 877.515,-366.879 821.166,-399.541 759,-318 733.43,-284.46 770.681,-258.2 746,-224 737.669,-212.456 724.454,-204.523 711.672,-199.17"/>
<polygon fill="darkgray" stroke="darkgray" points="712.662,-195.803 702.07,-195.561 710.2,-202.355 712.662,-195.803"/>
<text text-anchor="middle" x="801.5" y="-310" font-family="sans-serif" font-size="10.00" fill="darkgray">double Q&#45;learning</text>
</g>
<!-- RAINBOW -->
<g id="node29" class="node"><title>RAINBOW</title>
<g id="a_node29"><a xlink:title="Combines six DQN extensions, namely Double Q&#45;Learning, prioritized replay,
dueling networks, multi&#45;step learning, distributional DQN, and noisy DQN into
single model to achieve state of the art performance

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1056.5,-289C1056.5,-289 1003.5,-289 1003.5,-289 997.5,-289 991.5,-283 991.5,-277 991.5,-277 991.5,-265 991.5,-265 991.5,-259 997.5,-253 1003.5,-253 1003.5,-253 1056.5,-253 1056.5,-253 1062.5,-253 1068.5,-259 1068.5,-265 1068.5,-265 1068.5,-277 1068.5,-277 1068.5,-283 1062.5,-289 1056.5,-289"/>
<text text-anchor="middle" x="1030" y="-267.9" font-family="sans-serif" font-size="12.00">RAINBOW</text>
</a>
</g>
</g>
<!-- DDQN&#45;&gt;RAINBOW -->
<g id="edge36" class="edge"><title>DDQN&#45;&gt;RAINBOW</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M1014.96,-408.904C1019.4,-398.483 1024.52,-384.757 1027,-372 1031.73,-347.722 1032.12,-319.51 1031.55,-299.254"/>
<polygon fill="darkgray" stroke="darkgray" points="1035.04,-299.03 1031.16,-289.172 1028.04,-299.299 1035.04,-299.03"/>
</g>
<!-- Duelling&#45;DQN -->
<g id="node39" class="node"><title>Duelling&#45;DQN</title>
<g id="a_node39"><a xlink:title="Duelling DQN represents two separate estimators: one for the state value
function and one for the state&#45;dependent action advantage function. The main
benefit of this factoring is to generalize learning across actions without
imposing any change to the underlying reinforcement learning algorithm.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1006,-372C1006,-372 936,-372 936,-372 930,-372 924,-366 924,-360 924,-360 924,-348 924,-348 924,-342 930,-336 936,-336 936,-336 1006,-336 1006,-336 1012,-336 1018,-342 1018,-348 1018,-348 1018,-360 1018,-360 1018,-366 1012,-372 1006,-372"/>
<text text-anchor="middle" x="971" y="-350.9" font-family="sans-serif" font-size="12.00">Duelling&#45;DQN</text>
</a>
</g>
</g>
<!-- DDQN&#45;&gt;Duelling&#45;DQN -->
<g id="edge35" class="edge"><title>DDQN&#45;&gt;Duelling&#45;DQN</title>
<path fill="none" stroke="black" d="M998.285,-408.813C994.085,-400.528 988.955,-390.41 984.281,-381.193"/>
<polygon fill="black" stroke="black" points="987.278,-379.365 979.634,-372.029 981.035,-382.531 987.278,-379.365"/>
</g>
<!-- DQN+HER&#45;&gt;DDPG+HER -->
<g id="edge19" class="edge"><title>DQN+HER&#45;&gt;DDPG+HER</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M776.873,-289.082C758.854,-297.892 735.839,-307.532 714,-312 705.292,-313.782 702.882,-312.346 694,-312 562.591,-306.885 529.585,-304.558 399,-289 376.218,-286.286 350.976,-282.314 330.232,-278.792"/>
<text text-anchor="middle" x="704" y="-310" font-family="sans-serif" font-size="10.00" fill="darkgray">HER</text>
</g>
<!-- APE&#45;X DQN&#45;&gt;APE&#45;X DDPG -->
<g id="edge20" class="edge"><title>APE&#45;X DQN&#45;&gt;APE&#45;X DDPG</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M826.95,-206C803.223,-215.179 772.75,-225.363 744.5,-230 691.487,-238.701 540.253,-211.972 463.651,-197.158"/>
<text text-anchor="middle" x="730" y="-227" font-family="sans-serif" font-size="10.00" fill="darkgray">APE&#45;X</text>
</g>
<!-- A3C&#45;&gt;ACER -->
<g id="edge61" class="edge"><title>A3C&#45;&gt;ACER</title>
<path fill="none" stroke="black" d="M663.477,-335.69C664.069,-326.95 665.334,-316.264 668,-307 668.806,-304.2 669.822,-301.351 670.956,-298.548"/>
<polygon fill="black" stroke="black" points="674.191,-299.889 675.125,-289.335 667.813,-297.003 674.191,-299.889"/>
</g>
<!-- A3C&#45;&gt;RAINBOW -->
<g id="edge21" class="edge"><title>A3C&#45;&gt;RAINBOW</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M690.07,-335.777C709.676,-323.447 733.642,-308.728 739,-307 834.475,-276.2 865.059,-305.573 964,-289 969.543,-288.072 975.301,-286.902 981.004,-285.614"/>
<polygon fill="darkgray" stroke="darkgray" points="982.237,-288.918 991.151,-283.191 980.612,-282.109 982.237,-288.918"/>
</g>
<!-- A2C -->
<g id="node53" class="node"><title>A2C</title>
<g id="a_node53"><a xlink:title="A2C is a synchronous, deterministic variant of Asynchronous Advantage Actor
Critic (A3C). It uses multiple workers to avoid the use of a replay buffer.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M610,-289C610,-289 580,-289 580,-289 574,-289 568,-283 568,-277 568,-277 568,-265 568,-265 568,-259 574,-253 580,-253 580,-253 610,-253 610,-253 616,-253 622,-259 622,-265 622,-265 622,-277 622,-277 622,-283 616,-289 610,-289"/>
<text text-anchor="middle" x="595" y="-267.9" font-family="sans-serif" font-size="12.00">A2C</text>
</a>
</g>
</g>
<!-- A3C&#45;&gt;A2C -->
<g id="edge60" class="edge"><title>A3C&#45;&gt;A2C</title>
<path fill="none" stroke="black" d="M648.579,-335.822C639.113,-324.546 626.575,-309.611 616.008,-297.024"/>
<polygon fill="black" stroke="black" points="618.511,-294.562 609.4,-289.153 613.149,-299.063 618.511,-294.562"/>
</g>
<!-- Q&#45;learning&#45;&gt;DQN -->
<g id="edge25" class="edge"><title>Q&#45;learning&#45;&gt;DQN</title>
<path fill="none" stroke="black" d="M980.622,-919.785C980.339,-905.407 980,-884.368 980,-866 980,-866 980,-866 980,-645 980,-630.651 980,-614.671 980,-601.511"/>
<polygon fill="black" stroke="black" points="983.5,-601.216 980,-591.216 976.5,-601.216 983.5,-601.216"/>
</g>
<!-- PER&#45;&gt;RAINBOW -->
<g id="edge37" class="edge"><title>PER&#45;&gt;RAINBOW</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M1073.57,-408.931C1065.11,-382.335 1048.7,-330.776 1038.61,-299.072"/>
<polygon fill="darkgray" stroke="darkgray" points="1041.82,-297.614 1035.46,-289.146 1035.15,-299.736 1041.82,-297.614"/>
</g>
<!-- QR&#45;DQN&#45;&gt;RAINBOW -->
<g id="edge39" class="edge"><title>QR&#45;DQN&#45;&gt;RAINBOW</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M955.75,-271C964.297,-271 972.844,-271 981.391,-271"/>
<polygon fill="darkgray" stroke="darkgray" points="981.425,-274.5 991.425,-271 981.425,-267.5 981.425,-274.5"/>
</g>
<!-- NGU -->
<g id="node40" class="node"><title>NGU</title>
<g id="a_node40"><a xlink:title="Never Give Up (NGU). (from the abstract) We propose a reinforcement learning
agent to solve hard exploration games by learning a range of directed
exploratory policies. We construct an episodic memory&#45;based intrinsic reward
using k&#45;nearest neighbors over the agent&#39;s recent experience to train the
directed exploratory policies, thereby encouraging the agent to repeatedly
revisit all states in its environment. A self&#45;supervised inverse dynamics
model is used to train the embeddings of the nearest neighbour lookup, biasing
the novelty signal towards what the agent can control. We employ the framework
of Universal Value Function Approximators (UVFA) to simultaneously learn many
directed exploration policies with the same neural network, with different
trade&#45;offs between exploration and exploitation. By using the same neural
network for different degrees of exploration/exploitation, transfer is
demonstrated from predominantly exploratory policies yielding effective
exploitative policies. The proposed method can be incorporated to run with
modern distributed RL agents that collect large amounts of experience from
many actors running in parallel on separate environment instances. Our method
doubles the performance of the base agent in all hard exploration in the
Atari&#45;57 suite while maintaining a very high score across the remaining games,
obtaining a median human normalised score of 1344.0%. Notably, the proposed
method is the first algorithm to achieve non&#45;zero rewards (with a mean score
of 8,400) in the game of Pitfall! without using demonstrations or hand&#45;crafted
features.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1067,-60C1067,-60 1037,-60 1037,-60 1031,-60 1025,-54 1025,-48 1025,-48 1025,-36 1025,-36 1025,-30 1031,-24 1037,-24 1037,-24 1067,-24 1067,-24 1073,-24 1079,-30 1079,-36 1079,-36 1079,-48 1079,-48 1079,-54 1073,-60 1067,-60"/>
<text text-anchor="middle" x="1052" y="-38.9" font-family="sans-serif" font-size="12.00">NGU</text>
</a>
</g>
</g>
<!-- R2D2&#45;&gt;NGU -->
<g id="edge40" class="edge"><title>R2D2&#45;&gt;NGU</title>
<path fill="none" stroke="black" d="M1109.6,-96.8129C1099.99,-87.8302 1088.07,-76.6933 1077.57,-66.8856"/>
<polygon fill="black" stroke="black" points="1079.92,-64.2979 1070.23,-60.0288 1075.15,-69.4131 1079.92,-64.2979"/>
</g>
<!-- Duelling&#45;DQN&#45;&gt;RAINBOW -->
<g id="edge38" class="edge"><title>Duelling&#45;DQN&#45;&gt;RAINBOW</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M983.512,-335.822C991.648,-324.653 1002.4,-309.895 1011.51,-297.384"/>
<polygon fill="darkgray" stroke="darkgray" points="1014.45,-299.297 1017.51,-289.153 1008.79,-295.176 1014.45,-299.297"/>
</g>
<!-- Agent57 -->
<g id="node41" class="node"><title>Agent57</title>
<g id="a_node41"><a xlink:title="(from the abstract) Atari games have been a long&#45;standing benchmark in the
reinforcement learning (RL) community for the past decade. This benchmark was
proposed to test general competency of RL algorithms. Previous work has
achieved good average performance by doing outstandingly well on many games of
the set, but very poorly in several of the most challenging games. We propose
Agent57, the first deep RL agent that outperforms the standard human benchmark
on all 57 Atari games. To achieve this result, we train a neural network which
parameterizes a family of policies ranging from very exploratory to purely
exploitative. We propose an adaptive mechanism to choose which policy to
prioritize throughout the training process. Additionally, we utilize a novel
parameterization of the architecture that allows for more consistent and
stable learning.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1169,-60C1169,-60 1127,-60 1127,-60 1121,-60 1115,-54 1115,-48 1115,-48 1115,-36 1115,-36 1115,-30 1121,-24 1127,-24 1127,-24 1169,-24 1169,-24 1175,-24 1181,-30 1181,-36 1181,-36 1181,-48 1181,-48 1181,-54 1175,-60 1169,-60"/>
<text text-anchor="middle" x="1148" y="-38.9" font-family="sans-serif" font-size="12.00">Agent57</text>
</a>
</g>
</g>
<!-- NGU&#45;&gt;Agent57 -->
<g id="edge41" class="edge"><title>NGU&#45;&gt;Agent57</title>
<path fill="none" stroke="black" d="M1079.38,-42C1087.76,-42 1096.15,-42 1104.53,-42"/>
<polygon fill="black" stroke="black" points="1104.76,-45.5001 1114.76,-42 1104.76,-38.5001 1104.76,-45.5001"/>
</g>
<!-- DPG&#45;&gt;DDPG -->
<g id="edge51" class="edge"><title>DPG&#45;&gt;DDPG</title>
<path fill="none" stroke="black" d="M376.821,-481.813C375.804,-473.789 374.569,-464.047 373.431,-455.069"/>
<polygon fill="black" stroke="black" points="376.888,-454.509 372.159,-445.029 369.944,-455.39 376.888,-454.509"/>
</g>
<!-- TRPO&#45;&gt;ACER -->
<g id="edge58" class="edge"><title>TRPO&#45;&gt;ACER</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M510.081,-408.697C516.667,-388.897 529.876,-356.563 551,-336 565.733,-321.658 614.127,-300.343 648.516,-286.311"/>
<polygon fill="darkgray" stroke="darkgray" points="649.844,-289.55 657.805,-282.559 647.222,-283.06 649.844,-289.55"/>
<text text-anchor="middle" x="589" y="-351.5" font-family="sans-serif" font-size="10.00" fill="darkgray">TRPO technique</text>
</g>
<!-- TRPO&#45;&gt;GAE -->
<g id="edge57" class="edge"><title>TRPO&#45;&gt;GAE</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M532.313,-427C541.529,-427 550.745,-427 559.962,-427"/>
<polygon fill="darkgray" stroke="darkgray" points="559.963,-430.5 569.963,-427 559.963,-423.5 559.963,-430.5"/>
</g>
<!-- PPO -->
<g id="node52" class="node"><title>PPO</title>
<g id="a_node52"><a xlink:title="Proximal Policy Optimization (PPO) is similar to TRPO but uses simpler
mechanism while retaining similar performance.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M538,-289C538,-289 508,-289 508,-289 502,-289 496,-283 496,-277 496,-277 496,-265 496,-265 496,-259 502,-253 508,-253 508,-253 538,-253 538,-253 544,-253 550,-259 550,-265 550,-265 550,-277 550,-277 550,-283 544,-289 538,-289"/>
<text text-anchor="middle" x="523" y="-267.9" font-family="sans-serif" font-size="12.00">PPO</text>
</a>
</g>
</g>
<!-- TRPO&#45;&gt;PPO -->
<g id="edge59" class="edge"><title>TRPO&#45;&gt;PPO</title>
<path fill="none" stroke="darkgray" stroke-dasharray="5,2" d="M506.995,-408.931C510.09,-382.451 516.078,-331.224 519.787,-299.486"/>
<polygon fill="darkgray" stroke="darkgray" points="523.311,-299.484 520.996,-289.146 516.359,-298.672 523.311,-299.484"/>
</g>
<!-- PPO&#45;&gt;SAC -->
<!-- A2C&#45;&gt;ACER -->
<!-- A2C&#45;&gt;ACKTR -->
<!-- A2C&#45;&gt;SVPG -->
<!-- A2C&#45;&gt;IMPALA -->
<!-- PILCO -->
<g id="node55" class="node"><title>PILCO</title>
<g id="a_node55"><a xlink:title="(from the abstract) In this paper, we introduce PILCO, a practical, data&#45;
efficient model&#45;based policy search method. PILCO reduces model bias, one of
the key problems of model&#45;based reinforcement learning, in a principled way.
By learning a probabilistic dynamics model and explicitly incorporating model
uncertainty into long&#45;term planning, PILCO can cope with very little data and
facilitates learning froms cratch in only a few trials. Policy evaluationis
performed in closed form using state&#45;of&#45;the&#45;art approximate inference.
Furthermore, policy gradients are computed analytically for policy
improvement. We report unprecedented learning efficiency on challenging and
high&#45;dimensional control tasks.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1509,-664C1509,-664 1479,-664 1479,-664 1473,-664 1467,-658 1467,-652 1467,-652 1467,-640 1467,-640 1467,-634 1473,-628 1479,-628 1479,-628 1509,-628 1509,-628 1515,-628 1521,-634 1521,-640 1521,-640 1521,-652 1521,-652 1521,-658 1515,-664 1509,-664"/>
<text text-anchor="middle" x="1494" y="-642.9" font-family="sans-serif" font-size="12.00">PILCO</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;PILCO -->
<g id="edge67" class="edge"><title>Model Based&#45;&gt;PILCO</title>
<path fill="none" stroke="black" d="M1549.67,-1115.89C1544.32,-1110.6 1538.95,-1104.45 1535,-1098 1513.99,-1063.73 1507,-1052.2 1507,-1012 1507,-1012 1507,-1012 1507,-718 1507,-703.267 1504.26,-687.091 1501.29,-673.905"/>
<polygon fill="black" stroke="black" points="1504.66,-672.956 1498.91,-664.056 1497.86,-674.599 1504.66,-672.956"/>
</g>
<!-- I2A -->
<g id="node56" class="node"><title>I2A</title>
<g id="a_node56"><a xlink:title="(from the abstract) We introduce Imagination&#45;Augmented Agents (I2As), a novel
architecture for deep reinforcement learning combining model&#45;free and model&#45;
based aspects. In contrast to most existing model&#45;based reinforcement learning
and planning methods, which prescribe how a model should be used to arrive at
a policy, I2As learn to interpret predictions from a learned environment model
to construct implicit plans in arbitrary ways, by using the predictions as
additional context in deep policy networks. I2As show improved data
efficiency, performance, and robustness to model misspecification compared to
several baselines.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1779,-289C1779,-289 1749,-289 1749,-289 1743,-289 1737,-283 1737,-277 1737,-277 1737,-265 1737,-265 1737,-259 1743,-253 1749,-253 1749,-253 1779,-253 1779,-253 1785,-253 1791,-259 1791,-265 1791,-265 1791,-277 1791,-277 1791,-283 1785,-289 1779,-289"/>
<text text-anchor="middle" x="1764" y="-267.9" font-family="sans-serif" font-size="12.00">I2A</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;I2A -->
<g id="edge68" class="edge"><title>Model Based&#45;&gt;I2A</title>
<path fill="none" stroke="black" d="M1615.37,-1127.61C1654.61,-1122.07 1707.41,-1112.25 1723,-1098 1752.67,-1070.88 1751,-1052.2 1751,-1012 1751,-1012 1751,-1012 1751,-353 1751,-334.948 1754.19,-314.915 1757.4,-299.288"/>
<polygon fill="black" stroke="black" points="1760.88,-299.769 1759.6,-289.252 1754.04,-298.274 1760.88,-299.769"/>
</g>
<!-- MBMF -->
<g id="node57" class="node"><title>MBMF</title>
<g id="a_node57"><a xlink:title="(from the abstract) Neural Network Dynamics for Model&#45;Based Deep Reinforcement
Learning with Model&#45;Free Fine&#45;Tuning. We demonstrate that medium&#45;sized neural
network models can in fact be combined with model predictive control (MPC) to
achieve excellent sample complexity in a model&#45;based reinforcement learning
algorithm, producing stable and plausible gaits to accomplish various complex
locomotion tasks. We also propose using deep neural network dynamics models to
initialize a model&#45;free learner, in order to combine the sample efficiency of
model&#45;based approaches with the high task&#45;specific performance of model&#45;free
methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure
model&#45;based approach trained on just random action data can follow arbitrary
trajectories with excellent sample efficiency, and that our hybrid algorithm
can accelerate model&#45;free learning on high&#45;speed benchmark tasks, achieving
sample efficiency gains of 3&#45;5x on swimmer, cheetah, hopper, and ant agents.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1439,-289C1439,-289 1409,-289 1409,-289 1403,-289 1397,-283 1397,-277 1397,-277 1397,-265 1397,-265 1397,-259 1403,-253 1409,-253 1409,-253 1439,-253 1439,-253 1445,-253 1451,-259 1451,-265 1451,-265 1451,-277 1451,-277 1451,-283 1445,-289 1439,-289"/>
<text text-anchor="middle" x="1424" y="-267.9" font-family="sans-serif" font-size="12.00">MBMF</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;MBMF -->
<g id="edge69" class="edge"><title>Model Based&#45;&gt;MBMF</title>
<path fill="none" stroke="black" d="M1562.89,-1115.7C1554.02,-1092.51 1540,-1049.88 1540,-1012 1540,-1012 1540,-1012 1540,-353 1540,-343.107 1494.06,-313.587 1460,-293.061"/>
<polygon fill="black" stroke="black" points="1461.58,-289.924 1451.2,-287.794 1457.98,-295.931 1461.58,-289.924"/>
</g>
<!-- Exit -->
<g id="node58" class="node"><title>Exit</title>
<g id="a_node58"><a xlink:title="Expert Iteration (ExIt) is a novel reinforcement learning algorithm which
decomposes the problem into separate planning and generalisation tasks.
Planning new policies is performed by tree search, while a deep neural network
generalises those plans. Subsequently, tree search is improved by using the
neural network policy to guide search, increasing the strength of new plans.
In contrast, standard deep Reinforcement Learning algorithms rely on a neural
network not only to generalise plans, but to discover them too. We show that
ExIt outperforms REINFORCE for training a neural network to play the board
game Hex, and our final tree search agent, trained tabula rasa, defeats MoHex
1.0, the most recent Olympiad Champion player to be publicly released. (from
the abstract)

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1511,-289C1511,-289 1481,-289 1481,-289 1475,-289 1469,-283 1469,-277 1469,-277 1469,-265 1469,-265 1469,-259 1475,-253 1481,-253 1481,-253 1511,-253 1511,-253 1517,-253 1523,-259 1523,-265 1523,-265 1523,-277 1523,-277 1523,-283 1517,-289 1511,-289"/>
<text text-anchor="middle" x="1496" y="-267.9" font-family="sans-serif" font-size="12.00">Exit</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;Exit -->
<g id="edge70" class="edge"><title>Model Based&#45;&gt;Exit</title>
<path fill="none" stroke="black" d="M1567.68,-1115.8C1564.73,-1092.38 1560,-1049.1 1560,-1012 1560,-1012 1560,-1012 1560,-353 1560,-331.496 1557.72,-324.339 1545,-307 1541.19,-301.8 1536.36,-297.074 1531.24,-292.895"/>
<polygon fill="black" stroke="black" points="1533.16,-289.958 1523.05,-286.789 1528.98,-295.571 1533.16,-289.958"/>
</g>
<!-- AlphaZero -->
<g id="node59" class="node"><title>AlphaZero</title>
<g id="a_node59"><a xlink:title="AlphaZero generalises tabula rasa reinforcement learning from games of self&#45;
play approach. Starting from random play, and given no domain knowledge except
the game rules, AlphaZero achieved within 24 hours a superhuman level of play
in the games of chess and shogi (Japanese chess) as well as Go, and
convincingly defeated a world&#45;champion program in each case. (from the
abstract)

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1606.5,-289C1606.5,-289 1553.5,-289 1553.5,-289 1547.5,-289 1541.5,-283 1541.5,-277 1541.5,-277 1541.5,-265 1541.5,-265 1541.5,-259 1547.5,-253 1553.5,-253 1553.5,-253 1606.5,-253 1606.5,-253 1612.5,-253 1618.5,-259 1618.5,-265 1618.5,-265 1618.5,-277 1618.5,-277 1618.5,-283 1612.5,-289 1606.5,-289"/>
<text text-anchor="middle" x="1580" y="-267.9" font-family="sans-serif" font-size="12.00">AlphaZero</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;AlphaZero -->
<g id="edge71" class="edge"><title>Model Based&#45;&gt;AlphaZero</title>
<path fill="none" stroke="black" d="M1572.32,-1115.8C1575.27,-1092.38 1580,-1049.1 1580,-1012 1580,-1012 1580,-1012 1580,-353 1580,-335.062 1580,-314.888 1580,-299.162"/>
<polygon fill="black" stroke="black" points="1583.5,-299.066 1580,-289.066 1576.5,-299.066 1583.5,-299.066"/>
</g>
<!-- MVE -->
<g id="node60" class="node"><title>MVE</title>
<g id="a_node60"><a xlink:title="(from the abstract) Recent model&#45;free reinforcement learning algorithms have
proposed incorporating learned dynamics models as a source of additional data
with the intention of reducing sample complexity. Such methods hold the
promise of incorporating imagined data coupled with a notion of model
uncertainty to accelerate the learning of continuous control tasks.
Unfortunately, they rely on heuristics that limit usage of the dynamics model.
We present model&#45;based value expansion, which controls for uncertainty in the
model by only allowing imagination to fixed depth. By enabling wider use of
learned dynamics models within a model&#45;free reinforcement learning algorithm,
we improve value estimation, which, in turn, reduces the sample complexity of
learning.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1439,-206C1439,-206 1409,-206 1409,-206 1403,-206 1397,-200 1397,-194 1397,-194 1397,-182 1397,-182 1397,-176 1403,-170 1409,-170 1409,-170 1439,-170 1439,-170 1445,-170 1451,-176 1451,-182 1451,-182 1451,-194 1451,-194 1451,-200 1445,-206 1439,-206"/>
<text text-anchor="middle" x="1424" y="-184.9" font-family="sans-serif" font-size="12.00">MVE</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;MVE -->
<g id="edge72" class="edge"><title>Model Based&#45;&gt;MVE</title>
<path fill="none" stroke="black" d="M1587.16,-1115.85C1607.3,-1093.82 1638,-1053.45 1638,-1012 1638,-1012 1638,-1012 1638,-270 1638,-202.085 1711.4,-298.266 1460.76,-205.983"/>
<polygon fill="black" stroke="black" points="1461.91,-202.677 1451.31,-202.475 1459.47,-209.239 1461.91,-202.677"/>
</g>
<!-- STEVE -->
<g id="node61" class="node"><title>STEVE</title>
<g id="a_node61"><a xlink:title="(from the abstract) Integrating model&#45;free and model&#45;based approaches in
reinforcement learning has the potential to achieve the high performance of
model&#45;free algorithms with low sample complexity. However, this is difficult
because an imperfect dynamics model can degrade the performance of the
learning algorithm, and in sufficiently complex environments, the dynamics
model will almost always be imperfect. As a result, a key challenge is to
combine model&#45;based approaches with model&#45;free learning in such a way that
errors in the model do not degrade performance. We propose stochastic ensemble
value expansion (STEVE), a novel model&#45;based technique that addresses this
issue. By dynamically interpolating between model rollouts of various horizon
lengths for each individual example, STEVE ensures that the model is only
utilized when doing so does not introduce significant errors. Our approach
outperforms model&#45;free baselines on challenging continuous control benchmarks
with an order&#45;of&#45;magnitude increase in sample efficiency, and in contrast to
previous model&#45;based approaches, performance does not degrade in complex
environments.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1511,-206C1511,-206 1481,-206 1481,-206 1475,-206 1469,-200 1469,-194 1469,-194 1469,-182 1469,-182 1469,-176 1475,-170 1481,-170 1481,-170 1511,-170 1511,-170 1517,-170 1523,-176 1523,-182 1523,-182 1523,-194 1523,-194 1523,-200 1517,-206 1511,-206"/>
<text text-anchor="middle" x="1496" y="-184.9" font-family="sans-serif" font-size="12.00">STEVE</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;STEVE -->
<g id="edge73" class="edge"><title>Model Based&#45;&gt;STEVE</title>
<path fill="none" stroke="black" d="M1615.31,-1119.39C1625.81,-1114.25 1635.96,-1107.3 1643,-1098 1666.43,-1067.07 1658,-1050.8 1658,-1012 1658,-1012 1658,-1012 1658,-270 1658,-248.496 1659.33,-237.987 1643,-224 1607.34,-193.46 1582.84,-217.492 1533.13,-205.585"/>
<polygon fill="black" stroke="black" points="1533.77,-202.13 1523.2,-202.811 1531.89,-208.872 1533.77,-202.13"/>
</g>
<!-- ME&#45;TRPO -->
<g id="node62" class="node"><title>ME&#45;TRPO</title>
<g id="a_node62"><a xlink:title="(from the abstract) Model&#45;free reinforcement learning (RL) methods are
succeeding in a growing number of tasks, aided by recent advances in deep
learning. However, they tend to suffer from high sample complexity, which
hinders their use in real&#45;world domains. Alternatively, model&#45;based
reinforcement learning promises to reduce sample complexity, but tends to
require careful tuning and to date have succeeded mainly in restrictive
domains where simple models are sufficient for learning. In this paper, we
analyze the behavior of vanilla model&#45;based reinforcement learning methods
when deep neural networks are used to learn both the model and the policy, and
show that the learned policy tends to exploit regions where insufficient data
is available for the model to be learned, causing instability in training. To
overcome this issue, we propose to use an ensemble of models to maintain the
model uncertainty and regularize the learning process. We further show that
the use of likelihood ratio derivatives yields much more stable learning than
backpropagation through time. Altogether, our approach Model&#45;Ensemble Trust&#45;
Region Policy Optimization (ME&#45;TRPO) significantly reduces the sample
complexity compared to model&#45;free deep RL methods on challenging continuous
control benchmark tasks.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1684.5,-206C1684.5,-206 1637.5,-206 1637.5,-206 1631.5,-206 1625.5,-200 1625.5,-194 1625.5,-194 1625.5,-182 1625.5,-182 1625.5,-176 1631.5,-170 1637.5,-170 1637.5,-170 1684.5,-170 1684.5,-170 1690.5,-170 1696.5,-176 1696.5,-182 1696.5,-182 1696.5,-194 1696.5,-194 1696.5,-200 1690.5,-206 1684.5,-206"/>
<text text-anchor="middle" x="1661" y="-184.9" font-family="sans-serif" font-size="12.00">ME&#45;TRPO</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;ME&#45;TRPO -->
<g id="edge74" class="edge"><title>Model Based&#45;&gt;ME&#45;TRPO</title>
<path fill="none" stroke="black" d="M1615.37,-1122.26C1632.56,-1115.69 1650.31,-1105.3 1660,-1089 1674.92,-1063.9 1698,-858.196 1698,-829 1698,-829 1698,-829 1698,-270 1698,-250.411 1688.94,-230.371 1679.78,-215.125"/>
<polygon fill="black" stroke="black" points="1682.48,-212.846 1674.15,-206.298 1676.58,-216.611 1682.48,-212.846"/>
</g>
<!-- MB&#45;MPO -->
<g id="node63" class="node"><title>MB&#45;MPO</title>
<g id="a_node63"><a xlink:title="(from the abstract) Model&#45;based reinforcement learning approaches carry the
promise of being data efficient. However, due to challenges in learning
dynamics models that sufficiently match the real&#45;world dynamics, they struggle
to achieve the same asymptotic performance as model&#45;free methods. We propose
Model&#45;Based Meta&#45;Policy&#45;Optimization (MB&#45;MPO), an approach that foregoes the
strong reliance on accurate learned dynamics models. Using an ensemble of
learned dynamic models, MB&#45;MPO meta&#45;learns a policy that can quickly adapt to
any model in the ensemble with one policy gradient step. This steers the meta&#45;
policy towards internalizing consistent dynamics predictions among the
ensemble while shifting the burden of behaving optimally w.r.t. the model
discrepancies towards the adaptation step. Our experiments show that MB&#45;MPO is
more robust to model imperfections than previous model&#45;based approaches.
Finally, we demonstrate that our approach is able to match the asymptotic
performance of model&#45;free methods while requiring significantly less
experience.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1595,-206C1595,-206 1553,-206 1553,-206 1547,-206 1541,-200 1541,-194 1541,-194 1541,-182 1541,-182 1541,-176 1547,-170 1553,-170 1553,-170 1595,-170 1595,-170 1601,-170 1607,-176 1607,-182 1607,-182 1607,-194 1607,-194 1607,-200 1601,-206 1595,-206"/>
<text text-anchor="middle" x="1574" y="-184.9" font-family="sans-serif" font-size="12.00">MB&#45;MPO</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;MB&#45;MPO -->
<g id="edge75" class="edge"><title>Model Based&#45;&gt;MB&#45;MPO</title>
<path fill="none" stroke="black" d="M1615.09,-1131.33C1639.37,-1127.84 1667.46,-1119.08 1683,-1098 1690.32,-1088.07 1678,-837.864 1678,-829 1678,-829 1678,-829 1678,-270 1678,-248.496 1677.82,-239.585 1663,-224 1649.39,-209.688 1639.4,-213.005 1617.04,-206.004"/>
<polygon fill="black" stroke="black" points="1618.04,-202.645 1607.45,-202.605 1615.71,-209.243 1618.04,-202.645"/>
</g>
<!-- World Models -->
<g id="node64" class="node"><title>World Models</title>
<g id="a_node64"><a xlink:title="(from the abstract) A generative recurrent neural network is quickly trained
in an unsupervised manner to model popular reinforcement learning environments
through compressed spatio&#45;temporal representations. The world model&#39;s
extracted features are fed into compact and simple policies trained by
evolution, achieving state of the art results in various environments. We also
train our agent entirely inside of an environment generated by its own
internal world model, and transfer this policy back into the actual
environment.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1799,-206C1799,-206 1727,-206 1727,-206 1721,-206 1715,-200 1715,-194 1715,-194 1715,-182 1715,-182 1715,-176 1721,-170 1727,-170 1727,-170 1799,-170 1799,-170 1805,-170 1811,-176 1811,-182 1811,-182 1811,-194 1811,-194 1811,-200 1805,-206 1799,-206"/>
<text text-anchor="middle" x="1763" y="-184.9" font-family="sans-serif" font-size="12.00">World Models</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;World Models -->
<g id="edge76" class="edge"><title>Model Based&#45;&gt;World Models</title>
<path fill="none" stroke="black" d="M1615.18,-1129.74C1639.85,-1125.72 1669.29,-1116.96 1689,-1098 1718.08,-1070.04 1718,-1052.34 1718,-1012 1718,-1012 1718,-1012 1718,-270 1718,-249.56 1729.09,-229.455 1740.25,-214.389"/>
<polygon fill="black" stroke="black" points="1743.32,-216.158 1746.75,-206.136 1737.82,-211.826 1743.32,-216.158"/>
</g>
<!-- PETS -->
<g id="node65" class="node"><title>PETS</title>
<g id="a_node65"><a xlink:title="(from the abstract) Model&#45;based reinforcement learning (RL) algorithms can
attain excellent sample efficiency, but often lag behind the best model&#45;free
algorithms in terms of asymptotic performance. This is especially true with
high&#45;capacity parametric function approximators, such as deep networks. In
this paper, we study how to bridge this gap, by employing uncertainty&#45;aware
dynamics models. We propose a new algorithm called probabilistic ensembles
with trajectory sampling (PETS) that combines uncertainty&#45;aware deep network
dynamics models with sampling&#45;based uncertainty propagation. Our comparison to
state&#45;of&#45;the&#45;art model&#45;based and model&#45;free deep RL algorithms shows that our
approach matches the asymptotic performance of model&#45;free algorithms on
several challenging benchmark tasks, while requiring significantly fewer
samples (e.g., 8 and 125 times fewer samples than Soft Actor Critic and
Proximal Policy Optimization respectively on the half&#45;cheetah task).

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1255,-206C1255,-206 1225,-206 1225,-206 1219,-206 1213,-200 1213,-194 1213,-194 1213,-182 1213,-182 1213,-176 1219,-170 1225,-170 1225,-170 1255,-170 1255,-170 1261,-170 1267,-176 1267,-182 1267,-182 1267,-194 1267,-194 1267,-200 1261,-206 1255,-206"/>
<text text-anchor="middle" x="1240" y="-184.9" font-family="sans-serif" font-size="12.00">PETS</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;PETS -->
<g id="edge77" class="edge"><title>Model Based&#45;&gt;PETS</title>
<path fill="none" stroke="black" d="M1524.98,-1130.31C1461.87,-1125.68 1351.4,-1114.15 1321,-1089 1290.97,-1064.15 1279,-1050.98 1279,-1012 1279,-1012 1279,-1012 1279,-270 1279,-250.038 1269.27,-229.804 1259.52,-214.553"/>
<polygon fill="black" stroke="black" points="1262.36,-212.507 1253.86,-206.189 1256.57,-216.431 1262.36,-212.507"/>
</g>
<!-- PlaNet -->
<g id="node66" class="node"><title>PlaNet</title>
<g id="a_node66"><a xlink:title="(from the abstract) We propose the Deep Planning Network (PlaNet), a purely
model&#45;based agent that learns the environment dynamics from images and chooses
actions through fast online planning in latent space. To achieve high
performance, the dynamics model must accurately predict the rewards ahead for
multiple time steps. We approach this using a latent dynamics model with both
deterministic and stochastic transition components. Moreover, we propose a
multi&#45;step variational inference objective that we name latent overshooting.
Using only pixel observations, our agent solves continuous control tasks with
contact dynamics, partial observability, and sparse rewards, which exceed the
difficulty of tasks that were previously solved by planning with learned
models. PlaNet uses substantially fewer episodes and reaches final performance
close to and sometimes higher than strong model&#45;free algorithms.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1327,-206C1327,-206 1297,-206 1297,-206 1291,-206 1285,-200 1285,-194 1285,-194 1285,-182 1285,-182 1285,-176 1291,-170 1297,-170 1297,-170 1327,-170 1327,-170 1333,-170 1339,-176 1339,-182 1339,-182 1339,-194 1339,-194 1339,-200 1333,-206 1327,-206"/>
<text text-anchor="middle" x="1312" y="-184.9" font-family="sans-serif" font-size="12.00">PlaNet</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;PlaNet -->
<g id="edge78" class="edge"><title>Model Based&#45;&gt;PlaNet</title>
<path fill="none" stroke="black" d="M1524.75,-1131.15C1468.88,-1127.95 1378.15,-1119.53 1353,-1098 1322.46,-1071.86 1325,-1052.2 1325,-1012 1325,-1012 1325,-1012 1325,-270 1325,-251.948 1321.81,-231.915 1318.6,-216.288"/>
<polygon fill="black" stroke="black" points="1321.96,-215.274 1316.4,-206.252 1315.12,-216.769 1321.96,-215.274"/>
</g>
<!-- SimPLe -->
<g id="node67" class="node"><title>SimPLe</title>
<g id="a_node67"><a xlink:title="Simulated Policy Learning (SimPLe) is a complete model&#45;based deep RL algorithm
based on video prediction models and present a comparison of several model
architectures, including a novel architecture that yields the best results in
our setting. Our experiments evaluate SimPLe on a range of Atari games in low
data regime of 100k interactions between the agent and the environment, which
corresponds to two hours of real&#45;time play. In most games SimPLe outperforms
state&#45;of&#45;the&#45;art model&#45;free algorithms, in some games by over an order of
magnitude. (from the abstract)

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1345.5,-133C1345.5,-133 1310.5,-133 1310.5,-133 1304.5,-133 1298.5,-127 1298.5,-121 1298.5,-121 1298.5,-109 1298.5,-109 1298.5,-103 1304.5,-97 1310.5,-97 1310.5,-97 1345.5,-97 1345.5,-97 1351.5,-97 1357.5,-103 1357.5,-109 1357.5,-109 1357.5,-121 1357.5,-121 1357.5,-127 1351.5,-133 1345.5,-133"/>
<text text-anchor="middle" x="1328" y="-111.9" font-family="sans-serif" font-size="12.00">SimPLe</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;SimPLe -->
<g id="edge79" class="edge"><title>Model Based&#45;&gt;SimPLe</title>
<path fill="none" stroke="black" d="M1524.64,-1128.49C1473.47,-1122.45 1394.67,-1110 1374,-1089 1349.48,-1064.09 1358,-1046.95 1358,-1012 1358,-1012 1358,-1012 1358,-187 1358,-171.279 1351.68,-155.034 1344.84,-142.057"/>
<polygon fill="black" stroke="black" points="1347.83,-140.22 1339.86,-133.24 1341.73,-143.667 1347.83,-140.22"/>
</g>
<!-- MuZero -->
<g id="node68" class="node"><title>MuZero</title>
<g id="a_node68"><a xlink:title="(from the abstract) Constructing agents with planning capabilities has long
been one of the main challenges in the pursuit of artificial intelligence.
Tree&#45;based planning methods have enjoyed huge success in challenging domains,
such as chess and Go, where a perfect simulator is available. However, in
real&#45;world problems the dynamics governing the environment are often complex
and unknown. In this work we present the MuZero algorithm which, by combining
a tree&#45;based search with a learned model, achieves superhuman performance in a
range of challenging and visually complex domains, without any knowledge of
their underlying dynamics. MuZero learns a model that, when applied
iteratively, predicts the quantities most directly relevant to planning: the
reward, the action&#45;selection policy, and the value function. When evaluated on
57 different Atari games &#45; the canonical video game environment for testing AI
techniques, in which model&#45;based planning approaches have historically
struggled &#45; our new algorithm achieved a new state of the art. When evaluated
on Go, chess and shogi, without any knowledge of the game rules, MuZero
matched the superhuman performance of the AlphaZero algorithm that was
supplied with the game rules.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1426,-133C1426,-133 1388,-133 1388,-133 1382,-133 1376,-127 1376,-121 1376,-121 1376,-109 1376,-109 1376,-103 1382,-97 1388,-97 1388,-97 1426,-97 1426,-97 1432,-97 1438,-103 1438,-109 1438,-109 1438,-121 1438,-121 1438,-127 1432,-133 1426,-133"/>
<text text-anchor="middle" x="1407" y="-111.9" font-family="sans-serif" font-size="12.00">MuZero</text>
</a>
</g>
</g>
<!-- Model Based&#45;&gt;MuZero -->
<g id="edge80" class="edge"><title>Model Based&#45;&gt;MuZero</title>
<path fill="none" stroke="black" d="M1524.98,-1125.14C1468.24,-1112.35 1378,-1081.15 1378,-1012 1378,-1012 1378,-1012 1378,-187 1378,-171.25 1384.19,-154.896 1390.85,-141.859"/>
<polygon fill="black" stroke="black" points="1393.97,-143.46 1395.7,-133.009 1387.83,-140.095 1393.97,-143.46"/>
</g>
<!-- DMRL -->
<g id="node70" class="node"><title>DMRL</title>
<g id="a_node70"><a xlink:title="Deep Meta RL. (from the abstract) In recent years deep reinforcement learning
(RL) systems have attained superhuman performance in a number of challenging
task domains. However, a major limitation of such applications is their demand
for massive amounts of training data. A critical present objective is thus to
develop deep RL methods that can adapt rapidly to new tasks. In the present
work we introduce a novel approach to this challenge, which we refer to as
deep meta&#45;reinforcement learning. Previous work has shown that recurrent
networks can support meta&#45;learning in a fully supervised context. We extend
this approach to the RL setting. What emerges is a system that is trained
using one RL algorithm, but whose recurrent dynamics implement a second, quite
separate RL procedure. This second, learned RL algorithm can differ from the
original one in arbitrary ways. Importantly, because it is learned, it is
configured to exploit structure in the training domain. We unpack these points
in a series of seven proof&#45;of&#45;concept experiments, each of which examines a
key aspect of deep meta&#45;RL. We consider prospects for extending and scaling up
the approach, and also point out some potentially important implications for
neuroscience.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1917,-372C1917,-372 1887,-372 1887,-372 1881,-372 1875,-366 1875,-360 1875,-360 1875,-348 1875,-348 1875,-342 1881,-336 1887,-336 1887,-336 1917,-336 1917,-336 1923,-336 1929,-342 1929,-348 1929,-348 1929,-360 1929,-360 1929,-366 1923,-372 1917,-372"/>
<text text-anchor="middle" x="1902" y="-350.9" font-family="sans-serif" font-size="12.00">DMRL</text>
</a>
</g>
</g>
<!-- Meta&#45;RL&#45;&gt;DMRL -->
<g id="edge81" class="edge"><title>Meta&#45;RL&#45;&gt;DMRL</title>
<path fill="none" stroke="black" d="M1879.45,-1115.91C1879.29,-1110.22 1879.12,-1103.83 1879,-1098 1876.63,-978.461 1876,-948.563 1876,-829 1876,-829 1876,-829 1876,-426 1876,-410.623 1881.44,-394.453 1887.35,-381.434"/>
<polygon fill="black" stroke="black" points="1890.64,-382.681 1891.87,-372.159 1884.34,-379.613 1890.64,-382.681"/>
</g>
<!-- RL^2 -->
<g id="node71" class="node"><title>RL^2</title>
<g id="a_node71"><a xlink:title="(from the abstract) Deep reinforcement learning (deep RL) has been successful
in learning sophisticated behaviors automatically; however, the learning
process requires a huge number of trials. In contrast, animals can learn new
tasks in just a few trials, benefiting from their prior knowledge about the
world. This paper seeks to bridge this gap. Rather than designing a &quot;fast&quot;
reinforcement learning algorithm, we propose to represent it as a recurrent
neural network (RNN) and learn it from data. In our proposed method, RL2, the
algorithm is encoded in the weights of the RNN, which are learned slowly
through a general&#45;purpose (&quot;slow&quot;) RL algorithm. The RNN receives all
information a typical RL algorithm would receive, including observations,
actions, rewards, and termination flags; and it retains its state across
episodes in a given Markov Decision Process (MDP). The activations of the RNN
store the state of the &quot;fast&quot; RL algorithm on the current (previously unseen)
MDP. We evaluate RL2 experimentally on both small&#45;scale and large&#45;scale
problems. On the small&#45;scale side, we train it to solve randomly generated
multi&#45;arm bandit problems and finite MDPs. After RL2 is trained, its
performance on new MDPs is close to human&#45;designed algorithms with optimality
guarantees. On the large&#45;scale side, we test RL2 on a vision&#45;based navigation
task and show that it scales up to high&#45;dimensional problems.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1989,-372C1989,-372 1959,-372 1959,-372 1953,-372 1947,-366 1947,-360 1947,-360 1947,-348 1947,-348 1947,-342 1953,-336 1959,-336 1959,-336 1989,-336 1989,-336 1995,-336 2001,-342 2001,-348 2001,-348 2001,-360 2001,-360 2001,-366 1995,-372 1989,-372"/>
<text text-anchor="middle" x="1974" y="-350.9" font-family="sans-serif" font-size="12.00">RL^2</text>
</a>
</g>
</g>
<!-- Meta&#45;RL&#45;&gt;RL^2 -->
<g id="edge82" class="edge"><title>Meta&#45;RL&#45;&gt;RL^2</title>
<path fill="none" stroke="black" d="M1885.57,-1115.98C1892.66,-1092.75 1904,-1049.69 1904,-1012 1904,-1012 1904,-1012 1904,-426 1904,-404.133 1921.05,-386.558 1938.31,-374.364"/>
<polygon fill="black" stroke="black" points="1940.31,-377.24 1946.74,-368.815 1936.46,-371.394 1940.31,-377.24"/>
</g>
<!-- MAML -->
<g id="node72" class="node"><title>MAML</title>
<g id="a_node72"><a xlink:title="(from the abstract) We propose an algorithm for meta&#45;learning that is model&#45;
agnostic, in the sense that it is compatible with any model trained with
gradient descent and applicable to a variety of different learning problems,
including classification, regression, and reinforcement learning. The goal of
meta&#45;learning is to train a model on a variety of learning tasks, such that it
can solve new learning tasks using only a small number of training samples. In
our approach, the parameters of the model are explicitly trained such that a
small number of gradient steps with a small amount of training data from a new
task will produce good generalization performance on that task. In effect, our
method trains the model to be easy to fine&#45;tune. We demonstrate that this
approach leads to state&#45;of&#45;the&#45;art performance on two few&#45;shot image
classification benchmarks, produces good results on few&#45;shot regression, and
accelerates fine&#45;tuning for policy gradient reinforcement learning with neural
network policies.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1877,-289C1877,-289 1847,-289 1847,-289 1841,-289 1835,-283 1835,-277 1835,-277 1835,-265 1835,-265 1835,-259 1841,-253 1847,-253 1847,-253 1877,-253 1877,-253 1883,-253 1889,-259 1889,-265 1889,-265 1889,-277 1889,-277 1889,-283 1883,-289 1877,-289"/>
<text text-anchor="middle" x="1862" y="-267.9" font-family="sans-serif" font-size="12.00">MAML</text>
</a>
</g>
</g>
<!-- Meta&#45;RL&#45;&gt;MAML -->
<g id="edge83" class="edge"><title>Meta&#45;RL&#45;&gt;MAML</title>
<path fill="none" stroke="black" d="M1869.35,-1115.68C1856.33,-1092.82 1836,-1050.89 1836,-1012 1836,-1012 1836,-1012 1836,-353 1836,-334.221 1842.41,-314.115 1848.87,-298.632"/>
<polygon fill="black" stroke="black" points="1852.2,-299.747 1853.05,-289.186 1845.8,-296.913 1852.2,-299.747"/>
</g>
<!-- SNAIL -->
<g id="node73" class="node"><title>SNAIL</title>
<g id="a_node73"><a xlink:title="(from the abstract) Deep neural networks excel in regimes with large amounts
of data, but tend to struggle when data is scarce or when they need to adapt
quickly to changes in the task. In response, recent work in meta&#45;learning
proposes training a meta&#45;learner on a distribution of similar tasks, in the
hopes of generalization to novel but related tasks by learning a high&#45;level
strategy that captures the essence of the problem it is asked to solve.
However, many recent meta&#45;learning approaches are extensively hand&#45;designed,
either using architectures specialized to a particular application, or hard&#45;
coding algorithmic components that constrain how the meta&#45;learner solves the
task. We propose a class of simple and generic meta&#45;learner architectures that
use a novel combination of temporal convolutions and soft attention; the
former to aggregate information from past experience and the latter to
pinpoint specific pieces of information. In the most extensive set of meta&#45;
learning experiments to date, we evaluate the resulting Simple Neural
AttentIve Learner (or SNAIL) on several heavily&#45;benchmarked tasks. On all
tasks, in both supervised and reinforcement learning, SNAIL attains state&#45;of&#45;
the&#45;art performance by significant margins.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M1949,-289C1949,-289 1919,-289 1919,-289 1913,-289 1907,-283 1907,-277 1907,-277 1907,-265 1907,-265 1907,-259 1913,-253 1919,-253 1919,-253 1949,-253 1949,-253 1955,-253 1961,-259 1961,-265 1961,-265 1961,-277 1961,-277 1961,-283 1955,-289 1949,-289"/>
<text text-anchor="middle" x="1934" y="-267.9" font-family="sans-serif" font-size="12.00">SNAIL</text>
</a>
</g>
</g>
<!-- Meta&#45;RL&#45;&gt;SNAIL -->
<g id="edge84" class="edge"><title>Meta&#45;RL&#45;&gt;SNAIL</title>
<path fill="none" stroke="black" d="M1880.74,-1115.79C1881.81,-1086.57 1883.22,-1025.45 1879,-974 1873.67,-908.968 1856,-894.25 1856,-829 1856,-829 1856,-829 1856,-353 1856,-326.693 1877.61,-305.639 1898.08,-291.595"/>
<polygon fill="black" stroke="black" points="1900.22,-294.38 1906.72,-286.011 1896.42,-288.501 1900.22,-294.38"/>
</g>
<!-- ProMP -->
<g id="node74" class="node"><title>ProMP</title>
<g id="a_node74"><a xlink:title="ProMP: Proximal Meta&#45;Policy Search (from the abstract) Credit assignment in
Meta&#45;reinforcement learning (Meta&#45;RL) is still poorly understood. Existing
methods either neglect credit assignment to pre&#45;adaptation behavior or
implement it naively. This leads to poor sample&#45;efficiency during meta&#45;
training as well as ineffective task identification strategies. This paper
provides a theoretical analysis of credit assignment in gradient&#45;based Meta&#45;
RL. Building on the gained insights we develop a novel meta&#45;learning algorithm
that overcomes both the issue of poor credit assignment and previous
difficulties in estimating meta&#45;policy gradients. By controlling the
statistical distance of both pre&#45;adaptation and adapted policies during meta&#45;
policy search, the proposed algorithm endows efficient and stable meta&#45;
learning. Our approach leads to superior pre&#45;adaptation policy behavior and
consistently outperforms previous Meta&#45;RL algorithms in sample&#45;efficiency,
wall&#45;clock time, and asymptotic performance.

">
<path fill="#dae8fc" stroke="black" stroke-width="2" d="M2009,-206C2009,-206 1979,-206 1979,-206 1973,-206 1967,-200 1967,-194 1967,-194 1967,-182 1967,-182 1967,-176 1973,-170 1979,-170 1979,-170 2009,-170 2009,-170 2015,-170 2021,-176 2021,-182 2021,-182 2021,-194 2021,-194 2021,-200 2015,-206 2009,-206"/>
<text text-anchor="middle" x="1994" y="-184.9" font-family="sans-serif" font-size="12.00">ProMP</text>
</a>
</g>
</g>
<!-- Meta&#45;RL&#45;&gt;ProMP -->
<g id="edge85" class="edge"><title>Meta&#45;RL&#45;&gt;ProMP</title>
<path fill="none" stroke="black" d="M1912.12,-1121.64C1953.45,-1104.73 2020,-1068.7 2020,-1012 2020,-1012 2020,-1012 2020,-270 2020,-251.221 2013.59,-231.115 2007.13,-215.632"/>
<polygon fill="black" stroke="black" points="2010.2,-213.913 2002.95,-206.186 2003.8,-216.747 2010.2,-213.913"/>
</g>
</g>
</svg>
